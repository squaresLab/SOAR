file_path,api_count,code
SE_Inception_resnet_v2.py,38,"b'import tensorflow as tf\nfrom tflearn.layers.conv import global_avg_pool\nfrom tensorflow.contrib.layers import batch_norm, flatten\nfrom tensorflow.contrib.framework import arg_scope\nfrom cifar10 import *\nimport numpy as np\n\nweight_decay = 0.0005\nmomentum = 0.9\n\ninit_learning_rate = 0.1\n\nreduction_ratio = 4\n\nbatch_size = 128\niteration = 391\n# 128 * 391 ~ 50,000\n\ntest_iteration = 10\n\ntotal_epochs = 100\n\ndef conv_layer(input, filter, kernel, stride=1, padding=\'SAME\', layer_name=""conv"", activation=True):\n    with tf.name_scope(layer_name):\n        network = tf.layers.conv2d(inputs=input, use_bias=True, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n        if activation :\n            network = Relu(network)\n        return network\n\ndef Fully_connected(x, units=class_num, layer_name=\'fully_connected\') :\n    with tf.name_scope(layer_name) :\n        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n\ndef Relu(x):\n    return tf.nn.relu(x)\n\ndef Sigmoid(x):\n    return tf.nn.sigmoid(x)\n\ndef Global_Average_Pooling(x):\n    return global_avg_pool(x, name=\'Global_avg_pooling\')\n\ndef Max_pooling(x, pool_size=[3,3], stride=2, padding=\'VALID\') :\n    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Batch_Normalization(x, training, scope):\n    with arg_scope([batch_norm],\n                   scope=scope,\n                   updates_collections=None,\n                   decay=0.9,\n                   center=True,\n                   scale=True,\n                   zero_debias_moving_mean=True) :\n        return tf.cond(training,\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n\ndef Concatenation(layers) :\n    return tf.concat(layers, axis=3)\n\ndef Dropout(x, rate, training) :\n    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n\ndef Evaluate(sess):\n    test_acc = 0.0\n    test_loss = 0.0\n    test_pre_index = 0\n    add = 1000\n\n    for it in range(test_iteration):\n        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n        test_pre_index = test_pre_index + add\n\n        test_feed_dict = {\n            x: test_batch_x,\n            label: test_batch_y,\n            learning_rate: epoch_learning_rate,\n            training_flag: False\n        }\n\n        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n\n        test_loss += loss_\n        test_acc += acc_\n\n    test_loss /= test_iteration # average loss\n    test_acc /= test_iteration # average accuracy\n\n    summary = tf.Summary(value=[tf.Summary.Value(tag=\'test_loss\', simple_value=test_loss),\n                                tf.Summary.Value(tag=\'test_accuracy\', simple_value=test_acc)])\n\n    return test_acc, test_loss, summary\n\nclass SE_Inception_resnet_v2():\n    def __init__(self, x, training):\n        self.training = training\n        self.model = self.Build_SEnet(x)\n\n    def Stem(self, x, scope):\n        with tf.name_scope(scope) :\n            x = conv_layer(x, filter=32, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_conv1\')\n            x = conv_layer(x, filter=32, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_conv2\')\n            block_1 = conv_layer(x, filter=64, kernel=[3,3], layer_name=scope+\'_conv3\')\n\n            split_max_x = Max_pooling(block_1)\n            split_conv_x = conv_layer(block_1, filter=96, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv1\')\n            x = Concatenation([split_max_x,split_conv_x])\n\n            split_conv_x1 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x1 = conv_layer(split_conv_x1, filter=96, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_split_conv3\')\n\n            split_conv_x2 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv4\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[7,1], layer_name=scope+\'_split_conv5\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[1,7], layer_name=scope+\'_split_conv6\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=96, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_split_conv7\')\n\n            x = Concatenation([split_conv_x1,split_conv_x2])\n\n            split_conv_x = conv_layer(x, filter=192, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv8\')\n            split_max_x = Max_pooling(x)\n\n            x = Concatenation([split_conv_x, split_max_x])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_resnet_A(self, x, scope):\n        with tf.name_scope(scope) :\n            init = x\n\n            split_conv_x1 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=32, kernel=[3,3], layer_name=scope+\'_split_conv3\')\n\n            split_conv_x3 = conv_layer(x, filter=32, kernel=[1,1], layer_name=scope+\'_split_conv4\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=48, kernel=[3,3], layer_name=scope+\'_split_conv5\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=64, kernel=[3,3], layer_name=scope+\'_split_conv6\')\n\n            x = Concatenation([split_conv_x1,split_conv_x2,split_conv_x3])\n            x = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+\'_final_conv1\', activation=False)\n\n            x = x*0.1\n            x = init + x\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_resnet_B(self, x, scope):\n        with tf.name_scope(scope) :\n            init = x\n\n            split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=128, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=160, kernel=[1,7], layer_name=scope+\'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=192, kernel=[7,1], layer_name=scope+\'_split_conv4\')\n\n            x = Concatenation([split_conv_x1, split_conv_x2])\n            x = conv_layer(x, filter=1152, kernel=[1,1], layer_name=scope+\'_final_conv1\', activation=False)\n            # 1154\n            x = x * 0.1\n            x = init + x\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_resnet_C(self, x, scope):\n        with tf.name_scope(scope) :\n            init = x\n\n            split_conv_x1 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=192, kernel=[1, 1], layer_name=scope + \'_split_conv2\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=224, kernel=[1, 3], layer_name=scope + \'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=256, kernel=[3, 1], layer_name=scope + \'_split_conv4\')\n\n            x = Concatenation([split_conv_x1,split_conv_x2])\n            x = conv_layer(x, filter=2144, kernel=[1,1], layer_name=scope+\'_final_conv2\', activation=False)\n            # 2048\n            x = x * 0.1\n            x = init + x\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Reduction_A(self, x, scope):\n        with tf.name_scope(scope) :\n            k = 256\n            l = 256\n            m = 384\n            n = 384\n\n            split_max_x = Max_pooling(x)\n\n            split_conv_x1 = conv_layer(x, filter=n, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=k, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=l, kernel=[3,3], layer_name=scope+\'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=m, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv4\')\n\n            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Reduction_B(self, x, scope):\n        with tf.name_scope(scope) :\n            split_max_x = Max_pooling(x)\n\n            split_conv_x1 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n            split_conv_x1 = conv_layer(split_conv_x1, filter=384, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv2\')\n\n            split_conv_x2 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=288, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv4\')\n\n            split_conv_x3 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv5\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=288, kernel=[3,3], layer_name=scope+\'_split_conv6\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=320, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv7\')\n\n            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2, split_conv_x3])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n        with tf.name_scope(layer_name) :\n\n\n            squeeze = Global_Average_Pooling(input_x)\n\n            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+\'_fully_connected1\')\n            excitation = Relu(excitation)\n            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+\'_fully_connected2\')\n            excitation = Sigmoid(excitation)\n\n            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n            scale = input_x * excitation\n\n            return scale\n\n    def Build_SEnet(self, input_x):\n        input_x = tf.pad(input_x, [[0, 0], [32, 32], [32, 32], [0, 0]])\n        # size 32 -> 96\n        print(np.shape(input_x))\n        # only cifar10 architecture\n\n        x = self.Stem(input_x, scope=\'stem\')\n\n        for i in range(5) :\n            x = self.Inception_resnet_A(x, scope=\'Inception_A\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_A\'+str(i))\n\n        x = self.Reduction_A(x, scope=\'Reduction_A\')\n   \n        channel = int(np.shape(x)[-1])\n        x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_A\')\n\n        for i in range(10)  :\n            x = self.Inception_resnet_B(x, scope=\'Inception_B\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_B\'+str(i))\n\n        x = self.Reduction_B(x, scope=\'Reduction_B\')\n        \n        channel = int(np.shape(x)[-1])\n        x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_B\')\n\n        for i in range(5) :\n            x = self.Inception_resnet_C(x, scope=\'Inception_C\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_C\'+str(i))\n         \n            \n        # channel = int(np.shape(x)[-1])\n        # x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_C\')\n        \n        x = Global_Average_Pooling(x)\n        x = Dropout(x, rate=0.2, training=self.training)\n        x = flatten(x)\n\n        x = Fully_connected(x, layer_name=\'final_fully_connected\')\n        return x\n\n\ntrain_x, train_y, test_x, test_y = prepare_data()\ntrain_x, test_x = color_preprocessing(train_x, test_x)\n\n\n# image_size = 32, img_channels = 3, class_num = 10 in cifar10\nx = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\nlabel = tf.placeholder(tf.float32, shape=[None, class_num])\n\ntraining_flag = tf.placeholder(tf.bool)\n\n\nlearning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n\nlogits = SE_Inception_resnet_v2(x, training=training_flag).model\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n\nl2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\ntrain = optimizer.minimize(cost + l2_loss * weight_decay)\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(\'./model\')\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n    summary_writer = tf.summary.FileWriter(\'./logs\', sess.graph)\n\n    epoch_learning_rate = init_learning_rate\n    for epoch in range(1, total_epochs + 1):\n        if epoch % 30 == 0 :\n            epoch_learning_rate = epoch_learning_rate / 10\n\n        pre_index = 0\n        train_acc = 0.0\n        train_loss = 0.0\n\n        for step in range(1, iteration + 1):\n            if pre_index + batch_size < 50000:\n                batch_x = train_x[pre_index: pre_index + batch_size]\n                batch_y = train_y[pre_index: pre_index + batch_size]\n            else:\n                batch_x = train_x[pre_index:]\n                batch_y = train_y[pre_index:]\n\n            batch_x = data_augmentation(batch_x)\n\n            train_feed_dict = {\n                x: batch_x,\n                label: batch_y,\n                learning_rate: epoch_learning_rate,\n                training_flag: True\n            }\n\n            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n\n            train_loss += batch_loss\n            train_acc += batch_acc\n            pre_index += batch_size\n\n\n        train_loss /= iteration # average loss\n        train_acc /= iteration # average accuracy\n\n        train_summary = tf.Summary(value=[tf.Summary.Value(tag=\'train_loss\', simple_value=train_loss),\n                                          tf.Summary.Value(tag=\'train_accuracy\', simple_value=train_acc)])\n\n        test_acc, test_loss, test_summary = Evaluate(sess)\n\n        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n        summary_writer.flush()\n\n        line = ""epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n"" % (\n            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n        print(line)\n\n        with open(\'logs.txt\', \'a\') as f:\n            f.write(line)\n\n        saver.save(sess=sess, save_path=\'./model/Inception_resnet_v2.ckpt\')\n'"
SE_Inception_v4.py,39,"b'import tensorflow as tf\nfrom tflearn.layers.conv import global_avg_pool\nfrom tensorflow.contrib.layers import batch_norm, flatten\nfrom tensorflow.contrib.framework import arg_scope\nfrom cifar10 import *\nimport numpy as np\n\nweight_decay = 0.0005\nmomentum = 0.9\n\ninit_learning_rate = 0.1\nreduction_ratio = 4\n\nbatch_size = 128\niteration = 391\n# 128 * 391 ~ 50,000\n\ntest_iteration = 10\n\ntotal_epochs = 100\n\ndef conv_layer(input, filter, kernel, stride=1, padding=\'SAME\', layer_name=""conv""):\n    with tf.name_scope(layer_name):\n        network = tf.layers.conv2d(inputs=input, use_bias=True, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n        network = Relu(network)\n        return network\n\ndef Fully_connected(x, units=class_num, layer_name=\'fully_connected\') :\n    with tf.name_scope(layer_name) :\n        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n\ndef Relu(x):\n    return tf.nn.relu(x)\n\ndef Sigmoid(x):\n    return tf.nn.sigmoid(x)\n\ndef Global_Average_Pooling(x):\n    return global_avg_pool(x, name=\'Global_avg_pooling\')\n\ndef Max_pooling(x, pool_size=[3,3], stride=2, padding=\'VALID\') :\n    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Avg_pooling(x, pool_size=[3,3], stride=1, padding=\'SAME\') :\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Batch_Normalization(x, training, scope):\n    with arg_scope([batch_norm],\n                   scope=scope,\n                   updates_collections=None,\n                   decay=0.9,\n                   center=True,\n                   scale=True,\n                   zero_debias_moving_mean=True) :\n        return tf.cond(training,\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n\ndef Concatenation(layers) :\n    return tf.concat(layers, axis=3)\n\ndef Dropout(x, rate, training) :\n    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n\ndef Evaluate(sess):\n    test_acc = 0.0\n    test_loss = 0.0\n    test_pre_index = 0\n    add = 1000\n\n    for it in range(test_iteration):\n        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n        test_pre_index = test_pre_index + add\n\n        test_feed_dict = {\n            x: test_batch_x,\n            label: test_batch_y,\n            learning_rate: epoch_learning_rate,\n            training_flag: False\n        }\n\n        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n\n        test_loss += loss_\n        test_acc += acc_\n\n    test_loss /= test_iteration # average loss\n    test_acc /= test_iteration # average accuracy\n\n    summary = tf.Summary(value=[tf.Summary.Value(tag=\'test_loss\', simple_value=test_loss),\n                                tf.Summary.Value(tag=\'test_accuracy\', simple_value=test_acc)])\n\n    return test_acc, test_loss, summary\n\nclass SE_Inception_v4():\n    def __init__(self, x, training):\n        self.training = training\n        self.model = self.Build_SEnet(x)\n\n    def Stem(self, x, scope):\n        with tf.name_scope(scope) :\n            x = conv_layer(x, filter=32, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_conv1\')\n            x = conv_layer(x, filter=32, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_conv2\')\n            block_1 = conv_layer(x, filter=64, kernel=[3,3], layer_name=scope+\'_conv3\')\n\n            split_max_x = Max_pooling(block_1)\n            split_conv_x = conv_layer(block_1, filter=96, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv1\')\n            x = Concatenation([split_max_x,split_conv_x])\n\n            split_conv_x1 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x1 = conv_layer(split_conv_x1, filter=96, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_split_conv3\')\n\n            split_conv_x2 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv4\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[7,1], layer_name=scope+\'_split_conv5\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=64, kernel=[1,7], layer_name=scope+\'_split_conv6\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=96, kernel=[3,3], padding=\'VALID\', layer_name=scope+\'_split_conv7\')\n\n            x = Concatenation([split_conv_x1,split_conv_x2])\n\n            split_conv_x = conv_layer(x, filter=192, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv8\')\n            split_max_x = Max_pooling(x)\n\n            x = Concatenation([split_conv_x, split_max_x])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_A(self, x, scope):\n        with tf.name_scope(scope) :\n            split_conv_x1 = Avg_pooling(x)\n            split_conv_x1 = conv_layer(split_conv_x1, filter=96, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=96, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n\n            split_conv_x3 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv3\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=96, kernel=[3,3], layer_name=scope+\'_split_conv4\')\n\n            split_conv_x4 = conv_layer(x, filter=64, kernel=[1,1], layer_name=scope+\'_split_conv5\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=96, kernel=[3,3], layer_name=scope+\'_split_conv6\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=96, kernel=[3,3], layer_name=scope+\'_split_conv7\')\n\n            x = Concatenation([split_conv_x1, split_conv_x2, split_conv_x3, split_conv_x4])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_B(self, x, scope):\n        with tf.name_scope(scope) :\n            init = x\n\n            split_conv_x1 = Avg_pooling(x)\n            split_conv_x1 = conv_layer(split_conv_x1, filter=128, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n\n            split_conv_x3 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+\'_split_conv3\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=224, kernel=[1,7], layer_name=scope+\'_split_conv4\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=256, kernel=[1,7], layer_name=scope+\'_split_conv5\')\n\n            split_conv_x4 = conv_layer(x, filter=192, kernel=[1,1], layer_name=scope+\'_split_conv6\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=192, kernel=[1,7], layer_name=scope+\'_split_conv7\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=224, kernel=[7,1], layer_name=scope+\'_split_conv8\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=224, kernel=[1,7], layer_name=scope+\'_split_conv9\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=256, kernel=[7,1], layer_name=scope+\'_split_connv10\')\n\n            x = Concatenation([split_conv_x1, split_conv_x2, split_conv_x3, split_conv_x4])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Inception_C(self, x, scope):\n        with tf.name_scope(scope) :\n            split_conv_x1 = Avg_pooling(x)\n            split_conv_x1 = conv_layer(split_conv_x1, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n\n            split_conv_x3 = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+\'_split_conv3\')\n            split_conv_x3_1 = conv_layer(split_conv_x3, filter=256, kernel=[1,3], layer_name=scope+\'_split_conv4\')\n            split_conv_x3_2 = conv_layer(split_conv_x3, filter=256, kernel=[3,1], layer_name=scope+\'_split_conv5\')\n\n            split_conv_x4 = conv_layer(x, filter=384, kernel=[1,1], layer_name=scope+\'_split_conv6\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=448, kernel=[1,3], layer_name=scope+\'_split_conv7\')\n            split_conv_x4 = conv_layer(split_conv_x4, filter=512, kernel=[3,1], layer_name=scope+\'_split_conv8\')\n            split_conv_x4_1 = conv_layer(split_conv_x4, filter=256, kernel=[3,1], layer_name=scope+\'_split_conv9\')\n            split_conv_x4_2 = conv_layer(split_conv_x4, filter=256, kernel=[1,3], layer_name=scope+\'_split_conv10\')\n\n            x = Concatenation([split_conv_x1, split_conv_x2, split_conv_x3_1, split_conv_x3_2, split_conv_x4_1, split_conv_x4_2])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Reduction_A(self, x, scope):\n        with tf.name_scope(scope) :\n            k = 256\n            l = 256\n            m = 384\n            n = 384\n\n            split_max_x = Max_pooling(x)\n\n            split_conv_x1 = conv_layer(x, filter=n, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv1\')\n\n            split_conv_x2 = conv_layer(x, filter=k, kernel=[1,1], layer_name=scope+\'_split_conv2\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=l, kernel=[3,3], layer_name=scope+\'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=m, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv4\')\n\n            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Reduction_B(self, x, scope):\n        with tf.name_scope(scope) :\n            split_max_x = Max_pooling(x)\n\n            split_conv_x1 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv1\')\n            split_conv_x1 = conv_layer(split_conv_x1, filter=384, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv2\')\n\n            split_conv_x2 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv3\')\n            split_conv_x2 = conv_layer(split_conv_x2, filter=288, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv4\')\n\n            split_conv_x3 = conv_layer(x, filter=256, kernel=[1,1], layer_name=scope+\'_split_conv5\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=288, kernel=[3,3], layer_name=scope+\'_split_conv6\')\n            split_conv_x3 = conv_layer(split_conv_x3, filter=320, kernel=[3,3], stride=2, padding=\'VALID\', layer_name=scope+\'_split_conv7\')\n\n            x = Concatenation([split_max_x, split_conv_x1, split_conv_x2, split_conv_x3])\n\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def Squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n        with tf.name_scope(layer_name) :\n            squeeze = Global_Average_Pooling(input_x)\n\n            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+\'_fully_connected1\')\n            excitation = Relu(excitation)\n            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+\'_fully_connected2\')\n            excitation = Sigmoid(excitation)\n\n            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n\n            scale = input_x * excitation\n\n            return scale\n\n    def Build_SEnet(self, input_x):\n        input_x = tf.pad(input_x, [[0, 0], [32, 32], [32, 32], [0, 0]])\n        # size 32 -> 96\n        # only cifar10 architecture\n\n        x = self.Stem(input_x, scope=\'stem\')\n\n        for i in range(4) :\n            x = self.Inception_A(x, scope=\'Inception_A\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_A\'+str(i))\n\n        x = self.Reduction_A(x, scope=\'Reduction_A\')\n\n        for i in range(7)  :\n            x = self.Inception_B(x, scope=\'Inception_B\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_B\'+str(i))\n\n        x = self.Reduction_B(x, scope=\'Reduction_B\')\n\n        for i in range(3) :\n            x = self.Inception_C(x, scope=\'Inception_C\'+str(i))\n            channel = int(np.shape(x)[-1])\n            x = self.Squeeze_excitation_layer(x, out_dim=channel, ratio=reduction_ratio, layer_name=\'SE_C\'+str(i))\n\n        x = Global_Average_Pooling(x)\n        x = Dropout(x, rate=0.2, training=self.training)\n        x = flatten(x)\n\n        x = Fully_connected(x, layer_name=\'final_fully_connected\')\n        return x\n\n\ntrain_x, train_y, test_x, test_y = prepare_data()\ntrain_x, test_x = color_preprocessing(train_x, test_x)\n\n\n# image_size = 32, img_channels = 3, class_num = 10 in cifar10\nx = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\nlabel = tf.placeholder(tf.float32, shape=[None, class_num])\n\ntraining_flag = tf.placeholder(tf.bool)\n\n\nlearning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n\nlogits = SE_Inception_v4(x, training=training_flag).model\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n\nl2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\ntrain = optimizer.minimize(cost + l2_loss * weight_decay)\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(\'./model\')\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n    summary_writer = tf.summary.FileWriter(\'./logs\', sess.graph)\n\n    epoch_learning_rate = init_learning_rate\n    for epoch in range(1, total_epochs + 1):\n        if epoch % 30 == 0 :\n            epoch_learning_rate = epoch_learning_rate / 10\n\n        pre_index = 0\n        train_acc = 0.0\n        train_loss = 0.0\n\n        for step in range(1, iteration + 1):\n            if pre_index + batch_size < 50000:\n                batch_x = train_x[pre_index: pre_index + batch_size]\n                batch_y = train_y[pre_index: pre_index + batch_size]\n            else:\n                batch_x = train_x[pre_index:]\n                batch_y = train_y[pre_index:]\n\n            batch_x = data_augmentation(batch_x)\n\n            train_feed_dict = {\n                x: batch_x,\n                label: batch_y,\n                learning_rate: epoch_learning_rate,\n                training_flag: True\n            }\n\n            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n\n            train_loss += batch_loss\n            train_acc += batch_acc\n            pre_index += batch_size\n\n\n        train_loss /= iteration # average loss\n        train_acc /= iteration # average accuracy\n\n        train_summary = tf.Summary(value=[tf.Summary.Value(tag=\'train_loss\', simple_value=train_loss),\n                                          tf.Summary.Value(tag=\'train_accuracy\', simple_value=train_acc)])\n\n        test_acc, test_loss, test_summary = Evaluate(sess)\n\n        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n        summary_writer.flush()\n\n        line = ""epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n"" % (\n            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n        print(line)\n\n        with open(\'logs.txt\', \'a\') as f:\n            f.write(line)\n\n        saver.save(sess=sess, save_path=\'./model/Inception_v4.ckpt\')'"
SE_ResNeXt.py,35,"b'import tensorflow as tf\nfrom tflearn.layers.conv import global_avg_pool\nfrom tensorflow.contrib.layers import batch_norm, flatten\nfrom tensorflow.contrib.framework import arg_scope\nfrom cifar10 import *\nimport numpy as np\n\nweight_decay = 0.0005\nmomentum = 0.9\n\ninit_learning_rate = 0.1\ncardinality = 8 # how many split ?\nblocks = 3 # res_block ! (split + transition)\ndepth = 64 # out channel\n\n""""""\nSo, the total number of layers is (3*blokcs)*residual_layer_num + 2\nbecause, blocks = split(conv 2) + transition(conv 1) = 3 layer\nand, first conv layer 1, last dense layer 1\nthus, total number of layers = (3*blocks)*residual_layer_num + 2\n""""""\n\nreduction_ratio = 4\n\nbatch_size = 128\niteration = 391\n# 128 * 391 ~ 50,000\n\ntest_iteration = 10\n\ntotal_epochs = 100\n\ndef conv_layer(input, filter, kernel, stride, padding=\'SAME\', layer_name=""conv""):\n    with tf.name_scope(layer_name):\n        network = tf.layers.conv2d(inputs=input, use_bias=False, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n        return network\n\ndef Global_Average_Pooling(x):\n    return global_avg_pool(x, name=\'Global_avg_pooling\')\n\ndef Average_pooling(x, pool_size=[2,2], stride=2, padding=\'SAME\'):\n    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n\ndef Batch_Normalization(x, training, scope):\n    with arg_scope([batch_norm],\n                   scope=scope,\n                   updates_collections=None,\n                   decay=0.9,\n                   center=True,\n                   scale=True,\n                   zero_debias_moving_mean=True) :\n        return tf.cond(training,\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n\ndef Relu(x):\n    return tf.nn.relu(x)\n\ndef Sigmoid(x) :\n    return tf.nn.sigmoid(x)\n\ndef Concatenation(layers) :\n    return tf.concat(layers, axis=3)\n\ndef Fully_connected(x, units=class_num, layer_name=\'fully_connected\') :\n    with tf.name_scope(layer_name) :\n        return tf.layers.dense(inputs=x, use_bias=False, units=units)\n\ndef Evaluate(sess):\n    test_acc = 0.0\n    test_loss = 0.0\n    test_pre_index = 0\n    add = 1000\n\n    for it in range(test_iteration):\n        test_batch_x = test_x[test_pre_index: test_pre_index + add]\n        test_batch_y = test_y[test_pre_index: test_pre_index + add]\n        test_pre_index = test_pre_index + add\n\n        test_feed_dict = {\n            x: test_batch_x,\n            label: test_batch_y,\n            learning_rate: epoch_learning_rate,\n            training_flag: False\n        }\n\n        loss_, acc_ = sess.run([cost, accuracy], feed_dict=test_feed_dict)\n\n        test_loss += loss_\n        test_acc += acc_\n\n    test_loss /= test_iteration # average loss\n    test_acc /= test_iteration # average accuracy\n\n    summary = tf.Summary(value=[tf.Summary.Value(tag=\'test_loss\', simple_value=test_loss),\n                                tf.Summary.Value(tag=\'test_accuracy\', simple_value=test_acc)])\n\n    return test_acc, test_loss, summary\n\nclass SE_ResNeXt():\n    def __init__(self, x, training):\n        self.training = training\n        self.model = self.Build_SEnet(x)\n\n    def first_layer(self, x, scope):\n        with tf.name_scope(scope) :\n            x = conv_layer(x, filter=64, kernel=[3, 3], stride=1, layer_name=scope+\'_conv1\')\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            return x\n\n    def transform_layer(self, x, stride, scope):\n        with tf.name_scope(scope) :\n            x = conv_layer(x, filter=depth, kernel=[1,1], stride=1, layer_name=scope+\'_conv1\')\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            x = Relu(x)\n\n            x = conv_layer(x, filter=depth, kernel=[3,3], stride=stride, layer_name=scope+\'_conv2\')\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch2\')\n            x = Relu(x)\n            return x\n\n    def transition_layer(self, x, out_dim, scope):\n        with tf.name_scope(scope):\n            x = conv_layer(x, filter=out_dim, kernel=[1,1], stride=1, layer_name=scope+\'_conv1\')\n            x = Batch_Normalization(x, training=self.training, scope=scope+\'_batch1\')\n            # x = Relu(x)\n\n            return x\n\n    def split_layer(self, input_x, stride, layer_name):\n        with tf.name_scope(layer_name) :\n            layers_split = list()\n            for i in range(cardinality) :\n                splits = self.transform_layer(input_x, stride=stride, scope=layer_name + \'_splitN_\' + str(i))\n                layers_split.append(splits)\n\n            return Concatenation(layers_split)\n\n    def squeeze_excitation_layer(self, input_x, out_dim, ratio, layer_name):\n        with tf.name_scope(layer_name) :\n\n\n            squeeze = Global_Average_Pooling(input_x)\n\n            excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+\'_fully_connected1\')\n            excitation = Relu(excitation)\n            excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+\'_fully_connected2\')\n            excitation = Sigmoid(excitation)\n\n            excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n            scale = input_x * excitation\n\n            return scale\n\n    def residual_layer(self, input_x, out_dim, layer_num, res_block=blocks):\n        # split + transform(bottleneck) + transition + merge\n        # input_dim = input_x.get_shape().as_list()[-1]\n\n        for i in range(res_block):\n            input_dim = int(np.shape(input_x)[-1])\n\n            if input_dim * 2 == out_dim:\n                flag = True\n                stride = 2\n                channel = input_dim // 2\n            else:\n                flag = False\n                stride = 1\n\n            x = self.split_layer(input_x, stride=stride, layer_name=\'split_layer_\'+layer_num+\'_\'+str(i))\n            x = self.transition_layer(x, out_dim=out_dim, scope=\'trans_layer_\'+layer_num+\'_\'+str(i))\n            x = self.squeeze_excitation_layer(x, out_dim=out_dim, ratio=reduction_ratio, layer_name=\'squeeze_layer_\'+layer_num+\'_\'+str(i))\n\n            if flag is True :\n                pad_input_x = Average_pooling(input_x)\n                pad_input_x = tf.pad(pad_input_x, [[0, 0], [0, 0], [0, 0], [channel, channel]]) # [?, height, width, channel]\n            else :\n                pad_input_x = input_x\n\n            input_x = Relu(x + pad_input_x)\n\n        return input_x\n\n\n    def Build_SEnet(self, input_x):\n        # only cifar10 architecture\n\n        input_x = self.first_layer(input_x, scope=\'first_layer\')\n\n        x = self.residual_layer(input_x, out_dim=64, layer_num=\'1\')\n        x = self.residual_layer(x, out_dim=128, layer_num=\'2\')\n        x = self.residual_layer(x, out_dim=256, layer_num=\'3\')\n\n        x = Global_Average_Pooling(x)\n        x = flatten(x)\n\n        x = Fully_connected(x, layer_name=\'final_fully_connected\')\n        return x\n\n\ntrain_x, train_y, test_x, test_y = prepare_data()\ntrain_x, test_x = color_preprocessing(train_x, test_x)\n\n\n# image_size = 32, img_channels = 3, class_num = 10 in cifar10\nx = tf.placeholder(tf.float32, shape=[None, image_size, image_size, img_channels])\nlabel = tf.placeholder(tf.float32, shape=[None, class_num])\n\ntraining_flag = tf.placeholder(tf.bool)\n\n\nlearning_rate = tf.placeholder(tf.float32, name=\'learning_rate\')\n\nlogits = SE_ResNeXt(x, training=training_flag).model\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits))\n\nl2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum, use_nesterov=True)\ntrain = optimizer.minimize(cost + l2_loss * weight_decay)\n\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\nsaver = tf.train.Saver(tf.global_variables())\n\nwith tf.Session() as sess:\n    ckpt = tf.train.get_checkpoint_state(\'./model\')\n    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n\n    summary_writer = tf.summary.FileWriter(\'./logs\', sess.graph)\n\n    epoch_learning_rate = init_learning_rate\n    for epoch in range(1, total_epochs + 1):\n        if epoch % 30 == 0 :\n            epoch_learning_rate = epoch_learning_rate / 10\n\n        pre_index = 0\n        train_acc = 0.0\n        train_loss = 0.0\n\n        for step in range(1, iteration + 1):\n            if pre_index + batch_size < 50000:\n                batch_x = train_x[pre_index: pre_index + batch_size]\n                batch_y = train_y[pre_index: pre_index + batch_size]\n            else:\n                batch_x = train_x[pre_index:]\n                batch_y = train_y[pre_index:]\n\n            batch_x = data_augmentation(batch_x)\n\n            train_feed_dict = {\n                x: batch_x,\n                label: batch_y,\n                learning_rate: epoch_learning_rate,\n                training_flag: True\n            }\n\n            _, batch_loss = sess.run([train, cost], feed_dict=train_feed_dict)\n            batch_acc = accuracy.eval(feed_dict=train_feed_dict)\n\n            train_loss += batch_loss\n            train_acc += batch_acc\n            pre_index += batch_size\n\n\n        train_loss /= iteration # average loss\n        train_acc /= iteration # average accuracy\n\n        train_summary = tf.Summary(value=[tf.Summary.Value(tag=\'train_loss\', simple_value=train_loss),\n                                          tf.Summary.Value(tag=\'train_accuracy\', simple_value=train_acc)])\n\n        test_acc, test_loss, test_summary = Evaluate(sess)\n\n        summary_writer.add_summary(summary=train_summary, global_step=epoch)\n        summary_writer.add_summary(summary=test_summary, global_step=epoch)\n        summary_writer.flush()\n\n        line = ""epoch: %d/%d, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f \\n"" % (\n            epoch, total_epochs, train_loss, train_acc, test_loss, test_acc)\n        print(line)\n\n        with open(\'logs.txt\', \'a\') as f:\n            f.write(line)\n\n        saver.save(sess=sess, save_path=\'./model/ResNeXt.ckpt\')\n'"
cifar10.py,0,"b'# -*- coding:utf-8 -*-\n\nimport os\nimport sys\nimport time\nimport pickle\nimport random\nimport numpy as np\n\nclass_num = 10\nimage_size = 32\nimg_channels = 3\n\n\n# ========================================================== #\n# \xe2\x94\x9c\xe2\x94\x80 prepare_data()\n#  \xe2\x94\x9c\xe2\x94\x80 download training data if not exist by download_data()\n#  \xe2\x94\x9c\xe2\x94\x80 load data by load_data()\n#  \xe2\x94\x94\xe2\x94\x80 shuffe and return data\n# ========================================================== #\n\n\n\ndef download_data():\n    dirname = \'cifar-10-batches-py\'\n    origin = \'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\'\n    fname = \'cifar-10-python.tar.gz\'\n    fpath = \'./\' + dirname\n\n    download = False\n    if os.path.exists(fpath) or os.path.isfile(fname):\n        download = False\n        print(""DataSet aready exist!"")\n    else:\n        download = True\n    if download:\n        print(\'Downloading data from\', origin)\n        import urllib.request\n        import tarfile\n\n        def reporthook(count, block_size, total_size):\n            global start_time\n            if count == 0:\n                start_time = time.time()\n                return\n            duration = time.time() - start_time\n            progress_size = int(count * block_size)\n            speed = int(progress_size / (1024 * duration))\n            percent = min(int(count * block_size * 100 / total_size), 100)\n            sys.stdout.write(""\\r...%d%%, %d MB, %d KB/s, %d seconds passed"" %\n                             (percent, progress_size / (1024 * 1024), speed, duration))\n            sys.stdout.flush()\n\n        urllib.request.urlretrieve(origin, fname, reporthook)\n        print(\'Download finished. Start extract!\', origin)\n        if (fname.endswith(""tar.gz"")):\n            tar = tarfile.open(fname, ""r:gz"")\n            tar.extractall()\n            tar.close()\n        elif (fname.endswith(""tar"")):\n            tar = tarfile.open(fname, ""r:"")\n            tar.extractall()\n            tar.close()\n\n\ndef unpickle(file):\n    with open(file, \'rb\') as fo:\n        dict = pickle.load(fo, encoding=\'bytes\')\n    return dict\n\n\ndef load_data_one(file):\n    batch = unpickle(file)\n    data = batch[b\'data\']\n    labels = batch[b\'labels\']\n    print(""Loading %s : %d."" % (file, len(data)))\n    return data, labels\n\n\ndef load_data(files, data_dir, label_count):\n    global image_size, img_channels\n    data, labels = load_data_one(data_dir + \'/\' + files[0])\n    for f in files[1:]:\n        data_n, labels_n = load_data_one(data_dir + \'/\' + f)\n        data = np.append(data, data_n, axis=0)\n        labels = np.append(labels, labels_n, axis=0)\n    labels = np.array([[float(i == label) for i in range(label_count)] for label in labels])\n    data = data.reshape([-1, img_channels, image_size, image_size])\n    data = data.transpose([0, 2, 3, 1])\n    return data, labels\n\n\ndef prepare_data():\n    print(""======Loading data======"")\n    download_data()\n    data_dir = \'./cifar-10-batches-py\'\n    image_dim = image_size * image_size * img_channels\n    meta = unpickle(data_dir + \'/batches.meta\')\n\n    label_names = meta[b\'label_names\']\n    label_count = len(label_names)\n    train_files = [\'data_batch_%d\' % d for d in range(1, 6)]\n    train_data, train_labels = load_data(train_files, data_dir, label_count)\n    test_data, test_labels = load_data([\'test_batch\'], data_dir, label_count)\n\n    print(""Train data:"", np.shape(train_data), np.shape(train_labels))\n    print(""Test data :"", np.shape(test_data), np.shape(test_labels))\n    print(""======Load finished======"")\n\n    print(""======Shuffling data======"")\n    indices = np.random.permutation(len(train_data))\n    train_data = train_data[indices]\n    train_labels = train_labels[indices]\n    print(""======Prepare Finished======"")\n\n    return train_data, train_labels, test_data, test_labels\n\n\n# ========================================================== #\n# \xe2\x94\x9c\xe2\x94\x80 _random_crop()\n# \xe2\x94\x9c\xe2\x94\x80 _random_flip_leftright()\n# \xe2\x94\x9c\xe2\x94\x80 data_augmentation()\n# \xe2\x94\x94\xe2\x94\x80 color_preprocessing()\n# ========================================================== #\n\ndef _random_crop(batch, crop_shape, padding=None):\n    oshape = np.shape(batch[0])\n\n    if padding:\n        oshape = (oshape[0] + 2 * padding, oshape[1] + 2 * padding)\n    new_batch = []\n    npad = ((padding, padding), (padding, padding), (0, 0))\n    for i in range(len(batch)):\n        new_batch.append(batch[i])\n        if padding:\n            new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n                                      mode=\'constant\', constant_values=0)\n        nh = random.randint(0, oshape[0] - crop_shape[0])\n        nw = random.randint(0, oshape[1] - crop_shape[1])\n        new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n                       nw:nw + crop_shape[1]]\n    return new_batch\n\n\ndef _random_flip_leftright(batch):\n    for i in range(len(batch)):\n        if bool(random.getrandbits(1)):\n            batch[i] = np.fliplr(batch[i])\n    return batch\n\n\ndef color_preprocessing(x_train, x_test):\n    x_train = x_train.astype(\'float32\')\n    x_test = x_test.astype(\'float32\')\n    x_train[:, :, :, 0] = (x_train[:, :, :, 0] - np.mean(x_train[:, :, :, 0])) / np.std(x_train[:, :, :, 0])\n    x_train[:, :, :, 1] = (x_train[:, :, :, 1] - np.mean(x_train[:, :, :, 1])) / np.std(x_train[:, :, :, 1])\n    x_train[:, :, :, 2] = (x_train[:, :, :, 2] - np.mean(x_train[:, :, :, 2])) / np.std(x_train[:, :, :, 2])\n\n    x_test[:, :, :, 0] = (x_test[:, :, :, 0] - np.mean(x_test[:, :, :, 0])) / np.std(x_test[:, :, :, 0])\n    x_test[:, :, :, 1] = (x_test[:, :, :, 1] - np.mean(x_test[:, :, :, 1])) / np.std(x_test[:, :, :, 1])\n    x_test[:, :, :, 2] = (x_test[:, :, :, 2] - np.mean(x_test[:, :, :, 2])) / np.std(x_test[:, :, :, 2])\n\n    return x_train, x_test\n\n\ndef data_augmentation(batch):\n    batch = _random_flip_leftright(batch)\n    batch = _random_crop(batch, [32, 32], 4)\n    return batch'"
