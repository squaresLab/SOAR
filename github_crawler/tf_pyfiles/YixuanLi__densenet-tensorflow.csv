file_path,api_count,code
cifar10-densenet.py,24,"b'#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\nimport argparse\nimport os\n\n\nfrom tensorpack import *\nfrom tensorpack.tfutils.symbolic_functions import *\nfrom tensorpack.tfutils.summary import *\n\n""""""\nCIFAR10 DenseNet example. See: http://arxiv.org/abs/1608.06993\nCode is developed based on Yuxin Wu\'s ResNet implementation: https://github.com/ppwwyyxx/tensorpack/tree/master/examples/ResNet\nResults using DenseNet (L=40, K=12) on Cifar10 with data augmentation: ~5.77% test error.\n\nRunning time:\nOn one TITAN X GPU (CUDA 7.5 and cudnn 5.1), the code should run ~5iters/s on a batch size 64.\n""""""\n\nBATCH_SIZE = 64\n\nclass Model(ModelDesc):\n    def __init__(self, depth):\n        super(Model, self).__init__()\n        self.N = int((depth - 4)  / 3)\n        self.growthRate =12\n\n    def _get_inputs(self):\n        return [InputDesc(tf.float32, [None, 32, 32, 3], \'input\'),\n                InputDesc(tf.int32, [None], \'label\')\n               ]\n\n    def _build_graph(self, input_vars):\n        image, label = input_vars\n        image = image / 128.0 - 1\n\n        def conv(name, l, channel, stride):\n            return Conv2D(name, l, channel, 3, stride=stride,\n                          nl=tf.identity, use_bias=False,\n                          W_init=tf.random_normal_initializer(stddev=np.sqrt(2.0/9/channel)))\n        def add_layer(name, l):\n            shape = l.get_shape().as_list()\n            in_channel = shape[3]\n            with tf.variable_scope(name) as scope:\n                c = BatchNorm(\'bn1\', l)\n                c = tf.nn.relu(c)\n                c = conv(\'conv1\', c, self.growthRate, 1)\n                l = tf.concat([c, l], 3)\n            return l\n\n        def add_transition(name, l):\n            shape = l.get_shape().as_list()\n            in_channel = shape[3]\n            with tf.variable_scope(name) as scope:\n                l = BatchNorm(\'bn1\', l)\n                l = tf.nn.relu(l)\n                l = Conv2D(\'conv1\', l, in_channel, 1, stride=1, use_bias=False, nl=tf.nn.relu)\n                l = AvgPooling(\'pool\', l, 2)\n            return l\n\n\n        def dense_net(name):\n            l = conv(\'conv0\', image, 16, 1)\n            with tf.variable_scope(\'block1\') as scope:\n\n                for i in range(self.N):\n                    l = add_layer(\'dense_layer.{}\'.format(i), l)\n                l = add_transition(\'transition1\', l)\n\n            with tf.variable_scope(\'block2\') as scope:\n\n                for i in range(self.N):\n                    l = add_layer(\'dense_layer.{}\'.format(i), l)\n                l = add_transition(\'transition2\', l)\n\n            with tf.variable_scope(\'block3\') as scope:\n\n                for i in range(self.N):\n                    l = add_layer(\'dense_layer.{}\'.format(i), l)\n            l = BatchNorm(\'bnlast\', l)\n            l = tf.nn.relu(l)\n            l = GlobalAvgPooling(\'gap\', l)\n            logits = FullyConnected(\'linear\', l, out_dim=10, nl=tf.identity)\n\n            return logits\n\n        logits = dense_net(""dense_net"")\n\n        prob = tf.nn.softmax(logits, name=\'output\')\n\n        cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n        cost = tf.reduce_mean(cost, name=\'cross_entropy_loss\')\n\n        wrong = prediction_incorrect(logits, label)\n        # monitor training error\n        add_moving_summary(tf.reduce_mean(wrong, name=\'train_error\'))\n\n        # weight decay on all W\n        wd_cost = tf.multiply(1e-4, regularize_cost(\'.*/W\', tf.nn.l2_loss), name=\'wd_cost\')\n        add_moving_summary(cost, wd_cost)\n\n        add_param_summary((\'.*/W\', [\'histogram\']))   # monitor W\n        self.cost = tf.add_n([cost, wd_cost], name=\'cost\')\n\n    def _get_optimizer(self):\n        lr = tf.get_variable(\'learning_rate\', initializer=0.1, trainable=False)\n        tf.summary.scalar(\'learning_rate\', lr)\n        return tf.train.MomentumOptimizer(lr, 0.9, use_nesterov=True)\n\n\ndef get_data(train_or_test):\n    isTrain = train_or_test == \'train\'\n    ds = dataset.Cifar10(train_or_test)\n    pp_mean = ds.get_per_pixel_mean()\n    if isTrain:\n        augmentors = [\n            imgaug.CenterPaste((40, 40)),\n            imgaug.RandomCrop((32, 32)),\n            imgaug.Flip(horiz=True),\n            #imgaug.Brightness(20),\n            #imgaug.Contrast((0.6,1.4)),\n            imgaug.MapImage(lambda x: x - pp_mean),\n        ]\n    else:\n        augmentors = [\n            imgaug.MapImage(lambda x: x - pp_mean)\n        ]\n    ds = AugmentImageComponent(ds, augmentors)\n    ds = BatchData(ds, BATCH_SIZE, remainder=not isTrain)\n    if isTrain:\n        ds = PrefetchData(ds, 3, 2)\n    return ds\n\ndef get_config():\n    log_dir = \'train_log/cifar10-single-fisrt%s-second%s-max%s\' % (str(args.drop_1), str(args.drop_2), str(args.max_epoch))\n    logger.set_logger_dir(log_dir, action=\'n\')\n\n    # prepare dataset\n    dataset_train = get_data(\'train\')\n    steps_per_epoch = dataset_train.size()\n    dataset_test = get_data(\'test\')\n\n    return TrainConfig(\n        dataflow=dataset_train,\n        callbacks=[\n            ModelSaver(),\n            InferenceRunner(dataset_test,\n                [ScalarStats(\'cost\'), ClassificationError()]),\n            ScheduledHyperParamSetter(\'learning_rate\',\n                                      [(1, 0.1), (args.drop_1, 0.01), (args.drop_2, 0.001)])\n        ],\n        model=Model(depth=args.depth),\n        steps_per_epoch=steps_per_epoch,\n        max_epoch=args.max_epoch,\n    )\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', help=\'comma separated list of GPU(s) to use.\') # nargs=\'*\' in multi mode\n    parser.add_argument(\'--load\', help=\'load model\')\n    parser.add_argument(\'--drop_1\',default=150, help=\'Epoch to drop learning rate to 0.01.\') # nargs=\'*\' in multi mode\n    parser.add_argument(\'--drop_2\',default=225,help=\'Epoch to drop learning rate to 0.001\')\n    parser.add_argument(\'--depth\',default=40, help=\'The depth of densenet\')\n    parser.add_argument(\'--max_epoch\',default=300,help=\'max epoch\')\n    args = parser.parse_args()\n\n    if args.gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu\n\n    config = get_config()\n    if args.load:\n        config.session_init = SaverRestore(args.load)\n    \n    nr_tower = 0\n    if args.gpu:\n        nr_tower = len(args.gpu.split(\',\'))\n    \n    # SyncMultiGPUTrainer(config).train()\n    launch_train_with_config(config, SyncMultiGPUTrainer(nr_tower))\n'"
