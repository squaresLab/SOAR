file_path,api_count,code
01-TF2.0-Overview/conv_train.py,17,"b'import os\nimport time\nimport numpy as np\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'  # or any {\'0\', \'1\', \'2\'}\n\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import summary_ops_v2\nfrom tensorflow import keras\nfrom tensorflow.keras import datasets, layers, models, optimizers, metrics\n\n\n\n\nmodel = tf.keras.Sequential([\n    layers.Reshape(\n        target_shape=[28, 28, 1],\n        input_shape=(28, 28,)),\n    layers.Conv2D(2, 5, padding=\'same\', activation=tf.nn.relu),\n    layers.MaxPooling2D((2, 2), (2, 2), padding=\'same\'),\n    layers.Conv2D(4, 5, padding=\'same\', activation=tf.nn.relu),\n    layers.MaxPooling2D((2, 2), (2, 2), padding=\'same\'),\n    layers.Flatten(),\n    layers.Dense(32, activation=tf.nn.relu),\n    layers.Dropout(rate=0.4),\n    layers.Dense(10)])\n\ncompute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ncompute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\noptimizer = optimizers.SGD(learning_rate=0.01, momentum=0.5)\n\n\ndef mnist_datasets():\n    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n    # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\n    x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\n    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    return train_dataset, test_dataset\n\n\ntrain_ds, test_ds = mnist_datasets()\ntrain_ds = train_ds.shuffle(60000).batch(100)\ntest_ds = test_ds.batch(100)\n\n\ndef train_step(model, optimizer, images, labels):\n\n    # Record the operations used to compute the loss, so that the gradient\n    # of the loss with respect to the variables can be computed.\n    with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        loss = compute_loss(labels, logits)\n        compute_accuracy(labels, logits)\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    return loss\n\n\ndef train(model, optimizer, dataset, log_freq=50):\n    """"""\n    Trains model on `dataset` using `optimizer`.\n    """"""\n    # Metrics are stateful. They accumulate values and return a cumulative\n    # result when you call .result(). Clear accumulated values with .reset_states()\n    avg_loss = metrics.Mean(\'loss\', dtype=tf.float32)\n\n    # Datasets can be iterated over like any other Python iterable.\n    for images, labels in dataset:\n        loss = train_step(model, optimizer, images, labels)\n        avg_loss(loss)\n\n        if tf.equal(optimizer.iterations % log_freq, 0):\n            # summary_ops_v2.scalar(\'loss\', avg_loss.result(), step=optimizer.iterations)\n            # summary_ops_v2.scalar(\'accuracy\', compute_accuracy.result(), step=optimizer.iterations)\n            print(\'step:\', int(optimizer.iterations),\n                  \'loss:\', avg_loss.result().numpy(),\n                  \'acc:\', compute_accuracy.result().numpy())\n            avg_loss.reset_states()\n            compute_accuracy.reset_states()\n\n\ndef test(model, dataset, step_num):\n    """"""\n    Perform an evaluation of `model` on the examples from `dataset`.\n    """"""\n    avg_loss = metrics.Mean(\'loss\', dtype=tf.float32)\n\n    for (images, labels) in dataset:\n        logits = model(images, training=False)\n        avg_loss(compute_loss(labels, logits))\n        compute_accuracy(labels, logits)\n\n    print(\'Model test set loss: {:0.4f} accuracy: {:0.2f}%\'.format(\n        avg_loss.result(), compute_accuracy.result() * 100))\n\n    print(\'loss:\', avg_loss.result(), \'acc:\', compute_accuracy.result())\n    # summary_ops_v2.scalar(\'loss\', avg_loss.result(), step=step_num)\n    # summary_ops_v2.scalar(\'accuracy\', compute_accuracy.result(), step=step_num)\n\n\n# Where to save checkpoints, tensorboard summaries, etc.\nMODEL_DIR = \'/tmp/tensorflow/mnist\'\n\n\ndef apply_clean():\n    if tf.io.gfile.exists(MODEL_DIR):\n        print(\'Removing existing model dir: {}\'.format(MODEL_DIR))\n        tf.io.gfile.rmtree(MODEL_DIR)\n\n\napply_clean()\n\ncheckpoint_dir = os.path.join(MODEL_DIR, \'checkpoints\')\ncheckpoint_prefix = os.path.join(checkpoint_dir, \'ckpt\')\n\ncheckpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n\n# Restore variables on creation if a checkpoint exists.\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n\nNUM_TRAIN_EPOCHS = 5\n\nfor i in range(NUM_TRAIN_EPOCHS):\n    start = time.time()\n    #   with train_summary_writer.as_default():\n    train(model, optimizer, train_ds, log_freq=500)\n    end = time.time()\n    print(\'Train time for epoch #{} ({} total steps): {}\'.format(\n        i + 1, int(optimizer.iterations), end - start))\n    #   with test_summary_writer.as_default():\n    #     test(model, test_ds, optimizer.iterations)\n    checkpoint.save(checkpoint_prefix)\n    print(\'saved checkpoint.\')\n\nexport_path = os.path.join(MODEL_DIR, \'export\')\ntf.saved_model.save(model, export_path)\nprint(\'saved SavedModel for exporting.\')'"
01-TF2.0-Overview/fc_train.py,12,"b""import os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers, datasets\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n\ndef mnist_dataset():\n  (x, y), _ = datasets.mnist.load_data()\n  ds = tf.data.Dataset.from_tensor_slices((x, y))\n  ds = ds.map(prepare_mnist_features_and_labels)\n  ds = ds.take(20000).shuffle(20000).batch(100)\n  return ds\n\n@tf.function\ndef prepare_mnist_features_and_labels(x, y):\n  x = tf.cast(x, tf.float32) / 255.0\n  y = tf.cast(y, tf.int64)\n  return x, y\n\n\nmodel = keras.Sequential([\n    layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n    layers.Dense(100, activation='relu'),\n    layers.Dense(100, activation='relu'),\n    layers.Dense(10)])\n\noptimizer = optimizers.Adam()\n\n\n@tf.function\ndef compute_loss(logits, labels):\n  return tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))\n\n@tf.function\ndef compute_accuracy(logits, labels):\n  predictions = tf.argmax(logits, axis=1)\n  return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n\n@tf.function\ndef train_one_step(model, optimizer, x, y):\n\n  with tf.GradientTape() as tape:\n    logits = model(x)\n    loss = compute_loss(logits, y)\n\n  # compute gradient\n  grads = tape.gradient(loss, model.trainable_variables)\n  # update to weights\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  accuracy = compute_accuracy(logits, y)\n\n  # loss and accuracy is scalar tensor\n  return loss, accuracy\n\n\ndef train(epoch, model, optimizer):\n\n  train_ds = mnist_dataset()\n  loss = 0.0\n  accuracy = 0.0\n  for step, (x, y) in enumerate(train_ds):\n    loss, accuracy = train_one_step(model, optimizer, x, y)\n\n    if step % 500 == 0:\n      print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n\n  return loss, accuracy\n\n\nfor epoch in range(20):\n  loss, accuracy = train(epoch, model, optimizer)\n\nprint('Final epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())"""
02-AutoGraph/main.py,4,"b'import  tensorflow as tf\n\nimport timeit\n\n\n\n\n\ncell = tf.keras.layers.LSTMCell(10)\n\n\n@tf.function\ndef fn(input, state):\n    """"""\n    use static graph to compute LSTM\n    :param input:\n    :param state:\n    :return:\n    """"""\n\n    return cell(input, state)\n\n\n\ninput = tf.zeros([10, 10])\nstate = [tf.zeros([10, 10])] * 2\n\n# warmup\ncell(input, state)\nfn(input, state)\n\n\ndynamic_graph_time = timeit.timeit(lambda: cell(input, state), number=100)\nstatic_graph_time = timeit.timeit(lambda: fn(input, state), number=100)\nprint(\'dynamic_graph_time:\', dynamic_graph_time)\nprint(\'static_graph_time:\', static_graph_time)\n\n\n\n'"
03-Play-with-MNIST/main.py,8,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\n\n\n(xs, ys),_ = datasets.mnist.load_data()\nprint('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n\n\nxs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\ndb = tf.data.Dataset.from_tensor_slices((xs,ys))\ndb = db.batch(32).repeat(10)\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(256, activation='relu'),\n                     layers.Dense(256, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.SGD(lr=0.01)\nacc_meter = metrics.Accuracy()\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10)\n        # [b, 10]\n        loss = tf.square(out-y_onehot)\n        # [b]\n        loss = tf.reduce_sum(loss) / 32\n\n\n    acc_meter.update_state(tf.argmax(out, axis=1), y)\n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 200==0:\n\n        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n        acc_meter.reset_states()\n\n\n\n\n\n"""
04-Linear-Regression/main.py,10,"b""import  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nimport  os\n\n\nclass Regressor(keras.layers.Layer):\n\n    def __init__(self):\n        super(Regressor, self).__init__()\n\n        # here must specify shape instead of tensor !\n        # name here is meanless !\n        # [dim_in, dim_out]\n        self.w = self.add_variable('meanless-name', [13, 1])\n        # [dim_out]\n        self.b = self.add_variable('meanless-name', [1])\n\n        print(self.w.shape, self.b.shape)\n        print(type(self.w), tf.is_tensor(self.w), self.w.name)\n        print(type(self.b), tf.is_tensor(self.b), self.b.name)\n\n\n    def call(self, x):\n\n        x = tf.matmul(x, self.w) + self.b\n\n        return x\n\ndef main():\n\n    tf.random.set_seed(22)\n    np.random.seed(22)\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    assert tf.__version__.startswith('2.')\n\n\n    (x_train, y_train), (x_val, y_val) = keras.datasets.boston_housing.load_data()\n    #\n    x_train, x_val = x_train.astype(np.float32), x_val.astype(np.float32)\n    # (404, 13) (404,) (102, 13) (102,)\n    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n    # Here has two mis-leading issues:\n    # 1. (x_train, y_train) cant be written as [x_train, y_train]\n    # 2.\n    db_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)\n    db_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(102)\n\n\n    model = Regressor()\n    criteon = keras.losses.MeanSquaredError()\n    optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n\n    for epoch in range(200):\n\n        for step, (x, y) in enumerate(db_train):\n\n            with tf.GradientTape() as tape:\n                # [b, 1]\n                logits = model(x)\n                # [b]\n                logits = tf.squeeze(logits, axis=1)\n                # [b] vs [b]\n                loss = criteon(y, logits)\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        print(epoch, 'loss:', loss.numpy())\n\n\n        if epoch % 10 == 0:\n\n            for x, y in db_val:\n                # [b, 1]\n                logits = model(x)\n                # [b]\n                logits = tf.squeeze(logits, axis=1)\n                # [b] vs [b]\n                loss = criteon(y, logits)\n\n                print(epoch, 'val loss:', loss.numpy())\n\n\n\n\n\nif __name__ == '__main__':\n    main()"""
05-FashionMNIST/mnist_Seqential_gradient.py,8,"b""import  os\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\ndef prepare_mnist_features_and_labels(x, y):\n  x = tf.cast(x, tf.float32) / 255.0\n  y = tf.cast(y, tf.int64)\n  return x, y\n\ndef mnist_dataset():\n  (x, y), _ = datasets.fashion_mnist.load_data()\n  ds = tf.data.Dataset.from_tensor_slices((x, y))\n  ds = ds.map(prepare_mnist_features_and_labels)\n  ds = ds.take(20000).shuffle(20000).batch(100)\n  return ds\n\n\n\n\n\n\ndef compute_loss(logits, labels):\n  return tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))\n\n\ndef compute_accuracy(logits, labels):\n  predictions = tf.argmax(logits, axis=1)\n  return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n\n\ndef train_one_step(model, optimizer, x, y):\n\n  with tf.GradientTape() as tape:\n\n    logits = model(x)\n    loss = compute_loss(logits, y)\n\n  # compute gradient\n  grads = tape.gradient(loss, model.trainable_variables)\n  # update to weights\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  accuracy = compute_accuracy(logits, y)\n\n  # loss and accuracy is scalar tensor\n  return loss, accuracy\n\n\ndef train(epoch, model, optimizer):\n\n  train_ds = mnist_dataset()\n  loss = 0.0\n  accuracy = 0.0\n  for step, (x, y) in enumerate(train_ds):\n    loss, accuracy = train_one_step(model, optimizer, x, y)\n    if step%500==0:\n      print('epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n  return loss, accuracy\n\n\n\ndef main():\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}\n\n    train_dataset = mnist_dataset()\n\n    model = keras.Sequential([\n        layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n        layers.Dense(200, activation='relu'),\n        layers.Dense(200, activation='relu'),\n        layers.Dense(10)])\n\n\n    optimizer = optimizers.Adam()\n\n    for epoch in range(20):\n        loss, accuracy = train(epoch, model, optimizer)\n    print('Final epoch', epoch, ': loss', loss.numpy(), '; accuracy', accuracy.numpy())\n\n\nif __name__ == '__main__':\n    main()"""
05-FashionMNIST/mnist_custommodel.py,12,"b""import  os\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\ndef prepare_mnist_features_and_labels(x, y):\n  x = tf.cast(x, tf.float32) / 255.0\n  y = tf.cast(y, tf.int64)\n  return x, y\n\ndef mnist_dataset():\n\n  (x, y), (x_val, y_val) = datasets.fashion_mnist.load_data()\n  print('x/y shape:', x.shape, y.shape)\n  y = tf.one_hot(y, depth=10)\n  y_val = tf.one_hot(y_val, depth=10)\n\n  ds = tf.data.Dataset.from_tensor_slices((x, y))\n  ds = ds.map(prepare_mnist_features_and_labels)\n  ds = ds.shuffle(60000).batch(100)\n\n  ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n  ds_val = ds_val.map(prepare_mnist_features_and_labels)\n  ds_val = ds_val.shuffle(10000).batch(100)\n\n  sample = next(iter(ds))\n  print('sample:', sample[0].shape, sample[1].shape)\n\n  return ds,ds_val\n\n\n\n\n\n\nclass MyModel(keras.Model):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n\n        # self.model = keras.Sequential([\n        #     layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n        #     layers.Dense(100, activation='relu'),\n        #     layers.Dense(100, activation='relu'),\n        #     layers.Dense(10)])\n\n        self.layer1 = layers.Dense(200, activation=tf.nn.relu)\n        self.layer2 = layers.Dense(200, activation=tf.nn.relu)\n        # self.layer3 = layers.Dense(200, activation=tf.nn.relu)\n        self.layer4 = layers.Dense(10)\n\n    def call(self, x, training=False):\n\n        x = tf.reshape(x, [-1, 28*28])\n        x = self.layer1(x)\n        x = self.layer2(x)\n        # x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\ndef main():\n\n    tf.random.set_seed(22)\n\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n\n    train_dataset, val_dataset = mnist_dataset()\n\n    model = MyModel()\n    model.compile(optimizer=optimizers.Adam(1e-3),\n                  loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n\n\n    model.fit(train_dataset.repeat(), epochs=30, steps_per_epoch=500, verbose=1,\n              validation_data=val_dataset.repeat(),\n              validation_steps=2)\n\n\nif __name__ == '__main__':\n    main()"""
05-FashionMNIST/mnist_fit.py,7,"b""import  os\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\n\n\ndef prepare_mnist_features_and_labels(x, y):\n  x = tf.cast(x, tf.float32) / 255.0\n  y = tf.cast(y, tf.int64)\n\n  return x, y\n\ndef mnist_dataset():\n  (x, y), (x_val, y_val) = datasets.fashion_mnist.load_data()\n  print('x/y shape:', x.shape, y.shape)\n  y = tf.one_hot(y, depth=10)\n  y_val = tf.one_hot(y_val, depth=10)\n  ds = tf.data.Dataset.from_tensor_slices((x, y))\n  ds = ds.map(prepare_mnist_features_and_labels)\n  ds = ds.shuffle(60000).batch(100)\n\n\n  ds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n  ds_val = ds_val.map(prepare_mnist_features_and_labels)\n  ds_val = ds_val.shuffle(10000).batch(100)\n\n  return ds,ds_val\n\n\n\n\n\n\ndef main():\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n\n    train_dataset, val_dataset = mnist_dataset()\n\n    model = keras.Sequential([\n        layers.Reshape(target_shape=(28 * 28,), input_shape=(28, 28)),\n        layers.Dense(200, activation='relu'),\n        layers.Dense(200, activation='relu'),\n        layers.Dense(200, activation='relu'),\n        layers.Dense(10)])\n    # no need to use compile if you have no loss/optimizer/metrics involved here.\n    model.compile(optimizer=optimizers.Adam(0.001),\n                  loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\n    model.fit(train_dataset.repeat(), epochs=30, steps_per_epoch=500,\n              validation_data=val_dataset.repeat(),\n              validation_steps=2\n              )\n\n\nif __name__ == '__main__':\n    main()"""
05-FashionMNIST/mnist_matmul.py,10,"b'import  os\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\ndef prepare_mnist_features_and_labels(x, y):\n  x = tf.cast(x, tf.float32) / 255.0\n  y = tf.cast(y, tf.int64)\n  return x, y\n\n\n\ndef mnist_dataset():\n  (x, y), _ = datasets.fashion_mnist.load_data()\n\n  print(\'x/y shape:\', x.shape, y.shape)\n\n  ds = tf.data.Dataset.from_tensor_slices((x, y))\n  ds = ds.map(prepare_mnist_features_and_labels)\n  ds = ds.take(20000).shuffle(20000).batch(100)\n  return ds\n\n\n\n\n\n\ndef compute_loss(logits, labels):\n  return tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))\n\n\ndef compute_accuracy(logits, labels):\n  predictions = tf.argmax(logits, axis=1)\n  return tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n\n\ndef train_one_step(model, optimizer, x, y):\n\n  with tf.GradientTape() as tape:\n\n    logits = model(x)\n    loss = compute_loss(logits, y)\n\n  # compute gradient\n  grads = tape.gradient(loss, model.trainable_variables)\n  # update to weights\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  accuracy = compute_accuracy(logits, y)\n\n  # loss and accuracy is scalar tensor\n  return loss, accuracy\n\n\ndef train(epoch, model, optimizer):\n\n  train_ds = mnist_dataset()\n  loss = 0.0\n  accuracy = 0.0\n\n  for step, (x, y) in enumerate(train_ds):\n\n    loss, accuracy = train_one_step(model, optimizer, x, y)\n\n    if step%500==0:\n      print(\'epoch\', epoch, \': loss\', loss.numpy(), \'; accuracy\', accuracy.numpy())\n  return loss, accuracy\n\n\n\n\n\nclass MyLayer(layers.Layer):\n\n\n    def __init__(self, units):\n        """"""\n\n        :param units: [input_dim, h1_dim,...,hn_dim, output_dim]\n        """"""\n        super(MyLayer, self).__init__()\n\n\n        for i in range(1, len(units)):\n            # w: [input_dim, output_dim]\n            self.add_variable(name=\'kernel%d\'%i, shape=[units[i-1], units[i]])\n            # b: [output_dim]\n            self.add_variable(name=\'bias%d\'%i,shape=[units[i]])\n\n\n\n    def call(self, x):\n        """"""\n\n        :param x: [b, input_dim]\n        :return:\n        """"""\n        num = len(self.trainable_variables)\n\n        x = tf.reshape(x, [-1, 28*28])\n\n        for i in range(0, num, 2):\n\n            x = tf.matmul(x, self.trainable_variables[i]) + self.trainable_variables[i+1]\n\n        return x\n\n\n\ndef main():\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'  # or any {\'0\', \'1\', \'2\'}\n\n    train_dataset = mnist_dataset()\n\n    model = MyLayer([28*28, 200, 200, 10])\n    for p in model.trainable_variables:\n        print(p.name, p.shape)\n    optimizer = optimizers.Adam()\n\n    for epoch in range(20):\n        loss, accuracy = train(epoch, model, optimizer)\n\n    print(\'Final epoch\', epoch, \': loss\', loss.numpy(), \'; accuracy\', accuracy.numpy())\n\n\nif __name__ == \'__main__\':\n    main()'"
06-CIFAR-VGG/main.py,18,"b'import  os\nimport  tensorflow as tf\nfrom    tensorflow import  keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  argparse\nimport  numpy as np\n\n\n\nfrom    network import VGG16\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'  # or any {\'0\', \'1\', \'2\'}\nargparser = argparse.ArgumentParser()\n\n\n\nargparser.add_argument(\'--train_dir\', type=str, default=\'/tmp/cifar10_train\',\n                           help=""Directory where to write event logs and checkpoint."")\nargparser.add_argument(\'--max_steps\', type=int, default=1000000,\n                            help=""""""Number of batches to run."""""")\nargparser.add_argument(\'--log_device_placement\', action=\'store_true\',\n                            help=""Whether to log device placement."")\nargparser.add_argument(\'--log_frequency\', type=int, default=10,\n                            help=""How often to log results to the console."")\n\n\ndef normalize(X_train, X_test):\n    # this function normalize inputs for zero mean and unit variance\n    # it is used when training a model.\n    # Input: training set and test set\n    # Output: normalized training set and test set according to the trianing set statistics.\n    X_train = X_train / 255.\n    X_test = X_test / 255.\n\n    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n    std = np.std(X_train, axis=(0, 1, 2, 3))\n    print(\'mean:\', mean, \'std:\', std)\n    X_train = (X_train - mean) / (std + 1e-7)\n    X_test = (X_test - mean) / (std + 1e-7)\n    return X_train, X_test\n\ndef prepare_cifar(x, y):\n\n    x = tf.cast(x, tf.float32)\n    y = tf.cast(y, tf.int32)\n    return x, y\n\n\n\ndef compute_loss(logits, labels):\n  return tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          logits=logits, labels=labels))\n\ndef main():\n\n    tf.random.set_seed(22)\n\n    print(\'loading data...\')\n    (x,y), (x_test, y_test) = datasets.cifar10.load_data()\n    x, x_test = normalize(x, x_test)\n    print(x.shape, y.shape, x_test.shape, y_test.shape)\n    # x = tf.convert_to_tensor(x)\n    # y = tf.convert_to_tensor(y)\n    train_loader = tf.data.Dataset.from_tensor_slices((x,y))\n    train_loader = train_loader.map(prepare_cifar).shuffle(50000).batch(256)\n\n    test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n    test_loader = test_loader.map(prepare_cifar).shuffle(10000).batch(256)\n    print(\'done.\')\n\n\n\n\n    model = VGG16([32, 32, 3])\n\n\n    # must specify from_logits=True!\n    criteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n    metric = keras.metrics.CategoricalAccuracy()\n\n    optimizer = optimizers.Adam(learning_rate=0.0001)\n\n\n    for epoch in range(250):\n\n        for step, (x, y) in enumerate(train_loader):\n            # [b, 1] => [b]\n            y = tf.squeeze(y, axis=1)\n            # [b, 10]\n            y = tf.one_hot(y, depth=10)\n\n            with tf.GradientTape() as tape:\n                logits = model(x)\n                loss = criteon(y, logits)\n                # loss2 = compute_loss(logits, tf.argmax(y, axis=1))\n                # mse_loss = tf.reduce_sum(tf.square(y-logits))\n                # print(y.shape, logits.shape)\n                metric.update_state(y, logits)\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            # MUST clip gradient here or it will disconverge!\n            grads = [ tf.clip_by_norm(g, 15) for g in grads]\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            if step % 40 == 0:\n                # for g in grads:\n                #     print(tf.norm(g).numpy())\n                print(epoch, step, \'loss:\', float(loss), \'acc:\', metric.result().numpy())\n                metric.reset_states()\n\n\n        if epoch % 1 == 0:\n\n            metric = keras.metrics.CategoricalAccuracy()\n            for x, y in test_loader:\n                # [b, 1] => [b]\n                y = tf.squeeze(y, axis=1)\n                # [b, 10]\n                y = tf.one_hot(y, depth=10)\n\n                logits = model.predict(x)\n                # be careful, these functions can accept y as [b] without warnning.\n                metric.update_state(y, logits)\n            print(\'test acc:\', metric.result().numpy())\n            metric.reset_states()\n\n\n\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
06-CIFAR-VGG/network.py,0,"b'import  tensorflow as tf\nfrom    tensorflow import  keras\nfrom    tensorflow.keras import datasets, layers, optimizers, models\nfrom    tensorflow.keras import regularizers\n\n\n\n\n\n\n\n\n\n\n\nclass VGG16(models.Model):\n\n\n    def __init__(self, input_shape):\n        """"""\n\n        :param input_shape: [32, 32, 3]\n        """"""\n        super(VGG16, self).__init__()\n\n        weight_decay = 0.000\n        self.num_classes = 10\n\n        model = models.Sequential()\n\n        model.add(layers.Conv2D(64, (3, 3), padding=\'same\',\n                         input_shape=input_shape, kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.3))\n\n        model.add(layers.Conv2D(64, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n        model.add(layers.Conv2D(128, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(128, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n        model.add(layers.Conv2D(256, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(256, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(256, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n        model.add(layers.Dropout(0.4))\n\n        model.add(layers.Conv2D(512, (3, 3), padding=\'same\',kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n        model.add(layers.Dropout(0.5))\n\n        model.add(layers.Flatten())\n        model.add(layers.Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n        model.add(layers.Activation(\'relu\'))\n        model.add(layers.BatchNormalization())\n\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(self.num_classes))\n        # model.add(layers.Activation(\'softmax\'))\n\n\n        self.model = model\n\n\n    def call(self, x):\n\n        x = self.model(x)\n\n        return x'"
07-Inception/main.py,8,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\n# In[1]:\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\n# In[2]:\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n# [b, 28, 28] => [b, 28, 28, 1]\nx_train, x_test = np.expand_dims(x_train, axis=3), np.expand_dims(x_test, axis=3)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(256)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(256)\n\n# In[3]:\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n# In[4]:\n\n\nclass ConvBNRelu(keras.Model):\n    \n    def __init__(self, ch, kernelsz=3, strides=1, padding='same'):\n        super(ConvBNRelu, self).__init__()\n        \n        self.model = keras.models.Sequential([\n            keras.layers.Conv2D(ch, kernelsz, strides=strides, padding=padding),\n            keras.layers.BatchNormalization(),\n            keras.layers.ReLU()\n        ])\n        \n        \n    def call(self, x, training=None):\n        \n        x = self.model(x, training=training)\n        \n        return x \n    \n        \n        \n\n\n# In[5]:\n\n\nclass InceptionBlk(keras.Model):\n    \n    def __init__(self, ch, strides=1):\n        super(InceptionBlk, self).__init__()\n        \n        self.ch = ch\n        self.strides = strides\n        \n        self.conv1 = ConvBNRelu(ch, strides=strides)\n        self.conv2 = ConvBNRelu(ch, kernelsz=3, strides=strides)\n        self.conv3_1 = ConvBNRelu(ch, kernelsz=3, strides=strides)\n        self.conv3_2 = ConvBNRelu(ch, kernelsz=3, strides=1)\n        \n        self.pool = keras.layers.MaxPooling2D(3, strides=1, padding='same')\n        self.pool_conv = ConvBNRelu(ch, strides=strides)\n        \n        \n    def call(self, x, training=None):\n        \n        \n        x1 = self.conv1(x, training=training)\n\n        x2 = self.conv2(x, training=training)\n                \n        x3_1 = self.conv3_1(x, training=training)\n        x3_2 = self.conv3_2(x3_1, training=training)\n                \n        x4 = self.pool(x)\n        x4 = self.pool_conv(x4, training=training)\n        \n        # concat along axis=channel\n        x = tf.concat([x1, x2, x3_2, x4], axis=3)\n        \n        return x\n\n\n# In[6]:\n\n\nclass Inception(keras.Model):\n    \n    def __init__(self, num_layers, num_classes, init_ch=16, **kwargs):\n        super(Inception, self).__init__(**kwargs)\n        \n        self.in_channels = init_ch\n        self.out_channels = init_ch\n        self.num_layers = num_layers\n        self.init_ch = init_ch\n        \n        self.conv1 = ConvBNRelu(init_ch)\n        \n        self.blocks = keras.models.Sequential(name='dynamic-blocks')\n        \n        for block_id in range(num_layers):\n            \n            for layer_id in range(2):\n                \n                if layer_id == 0:\n                    \n                    block = InceptionBlk(self.out_channels, strides=2)\n                    \n                else:\n                    block = InceptionBlk(self.out_channels, strides=1)\n                    \n                self.blocks.add(block)\n            \n            # enlarger out_channels per block    \n            self.out_channels *= 2\n            \n        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n        self.fc = keras.layers.Dense(num_classes)\n        \n        \n    def call(self, x, training=None):\n        \n        out = self.conv1(x, training=training)\n        \n        out = self.blocks(out, training=training)\n        \n        out = self.avg_pool(out)\n        out = self.fc(out)\n        \n        return out    \n            \n        \n\n\n# In[7]:\n\n\n# build model and optimizer\nbatch_size = 32\nepochs = 100\nmodel = Inception(2, 10)\n# derive input shape for every layers.\nmodel.build(input_shape=(None, 28, 28, 1))\nmodel.summary()\n\noptimizer = keras.optimizers.Adam(learning_rate=1e-3)\ncriteon = keras.losses.CategoricalCrossentropy(from_logits=True)\n\nacc_meter = keras.metrics.Accuracy()\n\n\nfor epoch in range(100):\n\n    for step, (x, y) in enumerate(db_train):\n\n        with tf.GradientTape() as tape:\n            # print(x.shape, y.shape)\n            # [b, 10]\n            logits = model(x)\n            # [b] vs [b, 10]\n            loss = criteon(tf.one_hot(y, depth=10), logits)\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        if step % 10 == 0:\n            print(epoch, step, 'loss:', loss.numpy())\n\n\n    acc_meter.reset_states()\n    for x, y in db_test:\n        # [b, 10]\n        logits = model(x, training=False)\n        # [b, 10] => [b]\n        pred = tf.argmax(logits, axis=1)\n        # [b] vs [b, 10]\n        acc_meter.update_state(y, pred)\n\n    print(epoch, 'evaluation acc:', acc_meter.result().numpy())\n"""
08-ResNet/main.py,10,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\n# In[1]:\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n# [b, 28, 28] => [b, 28, 28, 1]\nx_train, x_test = np.expand_dims(x_train, axis=3), np.expand_dims(x_test, axis=3)\n# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\n# and tensors as input to keras\ny_train_ohe = tf.one_hot(y_train, depth=10).numpy()\ny_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n\n# In[2]:\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n\n# 3x3 convolution\ndef conv3x3(channels, stride=1, kernel=(3, 3)):\n    return keras.layers.Conv2D(channels, kernel, strides=stride, padding=\'same\',\n                               use_bias=False,\n                            kernel_initializer=tf.random_normal_initializer())\n\nclass ResnetBlock(keras.Model):\n\n    def __init__(self, channels, strides=1, residual_path=False):\n        super(ResnetBlock, self).__init__()\n\n        self.channels = channels\n        self.strides = strides\n        self.residual_path = residual_path\n\n        self.conv1 = conv3x3(channels, strides)\n        self.bn1 = keras.layers.BatchNormalization()\n        self.conv2 = conv3x3(channels)\n        self.bn2 = keras.layers.BatchNormalization()\n\n        if residual_path:\n            self.down_conv = conv3x3(channels, strides, kernel=(1, 1))\n            self.down_bn = tf.keras.layers.BatchNormalization()\n\n    def call(self, inputs, training=None):\n        residual = inputs\n\n        x = self.bn1(inputs, training=training)\n        x = tf.nn.relu(x)\n        x = self.conv1(x)\n        x = self.bn2(x, training=training)\n        x = tf.nn.relu(x)\n        x = self.conv2(x)\n\n        # this module can be added into self.\n        # however, module in for can not be added.\n        if self.residual_path:\n            residual = self.down_bn(inputs, training=training)\n            residual = tf.nn.relu(residual)\n            residual = self.down_conv(residual)\n\n        x = x + residual\n        return x\n\n\nclass ResNet(keras.Model):\n\n    def __init__(self, block_list, num_classes, initial_filters=16, **kwargs):\n        super(ResNet, self).__init__(**kwargs)\n\n        self.num_blocks = len(block_list)\n        self.block_list = block_list\n\n        self.in_channels = initial_filters\n        self.out_channels = initial_filters\n        self.conv_initial = conv3x3(self.out_channels)\n\n        self.blocks = keras.models.Sequential(name=\'dynamic-blocks\')\n\n        # build all the blocks\n        for block_id in range(len(block_list)):\n            for layer_id in range(block_list[block_id]):\n\n                if block_id != 0 and layer_id == 0:\n                    block = ResnetBlock(self.out_channels, strides=2, residual_path=True)\n                else:\n                    if self.in_channels != self.out_channels:\n                        residual_path = True\n                    else:\n                        residual_path = False\n                    block = ResnetBlock(self.out_channels, residual_path=residual_path)\n\n                self.in_channels = self.out_channels\n\n                self.blocks.add(block)\n\n            self.out_channels *= 2\n\n        self.final_bn = keras.layers.BatchNormalization()\n        self.avg_pool = keras.layers.GlobalAveragePooling2D()\n        self.fc = keras.layers.Dense(num_classes)\n\n    def call(self, inputs, training=None):\n\n        out = self.conv_initial(inputs)\n\n        out = self.blocks(out, training=training)\n\n        out = self.final_bn(out, training=training)\n        out = tf.nn.relu(out)\n\n        out = self.avg_pool(out)\n        out = self.fc(out)\n\n\n        return out\n\n# In[3]:\n\ndef main():\n    num_classes = 10\n    batch_size = 32\n    epochs = 1\n\n    # build model and optimizer\n    model = ResNet([2, 2, 2], num_classes)\n    model.compile(optimizer=keras.optimizers.Adam(0.001),\n                  loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n                  metrics=[\'accuracy\'])\n    model.build(input_shape=(None, 28, 28, 1))\n    print(""Number of variables in the model :"", len(model.variables))\n    model.summary()\n\n    # train\n    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n              validation_data=(x_test, y_test_ohe), verbose=1)\n\n    # evaluate on test set\n    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n    print(""Final test loss and accuracy :"", scores)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
09-RNN-Sentiment-Analysis/main.py,4,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\n# In[16]:\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\n# fix random seed for reproducibility\nnp.random.seed(7)\n# load the dataset but only keep the top n words, zero the rest\ntop_words = 10000\n# truncate and pad input sequences\nmax_review_length = 80\n(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=top_words)\n# X_train = tf.convert_to_tensor(X_train)\n# y_train = tf.one_hot(y_train, depth=2)\nprint(\'Pad sequences (samples x time)\')\nx_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\nx_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\nprint(\'x_train shape:\', x_train.shape)\nprint(\'x_test shape:\', x_test.shape)\n\n\nclass RNN(keras.Model):\n\n    def __init__(self, units, num_classes, num_layers):\n        super(RNN, self).__init__()\n\n\n        # self.cells = [keras.layers.LSTMCell(units) for _ in range(num_layers)]\n        #\n        # self.rnn = keras.layers.RNN(self.cells, unroll=True)\n        self.rnn = keras.layers.LSTM(units, return_sequences=True)\n        self.rnn2 = keras.layers.LSTM(units)\n\n        # self.cells = (keras.layers.LSTMCell(units) for _ in range(num_layers))\n        # #\n        # self.rnn = keras.layers.RNN(self.cells, return_sequences=True, return_state=True)\n        # self.rnn = keras.layers.LSTM(units, unroll=True)\n        # self.rnn = keras.layers.StackedRNNCells(self.cells)\n\n\n        # have 1000 words totally, every word will be embedding into 100 length vector\n        # the max sentence lenght is 80 words\n        self.embedding = keras.layers.Embedding(top_words, 100, input_length=max_review_length)\n        self.fc = keras.layers.Dense(1)\n\n    def call(self, inputs, training=None, mask=None):\n\n        # print(\'x\', inputs.shape)\n        # [b, sentence len] => [b, sentence len, word embedding]\n        x = self.embedding(inputs)\n        # print(\'embedding\', x.shape)\n        x = self.rnn(x) \n        x = self.rnn2(x) \n        # print(\'rnn\', x.shape)\n\n        x = self.fc(x)\n        print(x.shape)\n\n        return x\n\n\ndef main():\n\n    units = 64\n    num_classes = 2\n    batch_size = 32\n    epochs = 20\n\n    model = RNN(units, num_classes, num_layers=2)\n\n\n    model.compile(optimizer=keras.optimizers.Adam(0.001),\n                  loss=keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=[\'accuracy\'])\n\n    # train\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n              validation_data=(x_test, y_test), verbose=1)\n\n    # evaluate on test set\n    scores = model.evaluate(x_test, y_test, batch_size, verbose=1)\n    print(""Final test loss and accuracy :"", scores)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
10-ColorBot/main.py,8,"b'import  os, six, time\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\nfrom    matplotlib import pyplot as plt\n\nfrom    utils import load_dataset, parse\nfrom    model import RNNColorbot\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\n\n\n\n\ndef test(model, eval_data):\n    """"""\n    Computes the average loss on eval_data, which should be a Dataset.\n    """"""\n    avg_loss = keras.metrics.Mean()\n\n    for (labels, chars, sequence_length) in eval_data:\n\n        predictions = model((chars, sequence_length), training=False)\n        avg_loss.update_state(keras.losses.mean_squared_error(labels, predictions))\n\n    print(""eval/loss: %.6f"" % avg_loss.result().numpy())\n\n\n\ndef train_one_epoch(model, optimizer, train_data, log_interval, epoch):\n    """"""\n    Trains model on train_data using optimizer.\n    """"""\n\n    for step, (labels, chars, sequence_length) in enumerate(train_data):\n\n        with tf.GradientTape() as tape:\n            predictions = model((chars, sequence_length), training=True)\n            loss = keras.losses.mean_squared_error(labels, predictions)\n            loss = tf.reduce_mean(loss)\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n        if step % 100 == 0:\n            print(epoch, step, \'loss:\', float(loss))\n\n\nSOURCE_TRAIN_URL = ""https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/archive/extras/colorbot/data/train.csv""\nSOURCE_TEST_URL = ""https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/archive/extras/colorbot/data/test.csv""\n\n\ndef main():\n\n    batchsz = 64\n    rnn_cell_sizes = [256, 128]\n    epochs = 40\n\n    data_dir = os.path.join(\'.\', ""data"")\n    train_data = load_dataset(\n        data_dir=data_dir, url=SOURCE_TRAIN_URL, batch_size=batchsz)\n    eval_data = load_dataset(\n        data_dir=data_dir, url=SOURCE_TEST_URL, batch_size=batchsz)\n\n    model = RNNColorbot(\n        rnn_cell_sizes=rnn_cell_sizes,\n        label_dimension=3,\n        keep_prob=0.5)\n    optimizer = keras.optimizers.Adam(0.01)\n\n\n\n    for epoch in range(epochs):\n\n        start = time.time()\n        train_one_epoch(model, optimizer, train_data, 50, epoch)\n        end = time.time()\n        # print(""train/time for epoch #%d: %.2f"" % (epoch, end - start))\n\n        if epoch % 10 == 0:\n            test(model, eval_data)\n\n    print(""Colorbot is ready to generate colors!"")\n    while True:\n        try:\n            color_name = six.moves.input(""Give me a color name (or press enter to exit): "")\n        except EOFError:\n            return\n\n        if not color_name:\n            return\n\n        _, chars, length = parse(color_name)\n\n        (chars, length) = (tf.identity(chars), tf.identity(length))\n        chars = tf.expand_dims(chars, 0)\n        length = tf.expand_dims(length, 0)\n        preds = tf.unstack(model((chars, length), training=False)[0])\n\n        # Predictions cannot be negative, as they are generated by a ReLU layer;\n        # they may, however, be greater than 1.\n        clipped_preds = tuple(min(float(p), 1.0) for p in preds)\n        rgb = tuple(int(p * 255) for p in clipped_preds)\n        print(""rgb:"", rgb)\n        data = [[clipped_preds]]\n\n        plt.imshow(data)\n        plt.title(color_name)\n        plt.savefig(color_name+\'.png\')\n\n\nif __name__ == ""__main__"":\n    main()\n'"
10-ColorBot/model.py,9,"b'import  tensorflow as tf\nfrom    tensorflow import keras\n\n\n\nclass RNNColorbot(keras.Model):\n    """"""\n    Multi-layer (LSTM) RNN that regresses on real-valued vector labels.\n    """"""\n\n    def __init__(self, rnn_cell_sizes, label_dimension, keep_prob):\n        """"""Constructs an RNNColorbot.\n        Args:\n          rnn_cell_sizes: list of integers denoting the size of each LSTM cell in\n            the RNN; rnn_cell_sizes[i] is the size of the i-th layer cell\n          label_dimension: the length of the labels on which to regress\n          keep_prob: (1 - dropout probability); dropout is applied to the outputs of\n            each LSTM layer\n        """"""\n        super(RNNColorbot, self).__init__(name="""")\n\n        self.rnn_cell_sizes = rnn_cell_sizes\n        self.label_dimension = label_dimension\n        self.keep_prob = keep_prob\n\n        self.cells = [keras.layers.LSTMCell(size) for size in rnn_cell_sizes]\n        self.relu = keras.layers.Dense(label_dimension, activation=tf.nn.relu)\n\n    def call(self, inputs, training=None):\n        """"""\n        Implements the RNN logic and prediction generation.\n        Args:\n          inputs: A tuple (chars, sequence_length), where chars is a batch of\n            one-hot encoded color names represented as a Tensor with dimensions\n            [batch_size, time_steps, 256] and sequence_length holds the length\n            of each character sequence (color name) as a Tensor with dimension\n            [batch_size].\n          training: whether the invocation is happening during training\n        Returns:\n          A tensor of dimension [batch_size, label_dimension] that is produced by\n          passing chars through a multi-layer RNN and applying a ReLU to the final\n          hidden state.\n        """"""\n        (chars, sequence_length) = inputs\n        # Transpose the first and second dimensions so that chars is of shape\n        # [time_steps, batch_size, dimension].\n        chars = tf.transpose(chars, [1, 0, 2])\n        # The outer loop cycles through the layers of the RNN; the inner loop\n        # executes the time steps for a particular layer.\n        batch_size = int(chars.shape[1])\n        for l in range(len(self.cells)): # for each layer\n            cell = self.cells[l]\n            outputs = []\n            # h_zero, c_zero\n            state = (tf.zeros((batch_size, self.rnn_cell_sizes[l])),\n                     tf.zeros((batch_size, self.rnn_cell_sizes[l])))\n            # Unstack the inputs to obtain a list of batches, one for each time step.\n            chars = tf.unstack(chars, axis=0)\n            for ch in chars: # for each time stamp\n                output, state = cell(ch, state)\n                outputs.append(output)\n            # The outputs of this layer are the inputs of the subsequent layer.\n            # [t, b, h]\n            chars = tf.stack(outputs, axis=0)\n            if training:\n                chars = tf.nn.dropout(chars, self.keep_prob)\n        # Extract the correct output (i.e., hidden state) for each example. All the\n        # character sequences in this batch were padded to the same fixed length so\n        # that they could be easily fed through the above RNN loop. The\n        # `sequence_length` vector tells us the true lengths of the character\n        # sequences, letting us obtain for each sequence the hidden state that was\n        # generated by its non-padding characters.\n        batch_range = [i for i in range(batch_size)]\n        # stack [64] with [64] => [64, 2]\n        indices = tf.stack([sequence_length - 1, batch_range], axis=1)\n        # [t, b, h]\n        # print(chars.shape)\n        hidden_states = tf.gather_nd(chars, indices)\n        # print(hidden_states.shape)\n        return self.relu(hidden_states)'"
10-ColorBot/utils.py,10,"b'import  os, six, time\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nimport  urllib\n\n\ndef parse(line):\n    """"""\n    Parse a line from the colors dataset.\n    """"""\n\n    # Each line of the dataset is comma-separated and formatted as\n    #    color_name, r, g, b\n    # so `items` is a list [color_name, r, g, b].\n    items = tf.string_split([line], "","").values\n    rgb = tf.strings.to_number(items[1:], out_type=tf.float32) / 255.\n    # Represent the color name as a one-hot encoded character sequence.\n    color_name = items[0]\n    chars = tf.one_hot(tf.io.decode_raw(color_name, tf.uint8), depth=256)\n    # The sequence length is needed by our RNN.\n    length = tf.cast(tf.shape(chars)[0], dtype=tf.int64)\n    return rgb, chars, length\n\n\ndef maybe_download(filename, work_directory, source_url):\n    """"""\n    Download the data from source url, unless it\'s already here.\n    Args:\n      filename: string, name of the file in the directory.\n      work_directory: string, path to working directory.\n      source_url: url to download from if file doesn\'t exist.\n    Returns:\n      Path to resulting file.\n    """"""\n    if not tf.io.gfile.exists(work_directory):\n        tf.io.gfile.makedirs(work_directory)\n    filepath = os.path.join(work_directory, filename)\n    if not tf.io.gfile.exists(filepath):\n        temp_file_name, _ = urllib.request.urlretrieve(source_url)\n        tf.io.gfile.copy(temp_file_name, filepath)\n        with tf.io.gfile.GFile(filepath) as f:\n            size = f.size()\n            print(""Successfully downloaded"", filename, size, ""bytes."")\n    return filepath\n\n\ndef load_dataset(data_dir, url, batch_size):\n    """"""Loads the colors data at path into a PaddedDataset.""""""\n\n    # Downloads data at url into data_dir/basename(url). The dataset has a header\n    # row (color_name, r, g, b) followed by comma-separated lines.\n    path = maybe_download(os.path.basename(url), data_dir, url)\n\n    # This chain of commands loads our data by:\n    #   1. skipping the header; (.skip(1))\n    #   2. parsing the subsequent lines; (.map(parse))\n    #   3. shuffling the data; (.shuffle(...))\n    #   3. grouping the data into padded batches (.padded_batch(...)).\n    dataset = tf.data.TextLineDataset(path).skip(1).map(parse).shuffle(\n                buffer_size=10000).padded_batch(\n                batch_size, padded_shapes=([None], [None, None], []))\n    return dataset'"
11-AE/main.py,16,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n\n\n\n# In[19]:\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n# image grid\nnew_im = Image.new(\'L\', (280, 280))\n\nimage_size = 28*28\nh_dim = 20\nnum_epochs = 55\nbatch_size = 100\nlearning_rate = 1e-3\n\n\nclass AE(tf.keras.Model):\n\n    def __init__(self):\n        super(AE, self).__init__()\n\n        # 784 => 512\n        self.fc1 = keras.layers.Dense(512)\n        # 512 => h\n        self.fc2 = keras.layers.Dense(h_dim)\n\n        # h => 512\n        self.fc3 = keras.layers.Dense(512)\n        # 512 => image\n        self.fc4 = keras.layers.Dense(image_size)\n\n    def encode(self, x):\n        x = tf.nn.relu(self.fc1(x))\n        x = (self.fc2(x))\n        return x\n\n\n\n    def decode_logits(self, h):\n        x = tf.nn.relu(self.fc3(h))\n        x = self.fc4(x)\n\n        return x\n\n    def decode(self, h):\n        return tf.nn.sigmoid(self.decode_logits(h))\n\n    def call(self, inputs, training=None, mask=None):\n        # encoder\n        h = self.encode(inputs)\n        # decode\n        x_reconstructed_logits = self.decode_logits(h)\n\n        return x_reconstructed_logits\n\n\nmodel = AE()\nmodel.build(input_shape=(4, image_size))\nmodel.summary()\noptimizer = keras.optimizers.Adam(learning_rate)\n\n# we do not need label\ndataset = tf.data.Dataset.from_tensor_slices(x_train)\ndataset = dataset.shuffle(batch_size * 5).batch(batch_size)\n\nnum_batches = x_train.shape[0] // batch_size\n\nfor epoch in range(num_epochs):\n\n    for step, x in enumerate(dataset):\n\n        x = tf.reshape(x, [-1, image_size])\n\n        with tf.GradientTape() as tape:\n\n            # Forward pass\n            x_reconstruction_logits = model(x)\n\n            # Compute reconstruction loss and kl divergence\n            # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n            # Scaled by `image_size` for each individual pixel.\n            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_reconstruction_logits)\n            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size\n\n        gradients = tape.gradient(reconstruction_loss, model.trainable_variables) \n        gradients,_ = tf.clip_by_global_norm(gradients, 15)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        if (step + 1) % 50 == 0:\n            print(""Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}""\n                  .format(epoch + 1, num_epochs, step + 1, num_batches, float(reconstruction_loss)))\n\n\n\n     # Save the reconstructed images of last batch\n    out_logits = model(x[:batch_size // 2])\n    out = tf.nn.sigmoid(out_logits)  # out is just the logits, use sigmoid\n    out = tf.reshape(out, [-1, 28, 28]).numpy() * 255\n\n    x = tf.reshape(x[:batch_size // 2], [-1, 28, 28])\n\n    x_concat = tf.concat([x, out], axis=0).numpy() * 255.\n    x_concat = x_concat.astype(np.uint8)\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = x_concat[index]\n            im = Image.fromarray(im, mode=\'L\')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(\'images/vae_reconstructed_epoch_%d.png\' % (epoch + 1))\n    plt.imshow(np.asarray(new_im))\n    plt.show()\n    print(\'New images saved !\')\n\n\n'"
12-VAE/main.py,23,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32)/255., x_test.astype(np.float32)/255.\n\n\n\n# In[19]:\n\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n# image grid\nnew_im = Image.new(\'L\', (280, 280))\n\nimage_size = 28*28\nh_dim = 512\nz_dim = 20\nnum_epochs = 55\nbatch_size = 100\nlearning_rate = 1e-3\n\n\nclass VAE(tf.keras.Model):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # input => h\n        self.fc1 = keras.layers.Dense(h_dim)\n        # h => mu and variance\n        self.fc2 = keras.layers.Dense(z_dim)\n        self.fc3 = keras.layers.Dense(z_dim)\n\n        # sampled z => h\n        self.fc4 = keras.layers.Dense(h_dim)\n        # h => image\n        self.fc5 = keras.layers.Dense(image_size)\n\n    def encode(self, x):\n        h = tf.nn.relu(self.fc1(x))\n        # mu, log_variance\n        return self.fc2(h), self.fc3(h)\n\n    def reparameterize(self, mu, log_var):\n        """"""\n        reparametrize trick\n        :param mu:\n        :param log_var:\n        :return:\n        """"""\n        std = tf.exp(log_var * 0.5)\n        eps = tf.random.normal(std.shape)\n\n        return mu + eps * std\n\n    def decode_logits(self, z):\n        h = tf.nn.relu(self.fc4(z))\n        return self.fc5(h)\n\n    def decode(self, z):\n        return tf.nn.sigmoid(self.decode_logits(z))\n\n    def call(self, inputs, training=None, mask=None):\n        # encoder\n        mu, log_var = self.encode(inputs)\n        # sample\n        z = self.reparameterize(mu, log_var)\n        # decode\n        x_reconstructed_logits = self.decode_logits(z)\n\n        return x_reconstructed_logits, mu, log_var\n\n\nmodel = VAE()\nmodel.build(input_shape=(4, image_size))\nmodel.summary()\noptimizer = keras.optimizers.Adam(learning_rate)\n\n# we do not need label\ndataset = tf.data.Dataset.from_tensor_slices(x_train)\ndataset = dataset.shuffle(batch_size * 5).batch(batch_size)\n\nnum_batches = x_train.shape[0] // batch_size\n\nfor epoch in range(num_epochs):\n\n    for step, x in enumerate(dataset):\n\n        x = tf.reshape(x, [-1, image_size])\n\n        with tf.GradientTape() as tape:\n\n            # Forward pass\n            x_reconstruction_logits, mu, log_var = model(x)\n\n            # Compute reconstruction loss and kl divergence\n            # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n            # Scaled by `image_size` for each individual pixel.\n            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_reconstruction_logits)\n            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size\n            # please refer to\n            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n            kl_div = - 0.5 * tf.reduce_sum(1. + log_var - tf.square(mu) - tf.exp(log_var), axis=-1)\n            kl_div = tf.reduce_mean(kl_div)\n\n            # Backprop and optimize\n            loss = tf.reduce_mean(reconstruction_loss) + kl_div\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        for g in gradients:\n            tf.clip_by_norm(g, 15)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n        if (step + 1) % 50 == 0:\n            print(""Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}""\n                  .format(epoch + 1, num_epochs, step + 1, num_batches, float(reconstruction_loss), float(kl_div)))\n\n\n\n    # Generative model\n    z = tf.random.normal((batch_size, z_dim))\n    out = model.decode(z)  # decode with sigmoid\n    out = tf.reshape(out, [-1, 28, 28]).numpy() * 255\n    out = out.astype(np.uint8)\n\n\n    # since we can not find image_grid function from vesion 2.0\n    # we do it by hand.\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = out[index]\n            im = Image.fromarray(im, mode=\'L\')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(\'images/vae_sampled_epoch_%d.png\' % (epoch + 1))\n    plt.imshow(np.asarray(new_im))\n    plt.show()\n\n    # Save the reconstructed images of last batch\n    out_logits, _, _ = model(x[:batch_size // 2])\n    out = tf.nn.sigmoid(out_logits)  # out is just the logits, use sigmoid\n    out = tf.reshape(out, [-1, 28, 28]).numpy() * 255\n\n    x = tf.reshape(x[:batch_size // 2], [-1, 28, 28])\n\n    x_concat = tf.concat([x, out], axis=0).numpy() * 255.\n    x_concat = x_concat.astype(np.uint8)\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = x_concat[index]\n            im = Image.fromarray(im, mode=\'L\')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(\'images/vae_reconstructed_epoch_%d.png\' % (epoch + 1))\n    plt.imshow(np.asarray(new_im))\n    plt.show()\n    print(\'New images saved !\')\n\n\n'"
13-DCGAN/gan.py,7,"b""import  tensorflow as tf\nfrom    tensorflow import keras\n\n\nclass Generator(keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.n_f = 512\n        self.n_k = 4\n\n        # input z vector is [None, 100]\n        self.dense1 = keras.layers.Dense(3 * 3 * self.n_f)\n        self.conv2 = keras.layers.Conv2DTranspose(self.n_f // 2, 3, 2, 'valid')\n        self.bn2 = keras.layers.BatchNormalization()\n        self.conv3 = keras.layers.Conv2DTranspose(self.n_f // 4, self.n_k, 2, 'same')\n        self.bn3 = keras.layers.BatchNormalization()\n        self.conv4 = keras.layers.Conv2DTranspose(1, self.n_k, 2, 'same')\n        return\n\n    def call(self, inputs, training=None):\n        # [b, 100] => [b, 3, 3, 512]\n        x = tf.nn.leaky_relu(tf.reshape(self.dense1(inputs), shape=[-1, 3, 3, self.n_f]))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n        x = tf.tanh(self.conv4(x))\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.n_f = 64\n        self.n_k = 4\n\n        # input image is [-1, 28, 28, 1]\n        self.conv1 = keras.layers.Conv2D(self.n_f, self.n_k, 2, 'same')\n        self.conv2 = keras.layers.Conv2D(self.n_f * 2, self.n_k, 2, 'same')\n        self.bn2 = keras.layers.BatchNormalization()\n        self.conv3 = keras.layers.Conv2D(self.n_f * 4, self.n_k, 2, 'same')\n        self.bn3 = keras.layers.BatchNormalization()\n        self.flatten4 = keras.layers.Flatten()\n        self.dense4 = keras.layers.Dense(1)\n        return\n\n    def call(self, inputs, training=None):\n        x = tf.nn.leaky_relu(self.conv1(inputs))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n        x = self.dense4(self.flatten4(x))\n        return x"""
13-DCGAN/main.py,11,"b""import  os\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    scipy.misc import toimage\n\nfrom    gan import Generator, Discriminator\n\n\n\ndef save_result(val_out, val_block_size, image_fn, color_mode):\n    def preprocess(img):\n        img = ((img + 1.0) * 127.5).astype(np.uint8)\n        return img\n\n    preprocesed = preprocess(val_out)\n    final_image = np.array([])\n    single_row = np.array([])\n    for b in range(val_out.shape[0]):\n        # concat image into a row\n        if single_row.size == 0:\n            single_row = preprocesed[b, :, :, :]\n        else:\n            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n\n        # concat image row to final_image\n        if (b+1) % val_block_size == 0:\n            if final_image.size == 0:\n                final_image = single_row\n            else:\n                final_image = np.concatenate((final_image, single_row), axis=0)\n\n            # reset single row\n            single_row = np.array([])\n\n    if final_image.shape[2] == 1:\n        final_image = np.squeeze(final_image, axis=2)\n    toimage(final_image, mode=color_mode).save(image_fn)\n\n\n# shorten sigmoid cross entropy loss calculation\ndef celoss_ones(logits, smooth=0.0):\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n                                                                  labels=tf.ones_like(logits)*(1.0 - smooth)))\n\n\ndef celoss_zeros(logits, smooth=0.0):\n    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n                                                                  labels=tf.zeros_like(logits)*(1.0 - smooth)))\n\n\n\n\ndef d_loss_fn(generator, discriminator, input_noise, real_image, is_trainig):\n    fake_image = generator(input_noise, is_trainig)\n    d_real_logits = discriminator(real_image, is_trainig)\n    d_fake_logits = discriminator(fake_image, is_trainig)\n\n    d_loss_real = celoss_ones(d_real_logits, smooth=0.1)\n    d_loss_fake = celoss_zeros(d_fake_logits, smooth=0.0)\n    loss = d_loss_real + d_loss_fake\n    return loss\n\n\ndef g_loss_fn(generator, discriminator, input_noise, is_trainig):\n    fake_image = generator(input_noise, is_trainig)\n    d_fake_logits = discriminator(fake_image, is_trainig)\n    loss = celoss_ones(d_fake_logits, smooth=0.1)\n    return loss\n\n\n\n\n\ndef main():\n\n    tf.random.set_seed(22)\n    np.random.seed(22)\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    assert tf.__version__.startswith('2.')\n\n\n    # hyper parameters\n    z_dim = 100\n    epochs = 3000000\n    batch_size = 128\n    learning_rate = 0.0002\n    is_training = True\n\n    # for validation purpose\n    assets_dir = './images'\n    if not os.path.isdir(assets_dir):\n        os.makedirs(assets_dir)\n    val_block_size = 10\n    val_size = val_block_size * val_block_size\n\n    # load mnist data\n    (x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n    x_train = x_train.astype(np.float32) / 255.\n    db = tf.data.Dataset.from_tensor_slices(x_train).shuffle(batch_size*4).batch(batch_size).repeat()\n    db_iter = iter(db)\n    inputs_shape = [-1, 28, 28, 1]\n\n\n    # create generator & discriminator\n    generator = Generator()\n    generator.build(input_shape=(batch_size, z_dim))\n    generator.summary()\n    discriminator = Discriminator()\n    discriminator.build(input_shape=(batch_size, 28, 28, 1))\n    discriminator.summary()\n\n    # prepare optimizer\n    d_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n    g_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n\n\n    for epoch in range(epochs):\n\n\n        # no need labels\n        batch_x = next(db_iter)\n\n        # rescale images to -1 ~ 1\n        batch_x = tf.reshape(batch_x, shape=inputs_shape)\n        # -1 - 1\n        batch_x = batch_x * 2.0 - 1.0\n\n        # Sample random noise for G\n        batch_z = tf.random.uniform(shape=[batch_size, z_dim], minval=-1., maxval=1.)\n\n\n        with tf.GradientTape() as tape:\n            d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n        grads = tape.gradient(d_loss, discriminator.trainable_variables)\n        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n\n        with tf.GradientTape() as tape:\n            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n        grads = tape.gradient(g_loss, generator.trainable_variables)\n        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n\n\n\n        if epoch % 100 == 0:\n\n            print(epoch, 'd loss:', float(d_loss), 'g loss:', float(g_loss))\n\n            # validation results at every epoch\n            val_z = np.random.uniform(-1, 1, size=(val_size, z_dim))\n            fake_image = generator(val_z, training=False)\n            image_fn = os.path.join('images', 'gan-val-{:03d}.png'.format(epoch + 1))\n            save_result(fake_image.numpy(), val_block_size, image_fn, color_mode='L')\n\n\n\n\nif __name__ == '__main__':\n    main()"""
14-Pixel2Pixel/gd.py,12,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\n\n\nclass Downsample(keras.Model):\n\n    def __init__(self, filters, size, apply_batchnorm=True):\n        super(Downsample, self).__init__()\n        \n        self.apply_batchnorm = apply_batchnorm\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.conv1 = keras.layers.Conv2D(filters,\n                                            (size, size),\n                                            strides=2,\n                                            padding='same',\n                                            kernel_initializer=initializer,\n                                            use_bias=False)\n        if self.apply_batchnorm:\n            self.batchnorm = keras.layers.BatchNormalization()\n\n    def call(self, x, training):\n        x = self.conv1(x)\n        if self.apply_batchnorm:\n            x = self.batchnorm(x, training=training)\n        x = tf.nn.leaky_relu(x)\n        return x\n\n\nclass Upsample(keras.Model):\n\n    def __init__(self, filters, size, apply_dropout=False):\n        super(Upsample, self).__init__()\n        \n        self.apply_dropout = apply_dropout\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.up_conv = keras.layers.Conv2DTranspose(filters,\n                                                       (size, size),\n                                                       strides=2,\n                                                       padding='same',\n                                                       kernel_initializer=initializer,\n                                                       use_bias=False)\n        self.batchnorm = keras.layers.BatchNormalization()\n        if self.apply_dropout:\n            self.dropout = keras.layers.Dropout(0.5)\n\n    def call(self, x1, x2, training=None):\n\n        x = self.up_conv(x1)\n        x = self.batchnorm(x, training=training)\n        if self.apply_dropout:\n            x = self.dropout(x, training=training)\n        x = tf.nn.relu(x)\n        x = tf.concat([x, x2], axis=-1)\n        return x\n\n\nclass Generator(keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.down1 = Downsample(64, 4, apply_batchnorm=False)\n        self.down2 = Downsample(128, 4)\n        self.down3 = Downsample(256, 4)\n        self.down4 = Downsample(512, 4)\n        self.down5 = Downsample(512, 4)\n        self.down6 = Downsample(512, 4)\n        self.down7 = Downsample(512, 4)\n        self.down8 = Downsample(512, 4)\n\n        self.up1 = Upsample(512, 4, apply_dropout=True)\n        self.up2 = Upsample(512, 4, apply_dropout=True)\n        self.up3 = Upsample(512, 4, apply_dropout=True)\n        self.up4 = Upsample(512, 4)\n        self.up5 = Upsample(256, 4)\n        self.up6 = Upsample(128, 4)\n        self.up7 = Upsample(64, 4)\n\n        self.last = keras.layers.Conv2DTranspose(3, (4, 4),\n                                                    strides=2,\n                                                    padding='same',\n                                                    kernel_initializer=initializer)\n\n\n    def call(self, x, training=None):\n\n        # x shape == (bs, 256, 256, 3)    \n        x1 = self.down1(x, training=training)  # (bs, 128, 128, 64)\n        x2 = self.down2(x1, training=training)  # (bs, 64, 64, 128)\n        x3 = self.down3(x2, training=training)  # (bs, 32, 32, 256)\n        x4 = self.down4(x3, training=training)  # (bs, 16, 16, 512)\n        x5 = self.down5(x4, training=training)  # (bs, 8, 8, 512)\n        x6 = self.down6(x5, training=training)  # (bs, 4, 4, 512)\n        x7 = self.down7(x6, training=training)  # (bs, 2, 2, 512)\n        x8 = self.down8(x7, training=training)  # (bs, 1, 1, 512)\n\n        x9 = self.up1(x8, x7, training=training)  # (bs, 2, 2, 1024)\n        x10 = self.up2(x9, x6, training=training)  # (bs, 4, 4, 1024)\n        x11 = self.up3(x10, x5, training=training)  # (bs, 8, 8, 1024)\n        x12 = self.up4(x11, x4, training=training)  # (bs, 16, 16, 1024)\n        x13 = self.up5(x12, x3, training=training)  # (bs, 32, 32, 512)\n        x14 = self.up6(x13, x2, training=training)  # (bs, 64, 64, 256)\n        x15 = self.up7(x14, x1, training=training)  # (bs, 128, 128, 128)\n\n        x16 = self.last(x15)  # (bs, 256, 256, 3)\n        x16 = tf.nn.tanh(x16)\n\n        return x16\n\n\nclass DiscDownsample(keras.Model):\n\n    def __init__(self, filters, size, apply_batchnorm=True):\n        super(DiscDownsample, self).__init__()\n\n        self.apply_batchnorm = apply_batchnorm\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.conv1 = keras.layers.Conv2D(filters, (size, size),\n                                            strides=2,\n                                            padding='same',\n                                            kernel_initializer=initializer,\n                                            use_bias=False)\n        if self.apply_batchnorm:\n            self.batchnorm = keras.layers.BatchNormalization()\n\n    def call(self, x, training=None):\n\n        x = self.conv1(x)\n        if self.apply_batchnorm:\n            x = self.batchnorm(x, training=training)\n        x = tf.nn.leaky_relu(x)\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.down1 = DiscDownsample(64, 4, False)\n        self.down2 = DiscDownsample(128, 4)\n        self.down3 = DiscDownsample(256, 4)\n\n        # we are zero padding here with 1 because we need our shape to \n        # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n        self.zero_pad1 = keras.layers.ZeroPadding2D()\n        self.conv = keras.layers.Conv2D(512, (4, 4),\n                                           strides=1,\n                                           kernel_initializer=initializer,\n                                           use_bias=False)\n        self.batchnorm1 = keras.layers.BatchNormalization()\n\n        # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n        self.zero_pad2 = keras.layers.ZeroPadding2D()\n        self.last = keras.layers.Conv2D(1, (4, 4),\n                                           strides=1,\n                                           kernel_initializer=initializer)\n\n\n    def call(self, inputs, training=None):\n        inp, target = inputs\n\n        # concatenating the input and the target\n        x = tf.concat([inp, target], axis=-1)  # (bs, 256, 256, channels*2)\n        x = self.down1(x, training=training)  # (bs, 128, 128, 64)\n        x = self.down2(x, training=training)  # (bs, 64, 64, 128)\n        x = self.down3(x, training=training)  # (bs, 32, 32, 256)\n\n        x = self.zero_pad1(x)  # (bs, 34, 34, 256)\n        x = self.conv(x)  # (bs, 31, 31, 512)\n        x = self.batchnorm1(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.zero_pad2(x)  # (bs, 33, 33, 512)\n        # don't add a sigmoid activation here since\n        # the loss function expects raw logits.\n        x = self.last(x)  # (bs, 30, 30, 1)\n\n        return x"""
14-Pixel2Pixel/main.py,32,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nimport  time\nfrom    matplotlib import pyplot as plt\n\nfrom    gd import Discriminator, Generator\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\nbatch_size = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\n\n\npath_to_zip = keras.utils.get_file(\'facades.tar.gz\',\n                                  cache_subdir=os.path.abspath(\'.\'),\n                                  origin=\'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/facades.tar.gz\',\n                                  extract=True)\n\nPATH = os.path.join(os.path.dirname(path_to_zip), \'facades/\')\nprint(\'dataset path:\', PATH)\n\ndef load_image(image_file, is_train):\n    """"""\n    load and preprocess images\n    :param image_file:\n    :param is_train:\n    :return:\n    """"""\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n\n    w = image.shape[1]\n\n    w = w // 2\n    real_image = image[:, :w, :]\n    input_image = image[:, w:, :]\n\n    input_image = tf.cast(input_image, tf.float32)\n    real_image = tf.cast(real_image, tf.float32)\n\n    if is_train:\n        # random jittering\n\n        # resizing to 286 x 286 x 3\n        input_image = tf.image.resize(input_image, [286, 286])\n        real_image = tf.image.resize(real_image, [286, 286])\n\n        # randomly cropping to 256 x 256 x 3\n        stacked_image = tf.stack([input_image, real_image], axis=0)\n        cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n        input_image, real_image = cropped_image[0], cropped_image[1]\n\n        if np.random.random() > 0.5:\n            # random mirroring\n            input_image = tf.image.flip_left_right(input_image)\n            real_image = tf.image.flip_left_right(real_image)\n    else:\n        input_image = tf.image.resize(input_image, size=[IMG_HEIGHT, IMG_WIDTH])\n        real_image = tf.image.resize(real_image, size=[IMG_HEIGHT, IMG_WIDTH])\n\n    # normalizing the images to [-1, 1]\n    input_image = (input_image / 127.5) - 1\n    real_image = (real_image / 127.5) - 1\n\n    # [256, 256, 3], [256, 256, 3]\n    # print(input_image.shape, real_image.shape)\n\n    # => [256, 256, 6]\n    out = tf.concat([input_image, real_image], axis=2)\n\n    return out\n\n\n\n\n\ntrain_dataset = tf.data.Dataset.list_files(PATH+\'/train/*.jpg\')\n# The following snippet can not work, so load it hand by hand.\n# train_dataset = train_dataset.map(lambda x: load_image(x, True)).batch(1)\ntrain_iter = iter(train_dataset)\ntrain_data = []\nfor x in train_iter:\n    train_data.append(load_image(x, True))\ntrain_data = tf.stack(train_data, axis=0)\n# [800, 256, 256, 3]\nprint(\'train:\', train_data.shape)\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_data)\ntrain_dataset = train_dataset.shuffle(400).batch(1)\n\ntest_dataset = tf.data.Dataset.list_files(PATH+\'test/*.jpg\')\n# test_dataset = test_dataset.map(lambda x: load_image(x, False)).batch(1)\ntest_iter = iter(test_dataset)\ntest_data = []\nfor x in test_iter:\n    test_data.append(load_image(x, False))\ntest_data = tf.stack(test_data, axis=0)\n# [800, 256, 256, 3]\nprint(\'test:\', test_data.shape)\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_data)\ntest_dataset = test_dataset.shuffle(400).batch(1)\n\ngenerator = Generator()\ngenerator.build(input_shape=(batch_size, 256, 256, 3))\ngenerator.summary()\ndiscriminator = Discriminator()\ndiscriminator.build(input_shape=[(batch_size, 256, 256, 3), (batch_size, 256, 256, 3)])\ndiscriminator.summary()\n\ng_optimizer = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\nd_optimizer = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n\n\n\n\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    # [1, 30, 30, 1] with [1, 30, 30, 1]\n    # print(disc_real_output.shape, disc_generated_output.shape)\n    real_loss = keras.losses.binary_crossentropy(\n                    tf.ones_like(disc_real_output), disc_real_output, from_logits=True)\n\n    generated_loss = keras.losses.binary_crossentropy(\n                    tf.zeros_like(disc_generated_output), disc_generated_output, from_logits=True)\n\n    real_loss = tf.reduce_mean(real_loss)\n    generated_loss = tf.reduce_mean(generated_loss)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss\n\n\n\ndef generator_loss(disc_generated_output, gen_output, target):\n\n    LAMBDA = 100\n\n    gan_loss = keras.losses.binary_crossentropy(\n                tf.ones_like(disc_generated_output), disc_generated_output, from_logits=True)\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    gan_loss = tf.reduce_mean(gan_loss)\n\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss\n\n\n\ndef generate_images(model, test_input, tar, epoch):\n    # the training=True is intentional here since\n    # we want the batch statistics while running the model\n    # on the test dataset. If we use training=False, we will get\n    # the accumulated statistics learned from the training dataset\n    # (which we don\'t want)\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = [\'Input Image\', \'Ground Truth\', \'Predicted Image\']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis(\'off\')\n    plt.savefig(\'images/epoch%d.png\'%epoch)\n    print(\'saved images.\')\n    # plt.show()\n\n\n\n\n\n\n\n\ndef main():\n\n    epochs = 1000\n\n    for epoch in range(epochs):\n\n        start = time.time()\n\n        for step, inputs in enumerate(train_dataset):\n\n            input_image, target = tf.split(inputs, num_or_size_splits=[3, 3], axis=3)\n            # print(input_image.shape, target.shape)\n\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n                # get generated pixel2pixel image\n                gen_output = generator(input_image, training=True)\n                # fed real pixel2pixel image together with original image\n                disc_real_output = discriminator([input_image, target], training=True)\n                # fed generated/fake pixel2pixel image together with original image\n                disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n                gen_loss = generator_loss(disc_generated_output, gen_output, target)\n                disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n            generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n            g_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n\n            discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n            d_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n\n            if step% 100 == 0:\n                # print(disc_loss.shape, gen_loss.shape)\n                print(epoch, step, float(disc_loss), float(gen_loss))\n\n        if epoch % 1 == 0:\n\n            for inputs in test_dataset:\n                input_image, target = tf.split(inputs, num_or_size_splits=[3, 3], axis=3)\n                generate_images(generator, input_image, target, epoch)\n                break\n\n        print(\'Time taken for epoch {} is {} sec\\n\'.format(epoch + 1, time.time() - start))\n\n\n\n    for inputs in test_dataset:\n        input_image, target = tf.split(inputs, num_or_size_splits=[3, 3], axis=3)\n        generate_images(generator, input_image, target, 99999)\n        break\n\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
15-CycleGAN/main.py,15,"b'import  os\nimport  time\nimport  numpy as np\nimport  matplotlib.pyplot as plt\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\nfrom    model import Generator, Discriminator, cycle_consistency_loss, generator_loss, discriminator_loss\n\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\nlearning_rate = 0.0002\nbatch_size = 1  # Set batch size to 4 or 16 if training multigpu\nimg_size = 256\ncyc_lambda = 10\nepochs = 1000\n\n""""""### Load Datasets""""""\n\npath_to_zip = keras.utils.get_file(\'horse2zebra.zip\',\n                              cache_subdir=os.path.abspath(\'.\'),\n                              origin=\'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\',\n                              extract=True)\n\nPATH = os.path.join(os.path.dirname(path_to_zip), \'horse2zebra/\')\n\ntrainA_path = os.path.join(PATH, ""trainA"")\ntrainB_path = os.path.join(PATH, ""trainB"")\n\ntrainA_size = len(os.listdir(trainA_path))\ntrainB_size = len(os.listdir(trainB_path))\nprint(\'train A:\', trainA_size)\nprint(\'train B:\', trainB_size)\n\n\ndef load_image(image_file):\n    image = tf.io.read_file(image_file)\n    image = tf.image.decode_jpeg(image, channels=3)\n    # scale alread!\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize(image, [256, 256])\n    # image = (image / 127.5) - 1\n    image = image * 2 - 1\n\n    return image\n\n\ntrain_datasetA = tf.data.Dataset.list_files(PATH + \'trainA/*.jpg\', shuffle=False)\ntrain_datasetA = train_datasetA.shuffle(trainA_size).repeat(epochs)\ntrain_datasetA = train_datasetA.map(lambda x: load_image(x))\ntrain_datasetA = train_datasetA.batch(batch_size)\ntrain_datasetA = train_datasetA.prefetch(batch_size)\ntrain_datasetA = iter(train_datasetA)\n\ntrain_datasetB = tf.data.Dataset.list_files(PATH + \'trainB/*.jpg\', shuffle=False)\ntrain_datasetB = train_datasetB.shuffle(trainB_size).repeat(epochs)\ntrain_datasetB = train_datasetB.map(lambda x: load_image(x))\ntrain_datasetB = train_datasetB.batch(batch_size)\ntrain_datasetB = train_datasetB.prefetch(batch_size)\ntrain_datasetB = iter(train_datasetB)\n\na = next(train_datasetA)\nprint(\'img shape:\', a.shape, a.numpy().min(), a.numpy().max())\n\ndiscA = Discriminator()\ndiscB = Discriminator()\ngenA2B = Generator()\ngenB2A = Generator()\n\ndiscA_optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.5)\ndiscB_optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.5)\ngenA2B_optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.5)\ngenB2A_optimizer = keras.optimizers.Adam(learning_rate, beta_1=0.5)\n\n\n\ndef generate_images(A, B, B2A, A2B, epoch):\n    """"""\n\n    :param A:\n    :param B:\n    :param B2A:\n    :param A2B:\n    :param epoch:\n    :return:\n    """"""\n    plt.figure(figsize=(15, 15))\n    A = tf.reshape(A, [256, 256, 3]).numpy()\n    B = tf.reshape(B, [256, 256, 3]).numpy()\n    B2A = tf.reshape(B2A, [256, 256, 3]).numpy()\n    A2B = tf.reshape(A2B, [256, 256, 3]).numpy()\n    display_list = [A, B, A2B, B2A]\n\n    title = [\'A\', \'B\', \'A2B\', \'B2A\']\n    for i in range(4):\n        plt.subplot(2, 2, i + 1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis(\'off\')\n    plt.savefig(\'images/generated_%d.png\'%epoch)\n    plt.close()\n\n\ndef train(train_datasetA, train_datasetB, epochs, lsgan=True, cyc_lambda=10):\n\n    for epoch in range(epochs):\n        start = time.time()\n\n\n        with tf.GradientTape() as genA2B_tape, tf.GradientTape() as genB2A_tape, \\\n                tf.GradientTape() as discA_tape, tf.GradientTape() as discB_tape:\n            try:\n                # Next training minibatches, default size 1\n                trainA = next(train_datasetA)\n                trainB = next(train_datasetB)\n            except tf.errors.OutOfRangeError:\n                print(""Error, run out of data"")\n                break\n\n            genA2B_output = genA2B(trainA, training=True)\n            genB2A_output = genB2A(trainB, training=True)\n\n\n            discA_real_output = discA(trainA, training=True)\n            discB_real_output = discB(trainB, training=True)\n\n            discA_fake_output = discA(genB2A_output, training=True)\n            discB_fake_output = discB(genA2B_output, training=True)\n\n            reconstructedA = genB2A(genA2B_output, training=True)\n            reconstructedB = genA2B(genB2A_output, training=True)\n\n            # generate_images(reconstructedA, reconstructedB)\n\n            # Use history buffer of 50 for disc loss\n            discA_loss = discriminator_loss(discA_real_output, discA_fake_output, lsgan=lsgan)\n            discB_loss = discriminator_loss(discB_real_output, discB_fake_output, lsgan=lsgan)\n\n            genA2B_loss = generator_loss(discB_fake_output, lsgan=lsgan) + \\\n                          cycle_consistency_loss(trainA, trainB, reconstructedA, reconstructedB,\n                                                 cyc_lambda=cyc_lambda)\n            genB2A_loss = generator_loss(discA_fake_output, lsgan=lsgan) + \\\n                          cycle_consistency_loss(trainA, trainB, reconstructedA, reconstructedB,\n                                                 cyc_lambda=cyc_lambda)\n\n        genA2B_gradients = genA2B_tape.gradient(genA2B_loss, genA2B.trainable_variables)\n        genB2A_gradients = genB2A_tape.gradient(genB2A_loss, genB2A.trainable_variables)\n\n        discA_gradients = discA_tape.gradient(discA_loss, discA.trainable_variables)\n        discB_gradients = discB_tape.gradient(discB_loss, discB.trainable_variables)\n\n        genA2B_optimizer.apply_gradients(zip(genA2B_gradients, genA2B.trainable_variables))\n        genB2A_optimizer.apply_gradients(zip(genB2A_gradients, genB2A.trainable_variables))\n\n        discA_optimizer.apply_gradients(zip(discA_gradients, discA.trainable_variables))\n        discB_optimizer.apply_gradients(zip(discB_gradients, discB.trainable_variables))\n\n\n        if epoch % 40 == 0:\n            generate_images(trainA, trainB, genB2A_output, genA2B_output, epoch)\n\n            print(\'Time taken for epoch {} is {} sec\'.format(epoch + 1, time.time() - start))\n\n\n\n\nif __name__ == \'__main__\':\n    train(train_datasetA, train_datasetB, epochs=epochs, lsgan=True, cyc_lambda=cyc_lambda)\n'"
15-CycleGAN/model.py,36,"b'import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\n\n\n\nclass Encoder(keras.Model):\n\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        # Small variance in initialization helps with preventing colour inversion.\n        self.conv1 = keras.layers.Conv2D(32, kernel_size=7, strides=1,\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv2 = keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv3 = keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        \n        # TODO: replace Instance Normalization for batchnorm\n        self.bn1 = keras.layers.BatchNormalization()\n        self.bn2 = keras.layers.BatchNormalization()\n        self.bn3 = keras.layers.BatchNormalization()\n\n    def call(self, inputs, training=True):\n        x = tf.pad(inputs, [[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"")\n\n        x = self.conv1(x)\n        x = self.bn1(x, training=training)\n        # Implement instance norm to more closely match orig. paper (momentum=0.1)?\n        x = tf.nn.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x, training=training)\n        x = tf.nn.relu(x)\n\n        return x\n\n\nclass Residual(keras.Model):\n\n    def __init__(self):\n        super(Residual, self).__init__()\n\n        self.conv1 = keras.layers.Conv2D(128, kernel_size=3, strides=1,\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv2 = keras.layers.Conv2D(128, kernel_size=3, strides=1,\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        \n\n        # TODO: replace Instance Normalization for batchnorm\n        self.bn1 = keras.layers.BatchNormalization()\n        self.bn2 = keras.layers.BatchNormalization() \n\n    def call(self, inputs, training=True):\n        x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n\n        x = self.conv1(x)\n        x = self.bn1(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\n\n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n\n        x = tf.add(x, inputs)\n\n        return x\n\n\nclass Decoder(keras.Model):\n\n    def __init__(self):\n        super(Decoder, self).__init__()\n\n        self.conv1 = keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding=\'same\',\n                                                     kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv2 = keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\'same\',\n                                                     kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv3 = keras.layers.Conv2D(3, kernel_size=7, strides=1,\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n\n\n        # TODO: replace Instance Normalization for batchnorm\n        self.bn1 = keras.layers.BatchNormalization()\n        self.bn2 = keras.layers.BatchNormalization()\n        self.bn3 = keras.layers.BatchNormalization()\n\n    def call(self, inputs, training=True):\n        x = self.conv1(inputs)\n        x = self.bn1(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x, training=training)\n        x = tf.nn.relu(x)\n\n        x = tf.pad(x, [[0, 0], [3, 3], [3, 3], [0, 0]], ""REFLECT"")\n\n        x = self.conv3(x)\n        x = self.bn3(x, training=training)\n        x = tf.nn.tanh(x)\n\n        return x\n\n\nclass Generator(keras.Model):\n\n    def __init__(self, img_size=256, skip=False):\n        super(Generator, self).__init__()\n\n        self.img_size = img_size\n        self.skip = skip  # TODO: Add skip\n\n        self.encoder = Encoder()\n        if (img_size == 128):\n            self.res1 = Residual()\n            self.res2 = Residual()\n            self.res3 = Residual()\n            self.res4 = Residual()\n            self.res5 = Residual()\n            self.res6 = Residual()\n        else:\n            self.res1 = Residual()\n            self.res2 = Residual()\n            self.res3 = Residual()\n            self.res4 = Residual()\n            self.res5 = Residual()\n            self.res6 = Residual()\n            self.res7 = Residual()\n            self.res8 = Residual()\n            self.res9 = Residual()\n        self.decoder = Decoder()\n\n\n    def call(self, inputs, training=True):\n\n        x = self.encoder(inputs, training)\n        if (self.img_size == 128):\n            x = self.res1(x, training)\n            x = self.res2(x, training)\n            x = self.res3(x, training)\n            x = self.res4(x, training)\n            x = self.res5(x, training)\n            x = self.res6(x, training)\n        else:\n            x = self.res1(x, training)\n            x = self.res2(x, training)\n            x = self.res3(x, training)\n            x = self.res4(x, training)\n            x = self.res5(x, training)\n            x = self.res6(x, training)\n            x = self.res7(x, training)\n            x = self.res8(x, training)\n            x = self.res9(x, training)\n        x = self.decoder(x, training)\n\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.conv1 = keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv2 = keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv3 = keras.layers.Conv2D(256, kernel_size=4, strides=2, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv4 = keras.layers.Conv2D(512, kernel_size=4, strides=1, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n        self.conv5 = keras.layers.Conv2D(1, kernel_size=4, strides=1, padding=\'same\',\n                                            kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n\n        self.leaky = keras.layers.LeakyReLU(0.2)\n\n\n        # TODO: replace Instance Normalization for batchnorm\n        self.bn1 = keras.layers.BatchNormalization()\n        self.bn2 = keras.layers.BatchNormalization()\n        self.bn3 = keras.layers.BatchNormalization()\n\n\n    def call(self, inputs, training=True):\n        x = self.conv1(inputs)\n        x = self.leaky(x)\n\n        x = self.conv2(x)\n        x = self.bn1(x, training=training)\n        x = self.leaky(x)\n\n        x = self.conv3(x)\n        x = self.bn2(x, training=training)\n        x = self.leaky(x)\n\n        x = self.conv4(x)\n        x = self.bn3(x, training=training)\n        x = self.leaky(x)\n\n        x = self.conv5(x)\n        # x = tf.nn.sigmoid(x) # use_sigmoid = not lsgan\n        return x\n\n\n\n\n\n\ndef discriminator_loss(disc_of_real_output, disc_of_gen_output, lsgan=True):\n    if lsgan:  # Use least squares loss\n        # real_loss = tf.reduce_mean(tf.squared_difference(disc_of_real_output, 1))\n        real_loss = keras.losses.mean_squared_error(disc_of_real_output, tf.ones_like(disc_of_real_output))\n        generated_loss = tf.reduce_mean(tf.square(disc_of_gen_output))\n\n        total_disc_loss = (real_loss + generated_loss) * 0.5  # 0.5 slows down rate that D learns compared to G\n    else:  # Use vanilla GAN loss\n        raise NotImplementedError\n        real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(disc_of_real_output),\n                                                    logits=disc_of_real_output)\n        generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(disc_of_gen_output),\n                                                         logits=disc_of_gen_output)\n\n        total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss\n\n\ndef generator_loss(disc_of_gen_output, lsgan=True):\n    if lsgan:  # Use least squares loss\n        # gen_loss = tf.reduce_mean(tf.squared_difference(disc_of_gen_output, 1))\n        gen_loss = keras.losses.mean_squared_error(disc_of_gen_output, tf.ones_like(disc_of_gen_output))\n    else:  # Use vanilla GAN loss\n        raise NotImplementedError\n        gen_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(disc_generated_output),\n                                                   logits=disc_generated_output)\n        # l1_loss = tf.reduce_mean(tf.abs(target - gen_output)) # Look up pix2pix loss\n    return gen_loss\n\n\ndef cycle_consistency_loss(data_A, data_B, reconstructed_data_A, reconstructed_data_B, cyc_lambda=10):\n    loss = tf.reduce_mean(tf.abs(data_A - reconstructed_data_A) + tf.abs(data_B - reconstructed_data_B))\n    return cyc_lambda * loss\n'"
16-fasterRCNN/inspect_model.py,11,"b""import matplotlib\nimport matplotlib.pyplot as plt\n\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport visualize\n\n\n\n\nprint(tf.__version__)\n\n\n# tensorflow config - using one gpu and extending the GPU \n# memory region needed by the TensorFlow process\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n\n\n# #### load dataset\n\n# In[10]:\n\n\nfrom detection.datasets import coco, data_generator\n\n\n# In[11]:\n\n\nimg_mean = (123.675, 116.28, 103.53)\n# img_std = (58.395, 57.12, 57.375)\nimg_std = (1., 1., 1.)\n\n\n# In[12]:\n\n\ntrain_dataset = coco.CocoDataSet('/scratch/llong/datasets/coco2017/', 'train',\n                                 flip_ratio=0.5,\n                                 pad_mode='fixed',\n                                 mean=img_mean,\n                                 std=img_std,\n                                 scale=(800, 1024))\n\n\n# #### display a sample\n\n# In[13]:\n\n\nimg, img_meta, bboxes, labels = train_dataset[6]\nrgb_img = np.round(img + img_mean)\nprint('origin shape:', img_meta[:3])\nprint('image  shape:', img_meta[3:6])\nprint('pad    shape:', img_meta[6:9])\nprint('scale factor:', img_meta[9:10])\nprint('flip  yet   :', img_meta[10:11])\nprint('img    shape:', img.shape)\nprint('bboxes:', bboxes.shape)\nprint('labels:', labels)\nprint('label2name:', train_dataset.get_categories())\nprint('lenof classes:', len(train_dataset.get_categories()))\nnames = [train_dataset.get_categories()[i] for i in labels]\nprint('names:', names)\n# In[24]:\n\n\nplt.imshow(rgb_img.astype(np.int32))\nplt.show()\n\n# In[35]:\n\n\n\n\n# In[6]:\n\n\nvisualize.display_instances(rgb_img, bboxes, labels, train_dataset.get_categories())\n\n\n# #### load model\n\n# In[36]:\n\n\nfrom detection.models.detectors import faster_rcnn\n\nmodel = faster_rcnn.FasterRCNN(num_classes=len(train_dataset.get_categories()))\n\n\n# In[37]:\n\n\n# [1, 1024, 1024, 3]\nbatch_imgs = tf.Variable(np.expand_dims(img, 0))\n# [1, 11]\nbatch_metas = tf.Variable(np.expand_dims(img_meta, 0))\n# [1, nnum of boxes, 4]\nbatch_bboxes = tf.Variable(np.expand_dims(bboxes, 0))\n# [1, num of boxes]\nbatch_labels = tf.Variable(np.expand_dims(labels, 0))\n\n_ = model((batch_imgs, batch_metas, batch_bboxes, batch_labels), training=True)\n\n\n# In[38]:\n\n\nmodel.load_weights('weights/faster_rcnn.h5')\n\n\n# ### Stage 1: Region Proposal Network\n# \n# #### 1.a RPN Targets\n\n# In[39]:\n\n\nanchors, valid_flags = model.rpn_head.generator.generate_pyramid_anchors(batch_metas)\n\nrpn_target_matchs, rpn_target_deltas = model.rpn_head.anchor_target.build_targets(\n                anchors, valid_flags, batch_bboxes, batch_labels)\n\n\n# In[40]:\n\n\npositive_anchors = tf.gather(anchors, tf.where(tf.equal(rpn_target_matchs, 1))[:, 1])\nnegative_anchors = tf.gather(anchors, tf.where(tf.equal(rpn_target_matchs, -1))[:, 1])\nneutral_anchors = tf.gather(anchors, tf.where(tf.equal(rpn_target_matchs, 0))[:, 1])\npositive_target_deltas = rpn_target_deltas[0, :tf.where(tf.equal(rpn_target_matchs, 1)).shape[0]]\n\n\n# In[41]:\n\n\nfrom detection.core.bbox import transforms\n    \nrefined_anchors = transforms.delta2bbox(\n    positive_anchors, positive_target_deltas, (0., 0., 0., 0.), (0.1, 0.1, 0.2, 0.2))\n\n\n# In[45]:\n\n\nprint('rpn_target_matchs:\\t', rpn_target_matchs[0].shape.as_list())\nprint('rpn_target_deltas:\\t', rpn_target_deltas[0].shape.as_list())\nprint('positive_anchors:\\t', positive_anchors.shape.as_list())\nprint('negative_anchors:\\t', negative_anchors.shape.as_list())\nprint('neutral_anchors:\\t', neutral_anchors.shape.as_list())\nprint('refined_anchors:\\t', refined_anchors.shape.as_list())\n\n\n# In[44]:\n\n\nvisualize.draw_boxes(rgb_img, \n                     boxes=positive_anchors.numpy(), \n                     refined_boxes=refined_anchors.numpy())\nplt.show()\n\n\n# #### 1.b RPN Predictions\n\n# In[15]:\n\n\ntraining = False\nC2, C3, C4, C5 = model.backbone(batch_imgs, \n                                training=training)\n\nP2, P3, P4, P5, P6 = model.neck([C2, C3, C4, C5], \n                                training=training)\n\nrpn_feature_maps = [P2, P3, P4, P5, P6]\nrcnn_feature_maps = [P2, P3, P4, P5]\n\nrpn_class_logits, rpn_probs, rpn_deltas = model.rpn_head(\n    rpn_feature_maps, training=training)\n\n\n# In[16]:\n\n\nrpn_probs_tmp = rpn_probs[0, :, 1]\n\n\n# In[17]:\n\n\n# Show top anchors by score (before refinement)\nlimit = 100\nix = tf.nn.top_k(rpn_probs_tmp, k=limit).indices[::-1]\n\n\n# In[18]:\n\n\nvisualize.draw_boxes(rgb_img, boxes=tf.gather(anchors, ix).numpy())\n\n\n# ### Stage 2:  Proposal Classification\n\n# In[19]:\n\n\nproposals_list = model.rpn_head.get_proposals(\n    rpn_probs, rpn_deltas, batch_metas)\n\n\n# In[20]:\n\n\nrois_list = proposals_list\n\npooled_regions_list = model.roi_align(\n    (rois_list, rcnn_feature_maps, batch_metas), training=training)\n\nrcnn_class_logits_list, rcnn_probs_list, rcnn_deltas_list =     model.bbox_head(pooled_regions_list, training=training)\n\n\n# In[21]:\n\n\ndetections_list = model.bbox_head.get_bboxes(\n    rcnn_probs_list, rcnn_deltas_list, rois_list, batch_metas)\n\n\n# In[22]:\n\n\ntmp = detections_list[0][:, :4]\n\n\n# In[23]:\n\n\nvisualize.draw_boxes(rgb_img, boxes=tmp.numpy())\n\n\n# ### Stage 3: Run model directly\n\n# In[24]:\n\n\ndetections_list = model((batch_imgs, batch_metas), training=False)\ntmp = detections_list[0][:, :4]\nvisualize.draw_boxes(rgb_img, boxes=tmp.numpy())\n\n\n# ### Stage 4: Test (Detection)\n\n# In[25]:\n\n\nfrom detection.datasets.utils import get_original_image\nori_img = get_original_image(img, img_meta, img_mean)\n\n\n# In[26]:\n\n\nproposals = model.simple_test_rpn(img, img_meta)\n\n\n# In[27]:\n\n\nres = model.simple_test_bboxes(img, img_meta, proposals)\n\n\n# In[28]:\n\n\nvisualize.display_instances(ori_img, res['rois'], res['class_ids'], \n                            train_dataset.get_categories(), scores=res['scores'])\n\n"""
16-fasterRCNN/roi_test.py,8,"b""import tensorflow as tf\nimport matplotlib.pyplot as plt\n\nimg = plt.imread('/home/llong/Downloads/number.jpg') /255.\nimg2 = plt.imread('/home/llong/Downloads/number2.jpg') /255.\nimg = tf.convert_to_tensor(img, dtype=tf.float32)\nimg = tf.expand_dims(img, axis=0)\nimg = tf.image.resize(img, (1000,1000))\nimg2 = tf.convert_to_tensor(img2, dtype=tf.float32)\nimg2 = tf.expand_dims(img2, axis=0)\nimg2 = tf.image.resize(img2, (1000,1000))\n\nimg = tf.concat([img, img2], axis=0)\nprint('img:', img.shape)\n\na = tf.image.crop_and_resize(img, [[0.5, 0.5, 1.0, 1.0], [0.5, 0.5, 1.5, 1.5]], [0, 1], crop_size=(500, 500))\nprint('a:', a.shape)\n\nplt.subplot(2,2,1)\nplt.imshow(img[0])\nplt.subplot(2,2,2)\nplt.imshow(img[1])\nplt.subplot(2,2,3)\nplt.imshow(a[0])\nplt.subplot(2,2,4)\nplt.imshow(a[1])\nplt.show()\n"""
16-fasterRCNN/train_model.py,8,"b""import  os\nimport  tensorflow as tf\nfrom    tensorflow import keras\nimport  numpy as np\nfrom    matplotlib import pyplot as plt\nimport  visualize\n\nfrom    detection.datasets import coco, data_generator\nfrom    detection.datasets.utils import get_original_image\nfrom    detection.models.detectors import faster_rcnn\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nprint(tf.__version__)\nassert tf.__version__.startswith('2.')\ntf.random.set_seed(22)\nnp.random.seed(22)\n\nimg_mean = (123.675, 116.28, 103.53)\n# img_std = (58.395, 57.12, 57.375)\nimg_std = (1., 1., 1.)\nbatch_size = 1\ntrain_dataset = coco.CocoDataSet('/scratch/llong/datasets/coco2017/', 'train',\n                                 flip_ratio=0.5,\n                                 pad_mode='fixed',\n                                 mean=img_mean,\n                                 std=img_std,\n                                 scale=(800, 1216))\nnum_classes = len(train_dataset.get_categories())\ntrain_generator = data_generator.DataGenerator(train_dataset)\ntrain_tf_dataset = tf.data.Dataset.from_generator(\n    train_generator, (tf.float32, tf.float32, tf.float32, tf.int32))\ntrain_tf_dataset = train_tf_dataset.batch(batch_size).prefetch(100).shuffle(100)\n# train_tf_dataset = train_tf_dataset.padded_batch(\n#     batch_size, padded_shapes=([None, None, None], [None], [None, None], [None]))\n\nmodel = faster_rcnn.FasterRCNN(num_classes=num_classes)\noptimizer = keras.optimizers.SGD(1e-3, momentum=0.9, nesterov=True)\n\nimg, img_meta, bboxes, labels = train_dataset[6] # [N, 4], shape:[N]=data:[62]\nrgb_img = np.round(img + img_mean)\nori_img = get_original_image(img, img_meta, img_mean)\n# visualize.display_instances(rgb_img, bboxes, labels, train_dataset.get_categories())\n\n\nbatch_imgs = tf.convert_to_tensor(np.expand_dims(img, 0)) # [1, 1216, 1216, 3]\nbatch_metas = tf.convert_to_tensor(np.expand_dims(img_meta, 0)) # [1, 11]\n\n# dummpy forward to build network variables\n_ = model((batch_imgs, batch_metas), training=False)\n\nproposals = model.simple_test_rpn(img, img_meta)\nres = model.simple_test_bboxes(img, img_meta, proposals)\nvisualize.display_instances(ori_img, res['rois'], res['class_ids'],\n                            train_dataset.get_categories(), scores=res['scores'])\nplt.savefig('image_demo_random.png')\n\nmodel.load_weights('weights/faster_rcnn.h5', by_name=True)\n\nproposals = model.simple_test_rpn(img, img_meta)\nres = model.simple_test_bboxes(img, img_meta, proposals)\nvisualize.display_instances(ori_img, res['rois'], res['class_ids'],\n                            train_dataset.get_categories(), scores=res['scores'])\nplt.savefig('image_demo_ckpt.png')\n\nfor epoch in range(100):\n\n    loss_history = []\n    for (batch, inputs) in enumerate(train_tf_dataset):\n\n        batch_imgs, batch_metas, batch_bboxes, batch_labels = inputs\n        with tf.GradientTape() as tape:\n            rpn_class_loss, rpn_bbox_loss, rcnn_class_loss, rcnn_bbox_loss = model(\n                (batch_imgs, batch_metas, batch_bboxes, batch_labels), training=True)\n\n            loss_value = rpn_class_loss + rpn_bbox_loss + rcnn_class_loss + rcnn_bbox_loss\n\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        loss_history.append(loss_value.numpy())\n\n        if batch % 10 == 0:\n            print('epoch', epoch, batch, np.mean(loss_history))\n\n            # img, img_meta = batch_imgs[0].numpy(), batch_metas[0].numpy()\n            # ori_img = get_original_image(img, img_meta, img_mean)\n            # proposals = model.simple_test_rpn(img, img_meta)\n            # res = model.simple_test_bboxes(img, img_meta, proposals)\n            # visualize.display_instances(ori_img, res['rois'], res['class_ids'],\n            #                             train_dataset.get_categories(), scores=res['scores'])\n            # plt.savefig('images/%d-%d.png' % (epoch, batch))\n"""
16-fasterRCNN/visualize.py,0,"b'import os\nimport sys\nimport random\nimport itertools\nimport colorsys\n\nimport numpy as np\nfrom skimage.measure import find_contours\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import patches, lines\nfrom matplotlib.patches import Polygon\n# import IPython.display\n\ndef random_colors(N, bright=True):\n    \'\'\'\n    Generate random colors.\n    To get visually distinct colors, generate them in HSV space then\n    convert to RGB.\n    \'\'\'\n    brightness = 1.0 if bright else 0.7\n    hsv = [(i / N, 1, brightness) for i in range(N)]\n    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n    random.shuffle(colors)\n    return colors\n\ndef display_instances(image, boxes, class_ids, class_names,\n                      scores=None, title="""",\n                      figsize=(16, 16), ax=None):\n    \'\'\'\n    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n    class_ids: [num_instances]\n    class_names: list of class names of the dataset\n    scores: (optional) confidence scores for each box\n    figsize: (optional) the size of the image.\n    \'\'\'\n    # Number of instances\n    N = boxes.shape[0]\n    if not N:\n        print(""\\n*** No instances to display *** \\n"")\n    # else:\n    #     assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n\n    if not ax:\n        _, ax = plt.subplots(1, figsize=figsize)\n\n    # Generate random colors\n    colors = random_colors(N)\n\n    # Show area outside image boundaries.\n    height, width = image.shape[:2]\n    ax.set_ylim(height + 10, -10)\n    ax.set_xlim(-10, width + 10)\n    ax.axis(\'off\')\n    ax.set_title(title)\n\n    masked_image = image.astype(np.uint32).copy()\n    for i in range(N):\n        color = colors[i]\n\n        # Bounding box\n        if not np.any(boxes[i]):\n            # Skip this instance. Has no bbox. Likely lost in image cropping.\n            continue\n        y1, x1, y2, x2 = boxes[i]\n        y1, x1, y2, x2 = int(y1), int(x1), int(y2), int(x2)\n        p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                              alpha=0.7, linestyle=""dashed"",\n                              edgecolor=color, facecolor=\'none\')\n        ax.add_patch(p)\n\n        # Label\n        class_id = class_ids[i]\n        score = scores[i] if scores is not None else None\n        label = class_names[class_id]\n        x = random.randint(x1, (x1 + x2) // 2)\n        caption = ""{} {:.3f}"".format(label, score) if score else label\n        ax.text(x1, y1 + 8, caption,\n                color=\'w\', size=11, backgroundcolor=""none"")\n\n\n    plt.imshow(image.astype(np.uint8))\n    \ndef draw_boxes(image, boxes=None, refined_boxes=None,\n               captions=None, visibilities=None,\n               title="""", ax=None):\n    \'\'\'Draw bounding boxes and segmentation masks with differnt\n    customizations.\n    boxes: [N, (y1, x1, y2, x2, class_id)] in image coordinates.\n    refined_boxes: Like boxes, but draw with solid lines to show\n        that they\'re the result of refining \'boxes\'.\n    captions: List of N titles to display on each box\n    visibilities: (optional) List of values of 0, 1, or 2. Determine how\n        prominant each bounding box should be.\n    title: An optional title to show over the image\n    ax: (optional) Matplotlib axis to draw on.\n    \'\'\'\n    # Number of boxes\n    N = boxes.shape[0] if boxes is not None else refined_boxes.shape[0]\n\n    # Matplotlib Axis\n    if not ax:\n        _, ax = plt.subplots(1, figsize=(16, 16))\n\n    # Generate random colors\n    colors = random_colors(N)\n\n    # Show area outside image boundaries.\n    margin = image.shape[0] // 10\n    ax.set_ylim(image.shape[0] + margin, -margin)\n    ax.set_xlim(-margin, image.shape[1] + margin)\n    ax.axis(\'off\')\n\n    ax.set_title(title)\n\n    for i in range(N):\n        # Box visibility\n        visibility = visibilities[i] if visibilities is not None else 1\n        if visibility == 0:\n            color = ""gray""\n            style = ""dotted""\n            alpha = 0.5\n        elif visibility == 1:\n            color = colors[i]\n            style = ""dotted""\n            alpha = 1\n        elif visibility == 2:\n            color = colors[i]\n            style = ""solid""\n            alpha = 1\n\n        # Boxes\n        if boxes is not None:\n            if not np.any(boxes[i]):\n                # Skip this instance. Has no bbox. Likely lost in cropping.\n                continue\n            y1, x1, y2, x2 = boxes[i]\n            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n                                  alpha=alpha, linestyle=style,\n                                  edgecolor=color, facecolor=\'none\')\n            ax.add_patch(p)\n\n        # Refined boxes\n        if refined_boxes is not None and visibility > 0:\n            ry1, rx1, ry2, rx2 = refined_boxes[i].astype(np.int32)\n            p = patches.Rectangle((rx1, ry1), rx2 - rx1, ry2 - ry1, linewidth=2,\n                                  edgecolor=color, facecolor=\'none\')\n            ax.add_patch(p)\n            # Connect the top-left corners of the anchor and proposal\n            if boxes is not None:\n                ax.add_line(lines.Line2D([x1, rx1], [y1, ry1], color=color))\n\n        # Captions\n        if captions is not None:\n            caption = captions[i]\n            # If there are refined boxes, display captions on them\n            if refined_boxes is not None:\n                y1, x1, y2, x2 = ry1, rx1, ry2, rx2\n            x = random.randint(x1, (x1 + x2) // 2)\n            ax.text(x1, y1, caption, size=11, verticalalignment=\'top\',\n                    color=\'w\', backgroundcolor=""none"",\n                    bbox={\'facecolor\': color, \'alpha\': 0.5,\n                          \'pad\': 2, \'edgecolor\': \'none\'})\n\n    ax.imshow(image.astype(np.uint8))'"
17-A2C/a2c.py,8,"b'import gym\nimport logging\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.layers as kl\nimport tensorflow.keras.losses as kls\nimport tensorflow.keras.optimizers as ko\n\n\nclass ProbabilityDistribution(tf.keras.Model):\n    def call(self, logits):\n        # sample a random categorical action from given logits\n        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n\n\nclass Model(tf.keras.Model):\n    def __init__(self, num_actions):\n        super().__init__(\'mlp_policy\')\n        # no tf.get_variable(), just simple Keras API\n        self.hidden1 = kl.Dense(128, activation=\'relu\')\n        self.hidden2 = kl.Dense(128, activation=\'relu\')\n        self.value = kl.Dense(1, name=\'value\')\n        # logits are unnormalized log probabilities\n        self.logits = kl.Dense(num_actions, name=\'policy_logits\')\n        self.dist = ProbabilityDistribution()\n\n    def call(self, inputs):\n        # inputs is a numpy array, convert to Tensor\n        x = tf.convert_to_tensor(inputs)\n        # separate hidden layers from the same input tensor\n        hidden_logs = self.hidden1(x)\n        hidden_vals = self.hidden2(x)\n        return self.logits(hidden_logs), self.value(hidden_vals)\n\n    def action_value(self, obs):\n        # executes call() under the hood\n        logits, value = self.predict(obs)\n        action = self.dist.predict(logits)\n        # a simpler option, will become clear later why we don\'t use it\n        # action = tf.random.categorical(logits, 1)\n        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\n        \n        \nclass A2CAgent:\n    def __init__(self, model):\n        # hyperparameters for loss terms, gamma is the discount coefficient\n        self.params = {\n            \'gamma\': 0.99,\n            \'value\': 0.5,\n            \'entropy\': 0.0001\n        }\n        self.model = model\n        self.model.compile(\n            optimizer=ko.RMSprop(lr=0.0007),\n            # define separate losses for policy logits and value estimate\n            loss=[self._logits_loss, self._value_loss]\n        )\n    \n    def train(self, env, batch_sz=32, updates=1000):\n        # storage helpers for a single batch of data\n        actions = np.empty((batch_sz,), dtype=np.int32)\n        rewards, dones, values = np.empty((3, batch_sz))\n        observations = np.empty((batch_sz,) + env.observation_space.shape)\n        # training loop: collect samples, send to optimizer, repeat updates times\n        ep_rews = [0.0]\n        next_obs = env.reset()\n        for update in range(updates):\n            for step in range(batch_sz):\n                observations[step] = next_obs.copy()\n                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n\n                ep_rews[-1] += rewards[step]\n                if dones[step]:\n                    ep_rews.append(0.0)\n                    next_obs = env.reset()\n                    logging.info(""Episode: %03d, Reward: %03d"" % (len(ep_rews)-1, ep_rews[-2]))\n\n            _, next_value = self.model.action_value(next_obs[None, :])\n            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n            # a trick to input actions and advantages through same API\n            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n            # performs a full training step on the collected batch\n            # note: no need to mess around with gradients, Keras API handles it\n            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n            logging.debug(""[%d/%d] Losses: %s"" % (update+1, updates, losses))\n        return ep_rews\n\n    def test(self, env, render=False):\n        obs, done, ep_reward = env.reset(), False, 0\n        while not done:\n            action, _ = self.model.action_value(obs[None, :])\n            obs, reward, done, _ = env.step(action)\n            ep_reward += reward\n            if render:\n                env.render()\n        return ep_reward\n\n    def _returns_advantages(self, rewards, dones, values, next_value):\n        # next_value is the bootstrap value estimate of a future state (the critic)\n        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n        # returns are calculated as discounted sum of future rewards\n        for t in reversed(range(rewards.shape[0])):\n            returns[t] = rewards[t] + self.params[\'gamma\'] * returns[t+1] * (1-dones[t])\n        returns = returns[:-1]\n        # advantages are returns - baseline, value estimates in our case\n        advantages = returns - values\n        return returns, advantages\n    \n    def _value_loss(self, returns, value):\n        # value loss is typically MSE between value estimates and returns\n        return self.params[\'value\']*kls.mean_squared_error(returns, value)\n\n    def _logits_loss(self, acts_and_advs, logits):\n        # a trick to input actions and advantages through same API\n        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n        # sparse categorical CE loss obj that supports sample_weight arg on call()\n        # from_logits argument ensures transformation into normalized probabilities\n        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n        # policy loss is defined by policy gradients, weighted by advantages\n        # note: we only calculate the loss on the actions we\'ve actually taken\n        actions = tf.cast(actions, tf.int32)\n        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n        # entropy loss can be calculated via CE over itself\n        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n        # here signs are flipped because optimizer minimizes\n        return policy_loss - self.params[\'entropy\']*entropy_loss\n\n\nif __name__ == \'__main__\':\n    logging.getLogger().setLevel(logging.INFO)\n\n    env = gym.make(\'CartPole-v0\')\n    model = Model(num_actions=env.action_space.n)\n    agent = A2CAgent(model)\n    \n    rewards_history = agent.train(env)\n    print(""Finished training."")\n    print(""Total Episode Reward: %d out of 200"" % agent.test(env, True))\n    \n    plt.style.use(\'seaborn\')\n    plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'Total Reward\')\n    plt.show()\n'"
18-GPT/model.py,39,"b'import os\nimport math\nimport collections\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\ndef gelu(x):\n    return 0.5 * x * (1 + tf.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\ndef swish(x):\n    return x * tf.sigmoid(x)\n \nclass namespace():\n    pass\nargs = namespace()\nargs.n_ctx = 512\nargs.n_embd = 768\nargs.n_head = 12\nargs.n_layer = 12\nargs.embd_pdrop = 0.1\nargs.attn_pdrop = 0.1\nargs.resid_pdrop = 0.1\nargs.clf_pdrop = 0.1\nargs.l2 = 0.1\nargs.n_transfer = 12\nargs.lm_coef = 0.5\nargs.b1 = 0.9\nargs.b2 = 0.999\nargs.e = 1e-8\nargs.n_valid = 374\nargs.afn = gelu\n\nzeros_init = keras.initializers.Zeros()\nones_init = keras.initializers.Ones()\n\nclass LayerNorm(keras.Model):\n    """"""Construct a layernorm module in the OpenAI style (epsilon inside the square root).""""""\n\n    def __init__(self, n_state=768, e=1e-5):\n        super(LayerNorm, self).__init__()\n        self.g = self.add_weight(shape=[n_state], initializer=ones_init)\n        self.b = self.add_weight(shape=[n_state], initializer=zeros_init)\n        self.e = e\n    \n    def call(self, x):\n        u = tf.reduce_mean(x, -1, keepdims=True)\n        s = tf.reduce_mean(tf.pow(x-u, 2), -1, keepdims=True)\n        x = (x-u) / tf.sqrt(s+self.e)\n        return self.g * x + self.b\n\n\nclass Conv1D(keras.Model):\n\n    def __init__(self, nf=768*3, rf=1, nx=768):\n        super(Conv1D, self).__init__()\n        self.rf = rf\n        self.nf = nf\n        if rf == 1: # faster 1x1 conv\n            self.w = self.add_weight(shape=[nx,nf], initializer=keras.initializers.RandomNormal(stddev=0.02))\n            self.b = self.add_weight(shape=[nf], initializer=zeros_init)\n        else:\n            raise NotImplementedError\n\n    def call(self, x):\n        if self.rf == 1:\n            size_out = list(x.shape[:-1]) + [self.nf]\n            x = tf.matmul(tf.reshape(x, [-1, x.shape[-1]]), self.w) + self.b\n            x = tf.reshape(x, size_out)\n        else:\n            raise NotImplementedError\n        return x\n\n    \nclass Attention(keras.Model):\n    \n    def __init__(self, nx=768, n_ctx=512, cfg=args, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx # in Attention: n_state = 768 (nx=n_emb)\n         # [switch nx => n_state from Block to Attention to keep identical to openai implem]\n        assert n_state % cfg.n_head == 0\n        self.b = self.add_weight(shape=[1, 1, n_ctx, n_ctx], initializer=ones_init) # register buffer\n        self.b.assign(tf.linalg.LinearOperatorLowerTriangular(self.b).to_dense())\n        self.n_head = cfg.n_head\n        #self.split_size = n_state\n        self.scale = scale\n        self.c_attn = Conv1D(n_state*3, 1, nx)\n        self.c_proj = Conv1D(n_state, 1, nx)\n        self.attn_dropout = keras.layers.Dropout(cfg.attn_pdrop)\n        self.resid_dropout = keras.layers.Dropout(cfg.resid_pdrop)\n    \n    def _attn(self, q, k, v):\n        w = tf.matmul(q, k)\n        if self.scale:\n            w = w / tf.sqrt(tf.cast(v.shape[-1], tf.float32))\n        # self.b may be larger than w, so we need to crop it\n        b = self.b[:, :, :w.shape[-2], :w.shape[-1]]\n        w = w * b + 1e-9 * (1 - b)\n        w = tf.nn.softmax(w, -1)\n        return tf.matmul(w, v)\n    \n    def merge_heads(self, x):\n        x = tf.transpose(x, [0,2,1,3])\n        new_x_shape = list(x.shape[:-2]) + [x.shape[-2]*x.shape[-1]]\n        return tf.reshape(x, new_x_shape) # in openai implem: fct merge_states\n    \n    def split_heads(self, x, k=False):\n        new_x_shape = list(x.shape[:-1]) + [self.n_head, x.shape[-1]//self.n_head]\n        x = tf.reshape(x, new_x_shape) # in openai implem: fct split_states\n        if k:\n            return tf.transpose(x, [0,2,3,1])\n        else:\n            return tf.transpose(x, [0,2,1,3])\n    \n    def call(self, x):\n        x = self.c_attn(x)\n        query, key, value = tf.split(x, 3, axis=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n        return a\n\n\nclass MLP(keras.Model):\n\n    def __init__(self, n_state=3072, cfg=args): # n_state=3072 (4*n_embd)\n        super(MLP, self).__init__()\n        nx = cfg.n_embd\n        self.c_fc = Conv1D(n_state, 1, nx)\n        self.c_proj = Conv1D(nx, 1, n_state)\n        self.act = cfg.afn\n        self.dropout = keras.layers.Dropout(cfg.resid_pdrop)\n    \n    def call(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(keras.Model):\n\n    def __init__(self, n_ctx=512, cfg=args, scale=False):\n        super(Block, self).__init__()\n        nx = cfg.n_embd\n        self.attn = Attention(nx, n_ctx, cfg, scale)\n        self.ln_1 = LayerNorm(nx)\n        self.mlp = MLP(4 * nx, cfg)\n        self.ln_2 = LayerNorm(nx)\n    \n    def call(self, x):\n        a = self.attn(x)\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n        return h\n\n\nclass TransformerModel(keras.Model):\n    """""" Transformer model """"""\n\n    def __init__(self, cfg=args, vocab=40558, n_ctx=512):\n        super(TransformerModel, self).__init__()\n        self.vocab = vocab\n        self.embed = keras.layers.Embedding(vocab, cfg.n_embd)\n        self.embed.build([1])\n        self.drop = keras.layers.Dropout(cfg.embd_pdrop)\n        self.h = [Block(n_ctx, cfg, scale=True) for _ in range(cfg.n_layer)]\n\n    def call(self, x):\n        x = tf.reshape(x, [-1,x.shape[-2],x.shape[-1]])\n        e = self.drop(self.embed(x))\n        # add the position information to input embeddings\n        h = tf.reduce_sum(e, 2)\n        for block in self.h:\n            h = block(h)\n        return h\n\n\nclass LMHead(keras.Model):\n    """""" Language Model Head for the transformer """"""\n\n    def __init__(self, model, cfg=args, trunc_and_reshape=True):\n        super(LMHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        embed_shape = model.embed.weights[0].shape\n        self.embed = model.embed.weights[0]\n        self.decoder = lambda x: tf.matmul(x, tf.transpose(self.embed))\n        self.trunc_and_reshape = trunc_and_reshape  # XD\n\n    def call(self, h):\n        # Truncated Language modeling logits (we remove the last token)\n        h_trunc = tf.reshape(h[:, :-1], [-1, self.n_embd]) \\\n            if self.trunc_and_reshape else h  # XD\n        lm_logits = self.decoder(h_trunc)\n        return lm_logits\n\n\nclass MultipleChoiceHead(keras.Model):\n    """""" Classifier Head for the transformer """"""\n\n    def __init__(self, clf_token=40480, cfg=args):\n        super(MultipleChoiceHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.n_ctx = cfg.n_ctx\n        self.clf_token = clf_token\n        self.dropout = keras.layers.Dropout(cfg.clf_pdrop, [1, 2, cfg.n_embd, 1]) # might need to change the 1s to smth else\n        self.linear = keras.layers.Dense(1, input_shape=[cfg.n_embd], \n            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n        self.linear.build([cfg.n_embd])\n\n    def call(self, h, x):\n        # Classification logits\n        clf_h = tf.reshape(h, [-1, self.n_embd])\n        flat = tf.reshape(x[..., 0], [-1])\n        clf_h = tf.boolean_mask(clf_h, tf.equal(flat, self.clf_token))\n        clf_h = tf.reshape(clf_h, [-1, x.shape[1], self.n_embd, 1])\n        # This double transposition is there to replicate the behavior\n        # of the noise_shape argument in the tensorflow\n        # implementation.  For more details, see\n        # https://github.com/huggingface/pytorch-openai-transformer-lm/issues/11\n        # clf_h = self.dropout(clf_h.transpose(1, 2)).transpose(1, 2)\n        clf_h = self.dropout(clf_h)\n        clf_h = tf.reshape(clf_h, [-1, self.n_embd])\n        clf_logits = self.linear(clf_h)\n\n        return tf.reshape(clf_logits, [-1, x.shape[1]])\n\n\nclass ClfHead(keras.Model):\n    """"""Classification Head for the transformer\n\n    TODO: test this class.""""""\n    def __init__(self, clf_token=40480, cfg=args, n_class=10):\n        super(ClfHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.clf_token = clf_token\n        self.dropout = keras.layers.Dropout(cfg.clf_pdrop)\n        self.linear = keras.layers.Dense(n_class, input_shape=[cfg.n_embd], \n            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n\n    def call(self, h, x):\n        clf_h = tf.reshape(h, [-1, self.n_embd])\n        flat = tf.reshape(x[..., 0], [-1])\n        clf_h = clf_h[flat == self.clf_token, :]\n        clf_h = tf.boolean_mask(clf_h, tf.equal(flat, self.clf_token))\n        clf_h = self.dropout(clf_h)\n        clf_logits = self.linear(clf_h)\n\n        return clf_logits\n\n\nclass SimilarityHead(keras.Model):\n    """""" Similarity Head for the transformer\n\n        TODO: test this class.""""""\n    def __init__(self, clf_token=40480, cfg=args):\n        super(SimilarityHead, self).__init__()\n        self.n_embd = cfg.n_embd\n        self.clf_token = clf_token\n        self.dropout = keras.layers.Dropout(cfg.clf_pdrop)\n        self.linear = keras.layers.Dense(n_class, input_shape=[cfg.n_embd], \n            kernel_initializer=keras.initializers.RandomNormal(stddev=0.02), \n            bias_initializer=keras.initializers.RandomNormal(stddev=1))\n\n    def call(self, h, x):\n        sim_h = tf.reshape(h, [-1, self.n_embd])\n        flat = tf.reshape(x[..., 0], [-1])\n        sim_h = tf.boolean_mask(sim_h, tf.equal(flat, self.clf_token))\n        sim_h = self.dropout(sim_h)\n        sim_h = tf.reduce_sum(sim_h, 1)\n        sim_logits = self.linear(sim_h)\n\n        return sim_logits\n\n\nclass LMModel(keras.Model):\n    """""" Transformer with language model head only """"""\n    def __init__(self, cfg=args, vocab=40990, n_ctx=512, return_probs=False):\n        super(LMModel, self).__init__()\n        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n        self.lm_head = LMHead(self.transformer, cfg, trunc_and_reshape=False)\n        self.return_probs = return_probs\n        if self.return_probs:\n            pos_emb_mask = tf.zeros([1, 1, vocab]) # register buffer\n            pos_emb_mask[:, :, -n_ctx:] = -1e12\n\n    def call(self, x):\n        h = self.transformer(x)\n        lm_logits = self.lm_head(h)\n        if self.return_probs:\n            lm_logits = tf.nn.softmax(lm_logits + self.pos_emb_mask, -1)\n        return lm_logits\n\n\nclass DoubleHeadModel(keras.Model):\n    """""" Transformer with language model and task specific heads """"""\n    def __init__(self, cfg=args, clf_token=40480, task_head_type=\'multiple_choice\', vocab=40990, n_ctx=512):\n        super(DoubleHeadModel, self).__init__()\n        self.transformer = TransformerModel(cfg, vocab=vocab, n_ctx=n_ctx)\n        self.lm_head = LMHead(self.transformer, cfg)\n        if isinstance(task_head_type, str):\n            if task_head_type == \'multiple_choice\':\n                self.task_head = MultipleChoiceHead(clf_token, cfg)\n            elif task_head_type == \'similarity\':\n                self.task_head = SimilarityHead(clf_token, cfg)\n            elif task_head_type == \'inference\':\n                # the three classes correspond to entailment, contradiction and neutral.\n                self.task_head = ClfHead(clf_token, cfg, 3)\n            else:\n                raise ValueError(""task_head_type is expected to be \'multiple_choice\' ""\n                                 ""\'similarity\', \'inference\' or (\'classification\', n_class) ""\n                                 f""got {task_head_type}."")\n        elif isinstance(task_head_type, collections.abc.Sequence) and len(task_head_type) == 2 and \\\n             task_head_type[0] == \'classification\':\n            n_class = task_head_type[1]\n            self.task_head = ClfHead(clf_token, cfg, n_class)\n        else:\n            raise ValueError(""task_head_type is expected to be \'multiple_choice\' ""\n                             ""\'similarity\', \'inference\' or (\'classification\', n_class) ""\n                             f""got {task_head_type}."")\n\n    def call(self, x):\n        h = self.transformer(x)\n        lm_logits = self.lm_head(h)\n        task_logits = self.task_head(h, x)\n\n        return lm_logits, task_logits'"
19-BERT/__init__.py,0,b'from .bert import *\nfrom .loader import *\nfrom .tokenizer import Tokenizer\n'
19-BERT/bert.py,1,"b'import math\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\nimport numpy as np\nfrom keras_pos_embd import PositionEmbedding\nfrom keras_layer_normalization import LayerNormalization\nfrom keras_transformer import get_encoders\nfrom keras_transformer import get_custom_objects as get_encoder_custom_objects\nfrom layers import (get_inputs, get_embedding, TokenEmbedding, EmbeddingSimilarity, Masked, Extract)\n\n\n__all__ = [\n    \'TOKEN_PAD\', \'TOKEN_UNK\', \'TOKEN_CLS\', \'TOKEN_SEP\', \'TOKEN_MASK\',\n    \'gelu\', \'get_model\', \'get_custom_objects\', \'get_base_dict\', \'gen_batch_inputs\',\n]\n\n\nTOKEN_PAD = \'\'  # Token for padding\nTOKEN_UNK = \'[UNK]\'  # Token for unknown words\nTOKEN_CLS = \'[CLS]\'  # Token for classification\nTOKEN_SEP = \'[SEP]\'  # Token for separation\nTOKEN_MASK = \'[MASK]\'  # Token for masking\n\n\ndef gelu(x):\n    if K.backend() == \'tensorflow\':\n        return 0.5 * x * (1.0 + tf.math.erf(x / tf.sqrt(2.0)))\n    return 0.5 * x * (1.0 + K.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n\n\ndef get_model(token_num,\n              pos_num=512,\n              seq_len=512,\n              embed_dim=768,\n              transformer_num=12,\n              head_num=12,\n              feed_forward_dim=3072,\n              dropout_rate=0.1,\n              attention_activation=None,\n              feed_forward_activation=gelu,\n              custom_layers=None,\n              training=True,\n              trainable=None,\n              lr=1e-4):\n    """"""Get BERT model.\n\n    See: https://arxiv.org/pdf/1810.04805.pdf\n\n    :param token_num: Number of tokens.\n    :param pos_num: Maximum position.\n    :param seq_len: Maximum length of the input sequence or None.\n    :param embed_dim: Dimensions of embeddings.\n    :param transformer_num: Number of transformers.\n    :param head_num: Number of heads in multi-head attention in each transformer.\n    :param feed_forward_dim: Dimension of the feed forward layer in each transformer.\n    :param dropout_rate: Dropout rate.\n    :param attention_activation: Activation for attention layers.\n    :param feed_forward_activation: Activation for feed-forward layers.\n    :param custom_layers: A function that takes the embedding tensor and returns the tensor after feature extraction.\n                          Arguments such as `transformer_num` and `head_num` will be ignored if `custom_layer` is not\n                          `None`.\n    :param training: The built model will be returned if it is `True`, otherwise the input layers and the last feature\n                     extraction layer will be returned.\n    :param trainable: Whether the model is trainable.\n    :param lr: Learning rate.\n    :return: The compiled model.\n    """"""\n    if trainable is None:\n        trainable = training\n    inputs = get_inputs(seq_len=seq_len)\n    embed_layer, embed_weights = get_embedding(\n        inputs,\n        token_num=token_num,\n        embed_dim=embed_dim,\n        pos_num=pos_num,\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    transformed = embed_layer\n    if custom_layers is not None:\n        kwargs = {}\n        if keras.utils.generic_utils.has_arg(custom_layers, \'trainable\'):\n            kwargs[\'trainable\'] = trainable\n        transformed = custom_layers(transformed, **kwargs)\n    else:\n        transformed = get_encoders(\n            encoder_num=transformer_num,\n            input_layer=transformed,\n            head_num=head_num,\n            hidden_dim=feed_forward_dim,\n            attention_activation=attention_activation,\n            feed_forward_activation=feed_forward_activation,\n            dropout_rate=dropout_rate,\n            trainable=trainable,\n        )\n    if not training:\n        return inputs[:2], transformed\n    mlm_dense_layer = keras.layers.Dense(\n        units=embed_dim,\n        activation=feed_forward_activation,\n        trainable=trainable,\n        name=\'MLM-Dense\',\n    )(transformed)\n    mlm_norm_layer = LayerNormalization(name=\'MLM-Norm\')(mlm_dense_layer)\n    mlm_pred_layer = EmbeddingSimilarity(name=\'MLM-Sim\')([mlm_norm_layer, embed_weights])\n    masked_layer = Masked(name=\'MLM\')([mlm_pred_layer, inputs[-1]])\n    extract_layer = Extract(index=0, name=\'Extract\')(transformed)\n    nsp_dense_layer = keras.layers.Dense(\n        units=embed_dim,\n        activation=\'tanh\',\n        trainable=trainable,\n        name=\'NSP-Dense\',\n    )(extract_layer)\n    nsp_pred_layer = keras.layers.Dense(\n        units=2,\n        activation=\'softmax\',\n        trainable=trainable,\n        name=\'NSP\',\n    )(nsp_dense_layer)\n    model = keras.models.Model(inputs=inputs, outputs=[masked_layer, nsp_pred_layer])\n    model.compile(\n        optimizer=keras.optimizers.Adam(lr=lr),\n        loss=keras.losses.sparse_categorical_crossentropy,\n    )\n    return model\n\n\ndef get_custom_objects():\n    """"""Get all custom objects for loading saved models.""""""\n    custom_objects = get_encoder_custom_objects()\n    custom_objects[\'PositionEmbedding\'] = PositionEmbedding\n    custom_objects[\'TokenEmbedding\'] = TokenEmbedding\n    custom_objects[\'EmbeddingSimilarity\'] = EmbeddingSimilarity\n    custom_objects[\'Masked\'] = Masked\n    custom_objects[\'Extract\'] = Extract\n    custom_objects[\'gelu\'] = gelu\n    return custom_objects\n\n\ndef get_base_dict():\n    """"""Get basic dictionary containing special tokens.""""""\n    return {\n        TOKEN_PAD: 0,\n        TOKEN_UNK: 1,\n        TOKEN_CLS: 2,\n        TOKEN_SEP: 3,\n        TOKEN_MASK: 4,\n    }\n\n\ndef gen_batch_inputs(sentence_pairs,\n                     token_dict,\n                     token_list,\n                     seq_len=512,\n                     mask_rate=0.15,\n                     mask_mask_rate=0.8,\n                     mask_random_rate=0.1,\n                     swap_sentence_rate=0.5,\n                     force_mask=True):\n    """"""Generate a batch of inputs and outputs for training.\n\n    :param sentence_pairs: A list of pairs containing lists of tokens.\n    :param token_dict: The dictionary containing special tokens.\n    :param token_list: A list containing all tokens.\n    :param seq_len: Length of the sequence.\n    :param mask_rate: The rate of choosing a token for prediction.\n    :param mask_mask_rate: The rate of replacing the token to `TOKEN_MASK`.\n    :param mask_random_rate: The rate of replacing the token to a random word.\n    :param swap_sentence_rate: The rate of swapping the second sentences.\n    :param force_mask: At least one position will be masked.\n    :return: All the inputs and outputs.\n    """"""\n    batch_size = len(sentence_pairs)\n    base_dict = get_base_dict()\n    unknown_index = token_dict[TOKEN_UNK]\n    # Generate sentence swapping mapping\n    nsp_outputs = np.zeros((batch_size,))\n    mapping = {}\n    if swap_sentence_rate > 0.0:\n        indices = [index for index in range(batch_size) if np.random.random() < swap_sentence_rate]\n        mapped = indices[:]\n        np.random.shuffle(mapped)\n        for i in range(len(mapped)):\n            if indices[i] != mapped[i]:\n                nsp_outputs[indices[i]] = 1.0\n        mapping = {indices[i]: mapped[i] for i in range(len(indices))}\n    # Generate MLM\n    token_inputs, segment_inputs, masked_inputs = [], [], []\n    mlm_outputs = []\n    for i in range(batch_size):\n        first, second = sentence_pairs[i][0], sentence_pairs[mapping.get(i, i)][1]\n        segment_inputs.append(([0] * (len(first) + 2) + [1] * (seq_len - (len(first) + 2)))[:seq_len])\n        tokens = [TOKEN_CLS] + first + [TOKEN_SEP] + second + [TOKEN_SEP]\n        tokens = tokens[:seq_len]\n        tokens += [TOKEN_PAD] * (seq_len - len(tokens))\n        token_input, masked_input, mlm_output = [], [], []\n        has_mask = False\n        for token in tokens:\n            mlm_output.append(token_dict.get(token, unknown_index))\n            if token not in base_dict and np.random.random() < mask_rate:\n                has_mask = True\n                masked_input.append(1)\n                r = np.random.random()\n                if r < mask_mask_rate:\n                    token_input.append(token_dict[TOKEN_MASK])\n                elif r < mask_mask_rate + mask_random_rate:\n                    while True:\n                        token = np.random.choice(token_list)\n                        if token not in base_dict:\n                            token_input.append(token_dict[token])\n                            break\n                else:\n                    token_input.append(token_dict.get(token, unknown_index))\n            else:\n                masked_input.append(0)\n                token_input.append(token_dict.get(token, unknown_index))\n        if force_mask and not has_mask:\n            masked_input[1] = 1\n        token_inputs.append(token_input)\n        masked_inputs.append(masked_input)\n        mlm_outputs.append(mlm_output)\n    inputs = [np.asarray(x) for x in [token_inputs, segment_inputs, masked_inputs]]\n    outputs = [np.asarray(np.expand_dims(x, axis=-1)) for x in [mlm_outputs, nsp_outputs]]\n    return inputs, outputs\n'"
19-BERT/loader.py,1,"b'import json\nfrom tensorflow import keras\nimport numpy as np\nimport tensorflow as tf\nfrom .bert import get_model\n\n\n__all__ = [\n    \'build_model_from_config\',\n    \'load_model_weights_from_checkpoint\',\n    \'load_trained_model_from_checkpoint\',\n]\n\n\ndef checkpoint_loader(checkpoint_file):\n    def _loader(name):\n        return tf.train.load_variable(checkpoint_file, name)\n    return _loader\n\n\ndef build_model_from_config(config_file,\n                            training=False,\n                            trainable=None,\n                            seq_len=None):\n    """"""Build the model from config file.\n\n    :param config_file: The path to the JSON configuration file.\n    :param training: If training, the whole model will be returned.\n    :param trainable: Whether the model is trainable.\n    :param seq_len: If it is not None and it is shorter than the value in the config file, the weights in\n                    position embeddings will be sliced to fit the new length.\n    :return: model and config\n    """"""\n    with open(config_file, \'r\') as reader:\n        config = json.loads(reader.read())\n    if seq_len is not None:\n        config[\'max_position_embeddings\'] = min(seq_len, config[\'max_position_embeddings\'])\n    if trainable is None:\n        trainable = training\n    model = get_model(\n        token_num=config[\'vocab_size\'],\n        pos_num=config[\'max_position_embeddings\'],\n        seq_len=config[\'max_position_embeddings\'],\n        embed_dim=config[\'hidden_size\'],\n        transformer_num=config[\'num_hidden_layers\'],\n        head_num=config[\'num_attention_heads\'],\n        feed_forward_dim=config[\'intermediate_size\'],\n        training=training,\n        trainable=trainable,\n    )\n    if not training:\n        inputs, outputs = model\n        model = keras.models.Model(inputs=inputs, outputs=outputs)\n        model.compile(\n            optimizer=keras.optimizers.Adam(),\n            loss=keras.losses.sparse_categorical_crossentropy,\n        )\n    return model, config\n\n\ndef load_model_weights_from_checkpoint(model,\n                                       config,\n                                       checkpoint_file,\n                                       training=False):\n    """"""Load trained official model from checkpoint.\n\n    :param model: Built keras model.\n    :param config: Loaded configuration file.\n    :param checkpoint_file: The path to the checkpoint files, should end with \'.ckpt\'.\n    :param training: If training, the whole model will be returned.\n                     Otherwise, the MLM and NSP parts will be ignored.\n    """"""\n    loader = checkpoint_loader(checkpoint_file)\n\n    model.get_layer(name=\'Embedding-Token\').set_weights([\n        loader(\'bert/embeddings/word_embeddings\'),\n    ])\n    model.get_layer(name=\'Embedding-Position\').set_weights([\n        loader(\'bert/embeddings/position_embeddings\')[:config[\'max_position_embeddings\'], :],\n    ])\n    model.get_layer(name=\'Embedding-Segment\').set_weights([\n        loader(\'bert/embeddings/token_type_embeddings\'),\n    ])\n    model.get_layer(name=\'Embedding-Norm\').set_weights([\n        loader(\'bert/embeddings/LayerNorm/gamma\'),\n        loader(\'bert/embeddings/LayerNorm/beta\'),\n    ])\n    for i in range(config[\'num_hidden_layers\']):\n        model.get_layer(name=\'Encoder-%d-MultiHeadSelfAttention\' % (i + 1)).set_weights([\n            loader(\'bert/encoder/layer_%d/attention/self/query/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/attention/self/query/bias\' % i),\n            loader(\'bert/encoder/layer_%d/attention/self/key/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/attention/self/key/bias\' % i),\n            loader(\'bert/encoder/layer_%d/attention/self/value/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/attention/self/value/bias\' % i),\n            loader(\'bert/encoder/layer_%d/attention/output/dense/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/attention/output/dense/bias\' % i),\n        ])\n        model.get_layer(name=\'Encoder-%d-MultiHeadSelfAttention-Norm\' % (i + 1)).set_weights([\n            loader(\'bert/encoder/layer_%d/attention/output/LayerNorm/gamma\' % i),\n            loader(\'bert/encoder/layer_%d/attention/output/LayerNorm/beta\' % i),\n        ])\n        model.get_layer(name=\'Encoder-%d-MultiHeadSelfAttention-Norm\' % (i + 1)).set_weights([\n            loader(\'bert/encoder/layer_%d/attention/output/LayerNorm/gamma\' % i),\n            loader(\'bert/encoder/layer_%d/attention/output/LayerNorm/beta\' % i),\n        ])\n        model.get_layer(name=\'Encoder-%d-FeedForward\' % (i + 1)).set_weights([\n            loader(\'bert/encoder/layer_%d/intermediate/dense/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/intermediate/dense/bias\' % i),\n            loader(\'bert/encoder/layer_%d/output/dense/kernel\' % i),\n            loader(\'bert/encoder/layer_%d/output/dense/bias\' % i),\n        ])\n        model.get_layer(name=\'Encoder-%d-FeedForward-Norm\' % (i + 1)).set_weights([\n            loader(\'bert/encoder/layer_%d/output/LayerNorm/gamma\' % i),\n            loader(\'bert/encoder/layer_%d/output/LayerNorm/beta\' % i),\n        ])\n    if training:\n        model.get_layer(name=\'MLM-Dense\').set_weights([\n            loader(\'cls/predictions/transform/dense/kernel\'),\n            loader(\'cls/predictions/transform/dense/bias\'),\n        ])\n        model.get_layer(name=\'MLM-Norm\').set_weights([\n            loader(\'cls/predictions/transform/LayerNorm/gamma\'),\n            loader(\'cls/predictions/transform/LayerNorm/beta\'),\n        ])\n        model.get_layer(name=\'MLM-Sim\').set_weights([\n            loader(\'cls/predictions/output_bias\'),\n        ])\n        model.get_layer(name=\'NSP-Dense\').set_weights([\n            loader(\'bert/pooler/dense/kernel\'),\n            loader(\'bert/pooler/dense/bias\'),\n        ])\n        model.get_layer(name=\'NSP\').set_weights([\n            np.transpose(loader(\'cls/seq_relationship/output_weights\')),\n            loader(\'cls/seq_relationship/output_bias\'),\n        ])\n\n\ndef load_trained_model_from_checkpoint(config_file,\n                                       checkpoint_file,\n                                       training=False,\n                                       trainable=None,\n                                       seq_len=None):\n    """"""Load trained official model from checkpoint.\n\n    :param config_file: The path to the JSON configuration file.\n    :param checkpoint_file: The path to the checkpoint files, should end with \'.ckpt\'.\n    :param training: If training, the whole model will be returned.\n                     Otherwise, the MLM and NSP parts will be ignored.\n    :param trainable: Whether the model is trainable. The default value is the same with `training`.\n    :param seq_len: If it is not None and it is shorter than the value in the config file, the weights in\n                    position embeddings will be sliced to fit the new length.\n    :return: model\n    """"""\n    model, config = build_model_from_config(config_file, training=training, trainable=trainable, seq_len=seq_len)\n    load_model_weights_from_checkpoint(model, config, checkpoint_file, training=training)\n    return model\n'"
19-BERT/main.py,0,"b""from tensorflow import keras\nfrom bert import get_base_dict, get_model, gen_batch_inputs\n\n\n# A toy input example\nsentence_pairs = [\n    [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n    [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n    [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n]\n\n\n# Build token dictionary\ntoken_dict = get_base_dict()  # A dict that contains some special tokens\nfor pairs in sentence_pairs:\n    for token in pairs[0] + pairs[1]:\n        if token not in token_dict:\n            token_dict[token] = len(token_dict)\ntoken_list = list(token_dict.keys())  # Used for selecting a random word\n\n\n# Build & train the model\nmodel = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n)\nmodel.summary()\n\ndef _generator():\n    while True:\n        yield gen_batch_inputs(\n            sentence_pairs,\n            token_dict,\n            token_list,\n            seq_len=20,\n            mask_rate=0.3,\n            swap_sentence_rate=1.0,\n        )\n\nmodel.fit_generator(\n    generator=_generator(),\n    steps_per_epoch=1000,\n    epochs=100,\n    validation_data=_generator(),\n    validation_steps=100,\n    callbacks=[\n        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n    ],\n)\n\n\n# Use the trained model\ninputs, output_layer = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n    training=False,      # The input layers and output layer will be returned if `training` is `False`\n    trainable=False,     # Whether the model is trainable. The default value is the same with `training`\n    output_layer_num=4,  # The number of layers whose outputs will be concatenated as a single output.\n                         # Only available when `training` is `False`.\n)"""
19-BERT/tokenizer.py,0,"b'import unicodedata\nfrom .bert import TOKEN_CLS, TOKEN_SEP, TOKEN_UNK\n\n\nclass Tokenizer(object):\n\n    def __init__(self,\n                 token_dict,\n                 token_cls=TOKEN_CLS,\n                 token_sep=TOKEN_SEP,\n                 token_unk=TOKEN_UNK,\n                 pad_index=0,\n                 cased=False):\n        """"""Initialize tokenizer.\n\n        :param token_dict: A dict maps tokens to indices.\n        :param token_cls: The token represents classification.\n        :param token_sep: The token represents separator.\n        :param token_unk: The token represents unknown token.\n        :param pad_index: The index to pad.\n        :param cased: Whether to keep the case.\n        """"""\n        self._token_dict = token_dict\n        self._token_cls = token_cls\n        self._token_sep = token_sep\n        self._token_unk = token_unk\n        self._pad_index = pad_index\n        self._cased = cased\n\n    @staticmethod\n    def _truncate(first_tokens, second_tokens=None, max_len=None):\n        if max_len is None:\n            return\n\n        if second_tokens is not None:\n            while True:\n                total_len = len(first_tokens) + len(second_tokens)\n                if total_len <= max_len - 3:  # 3 for [CLS] .. tokens_a .. [SEP] .. tokens_b [SEP]\n                    break\n                if len(first_tokens) > len(second_tokens):\n                    first_tokens.pop()\n                else:\n                    second_tokens.pop()\n        else:\n            del first_tokens[max_len - 2:]  # 2 for [CLS] .. tokens .. [SEP]\n\n    def _pack(self, first_tokens, second_tokens=None):\n        first_packed_tokens = [self._token_cls] + first_tokens + [self._token_sep]\n        if second_tokens is not None:\n            second_packed_tokens = second_tokens + [self._token_sep]\n            return first_packed_tokens + second_packed_tokens, len(first_packed_tokens), len(second_packed_tokens)\n        else:\n            return first_packed_tokens, len(first_packed_tokens), 0\n\n    def _convert_tokens_to_ids(self, tokens):\n        unk_id = self._token_dict.get(self._token_unk)\n        return [self._token_dict.get(token, unk_id) for token in tokens]\n\n    def tokenize(self, first, second=None):\n        first_tokens = self._tokenize(first)\n        second_tokens = self._tokenize(second) if second is not None else None\n        tokens, _, _ = self._pack(first_tokens, second_tokens)\n        return tokens\n\n    def encode(self, first, second=None, max_len=None):\n        first_tokens = self._tokenize(first)\n        second_tokens = self._tokenize(second) if second is not None else None\n        self._truncate(first_tokens, second_tokens, max_len)\n        tokens, first_len, second_len = self._pack(first_tokens, second_tokens)\n\n        token_ids = self._convert_tokens_to_ids(tokens)\n        segment_ids = [0] * first_len + [1] * second_len\n\n        if max_len is not None:\n            pad_len = max_len - first_len - second_len\n            token_ids += [self._pad_index] * pad_len\n            segment_ids += [0] * pad_len\n\n        return token_ids, segment_ids\n\n    def _tokenize(self, text):\n        if not self._cased:\n            text = unicodedata.normalize(\'NFD\', text)\n            text = \'\'.join([ch for ch in text if unicodedata.category(ch) != \'Mn\'])\n            text = text.lower()\n        spaced = \'\'\n        for ch in text:\n            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n                spaced += \' \' + ch + \' \'\n            elif self._is_space(ch):\n                spaced += \' \'\n            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n                continue\n            else:\n                spaced += ch\n        tokens = []\n        for word in spaced.strip().split():\n            tokens += self._word_piece_tokenize(word)\n        return tokens\n\n    def _word_piece_tokenize(self, word):\n        if word in self._token_dict:\n            return [word]\n        tokens = []\n        start, stop = 0, 0\n        while start < len(word):\n            stop = len(word)\n            while stop > start:\n                sub = word[start:stop]\n                if start > 0:\n                    sub = \'##\' + sub\n                if sub in self._token_dict:\n                    break\n                stop -= 1\n            if start == stop:\n                stop += 1\n            tokens.append(sub)\n            start = stop\n        return tokens\n\n    @staticmethod\n    def _is_punctuation(ch):\n        code = ord(ch)\n        return 33 <= code <= 47 or \\\n            58 <= code <= 64 or \\\n            91 <= code <= 96 or \\\n            123 <= code <= 126 or \\\n            unicodedata.category(ch).startswith(\'P\')\n\n    @staticmethod\n    def _is_cjk_character(ch):\n        code = ord(ch)\n        return 0x4E00 <= code <= 0x9FFF or \\\n            0x3400 <= code <= 0x4DBF or \\\n            0x20000 <= code <= 0x2A6DF or \\\n            0x2A700 <= code <= 0x2B73F or \\\n            0x2B740 <= code <= 0x2B81F or \\\n            0x2B820 <= code <= 0x2CEAF or \\\n            0xF900 <= code <= 0xFAFF or \\\n            0x2F800 <= code <= 0x2FA1F\n\n    @staticmethod\n    def _is_space(ch):\n        return ch == \' \' or ch == \'\\n\' or ch == \'\\r\' or ch == \'\\t\' or \\\n            unicodedata.category(ch) == \'Zs\'\n\n    @staticmethod\n    def _is_control(ch):\n        return unicodedata.category(ch).startswith(\'C\')\n'"
20-GCN/config.py,0,"b""import  argparse\n\nargs = argparse.ArgumentParser()\nargs.add_argument('--dataset', default='cora')\nargs.add_argument('--model', default='gcn')\nargs.add_argument('--learning_rate', default=0.01)\nargs.add_argument('--epochs', default=200)\nargs.add_argument('--hidden1', default=16)\nargs.add_argument('--dropout', default=0.5)\nargs.add_argument('--weight_decay', default=5e-4)\nargs.add_argument('--early_stopping', default=10)\nargs.add_argument('--max_degree', default=3)\n\n\nargs = args.parse_args()\nprint(args)"""
20-GCN/inits.py,8,"b'import tensorflow as tf\nimport numpy as np\n\n\ndef uniform(shape, scale=0.05, name=None):\n    """"""Uniform init.""""""\n    initial = tf.random.uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef glorot(shape, name=None):\n    """"""Glorot & Bengio (AISTATS 2010) init.""""""\n    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n    initial = tf.random.uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef zeros(shape, name=None):\n    """"""All zeros.""""""\n    initial = tf.zeros(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)\n\n\ndef ones(shape, name=None):\n    """"""All ones.""""""\n    initial = tf.ones(shape, dtype=tf.float32)\n    return tf.Variable(initial, name=name)'"
20-GCN/layers.py,12,"b'from    inits import *\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\nfrom    config import args\n\n\n\n\n# global unique layer ID dictionary for layer name assignment\n_LAYER_UIDS = {}\n\n\ndef get_layer_uid(layer_name=\'\'):\n    """"""Helper function, assigns unique layer IDs.""""""\n    if layer_name not in _LAYER_UIDS:\n        _LAYER_UIDS[layer_name] = 1\n        return 1\n    else:\n        _LAYER_UIDS[layer_name] += 1\n        return _LAYER_UIDS[layer_name]\n\n\ndef sparse_dropout(x, rate, noise_shape):\n    """"""\n    Dropout for sparse tensors.\n    """"""\n    random_tensor = 1 - rate\n    random_tensor += tf.random.uniform(noise_shape)\n    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n    pre_out = tf.sparse.retain(x, dropout_mask)\n    return pre_out * (1./(1 - rate))\n\n\ndef dot(x, y, sparse=False):\n    """"""\n    Wrapper for tf.matmul (sparse vs dense).\n    """"""\n    if sparse:\n        res = tf.sparse.sparse_dense_matmul(x, y)\n    else:\n        res = tf.matmul(x, y)\n    return res\n\n\n\n\nclass Dense(layers.Layer):\n    """"""Dense layer.""""""\n    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n        super(Dense, self).__init__(**kwargs)\n\n        if dropout:\n            self.dropout = placeholders[\'dropout\']\n        else:\n            self.dropout = 0.\n\n        self.act = act\n        self.sparse_inputs = sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n\n        # helper variable for sparse dropout\n        self.num_features_nonzero = placeholders[\'num_features_nonzero\']\n\n        with tf.variable_scope(self.name + \'_vars\'):\n            self.vars[\'weights\'] = glorot([input_dim, output_dim],\n                                          name=\'weights\')\n            if self.bias:\n                self.vars[\'bias\'] = zeros([output_dim], name=\'bias\')\n\n        if self.logging:\n            self._log_vars()\n\n    def _call(self, inputs):\n        x = inputs\n\n        # dropout\n        if self.sparse_inputs:\n            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n        else:\n            x = tf.nn.dropout(x, 1-self.dropout)\n\n        # transform\n        output = dot(x, self.vars[\'weights\'], sparse=self.sparse_inputs)\n\n        # bias\n        if self.bias:\n            output += self.vars[\'bias\']\n\n        return self.act(output)\n\n\nclass GraphConvolution(layers.Layer):\n    """"""\n    Graph convolution layer.\n    """"""\n    def __init__(self, input_dim, output_dim, num_features_nonzero,\n                 dropout=0.,\n                 is_sparse_inputs=False,\n                 activation=tf.nn.relu,\n                 bias=False,\n                 featureless=False, **kwargs):\n        super(GraphConvolution, self).__init__(**kwargs)\n\n        self.dropout = dropout\n        self.activation = activation\n        self.is_sparse_inputs = is_sparse_inputs\n        self.featureless = featureless\n        self.bias = bias\n        self.num_features_nonzero = num_features_nonzero\n\n        self.weights_ = []\n        for i in range(1):\n            w = self.add_variable(\'weight\' + str(i), [input_dim, output_dim])\n            self.weights_.append(w)\n        if self.bias:\n            self.bias = self.add_variable(\'bias\', [output_dim])\n\n\n        # for p in self.trainable_variables:\n        #     print(p.name, p.shape)\n\n\n\n    def call(self, inputs, training=None):\n        x, support_ = inputs\n\n        # dropout\n        if training is not False and self.is_sparse_inputs:\n            x = sparse_dropout(x, self.dropout, self.num_features_nonzero)\n        elif training is not False:\n            x = tf.nn.dropout(x, self.dropout)\n\n\n        # convolve\n        supports = list()\n        for i in range(len(support_)):\n            if not self.featureless: # if it has features x\n                pre_sup = dot(x, self.weights_[i], sparse=self.is_sparse_inputs)\n            else:\n                pre_sup = self.weights_[i]\n\n            support = dot(support_[i], pre_sup, sparse=True)\n            supports.append(support)\n\n        output = tf.add_n(supports)\n\n        # bias\n        if self.bias:\n            output += self.bias\n\n        return self.activation(output)\n'"
20-GCN/metrics.py,9,"b'import tensorflow as tf\n\n\ndef masked_softmax_cross_entropy(preds, labels, mask):\n    """"""\n    Softmax cross-entropy loss with masking.\n    """"""\n    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    loss *= mask\n    return tf.reduce_mean(loss)\n\n\ndef masked_accuracy(preds, labels, mask):\n    """"""\n    Accuracy with masking.\n    """"""\n    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n    accuracy_all = tf.cast(correct_prediction, tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    mask /= tf.reduce_mean(mask)\n    accuracy_all *= mask\n    return tf.reduce_mean(accuracy_all)\n'"
20-GCN/models.py,8,"b'import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    layers import *\nfrom    metrics import *\nfrom    config import args \n\n\n\n\n\nclass MLP(keras.Model):\n    def __init__(self, placeholders, input_dim, **kwargs):\n        super(MLP, self).__init__(**kwargs)\n\n        self.inputs = placeholders[\'features\']\n        self.input_dim = input_dim\n        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n        self.output_dim = placeholders[\'labels\'].get_shape().as_list()[1]\n        self.placeholders = placeholders\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=args.learning_rate)\n\n        self.build()\n\n    def _loss(self):\n        # Weight decay loss\n        for var in self.layers[0].vars.values():\n            self.loss += args.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders[\'labels\'],\n                                                  self.placeholders[\'labels_mask\'])\n\n    def _accuracy(self):\n        self.accuracy = masked_accuracy(self.outputs, self.placeholders[\'labels\'],\n                                        self.placeholders[\'labels_mask\'])\n\n    def _build(self):\n        self.layers.append(Dense(input_dim=self.input_dim,\n                                 output_dim=args.hidden1,\n                                 placeholders=self.placeholders,\n                                 act=tf.nn.relu,\n                                 dropout=True,\n                                 sparse_inputs=True,\n                                 logging=self.logging))\n\n        self.layers.append(Dense(input_dim=args.hidden1,\n                                 output_dim=self.output_dim,\n                                 placeholders=self.placeholders,\n                                 act=lambda x: x,\n                                 dropout=True,\n                                 logging=self.logging))\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)\n\n\nclass GCN(keras.Model):\n\n    def __init__(self, input_dim, output_dim, num_features_nonzero, **kwargs):\n        super(GCN, self).__init__(**kwargs)\n\n        self.input_dim = input_dim # 1433\n        self.output_dim = output_dim\n\n        print(\'input dim:\', input_dim)\n        print(\'output dim:\', output_dim)\n        print(\'num_features_nonzero:\', num_features_nonzero)\n\n        self.layers_ = []\n        self.layers_.append(GraphConvolution(input_dim=self.input_dim, # 1433\n                                            output_dim=args.hidden1, # 16\n                                            num_features_nonzero=num_features_nonzero,\n                                            activation=tf.nn.relu,\n                                            dropout=args.dropout,\n                                            is_sparse_inputs=True))\n\n\n\n\n\n        self.layers_.append(GraphConvolution(input_dim=args.hidden1, # 16\n                                            output_dim=self.output_dim, # 7\n                                            num_features_nonzero=num_features_nonzero,\n                                            activation=lambda x: x,\n                                            dropout=args.dropout))\n\n\n        for p in self.trainable_variables:\n            print(p.name, p.shape)\n\n    def call(self, inputs, training=None):\n        """"""\n\n        :param inputs:\n        :param training:\n        :return:\n        """"""\n        x, label, mask, support = inputs\n\n        outputs = [x]\n\n        for layer in self.layers:\n            hidden = layer((outputs[-1], support), training)\n            outputs.append(hidden)\n        output = outputs[-1]\n\n        # # Weight decay loss\n        loss = tf.zeros([])\n        for var in self.layers_[0].trainable_variables:\n            loss += args.weight_decay * tf.nn.l2_loss(var)\n\n        # Cross entropy error\n        loss += masked_softmax_cross_entropy(output, label, mask)\n\n        acc = masked_accuracy(output, label, mask)\n\n        return loss, acc\n\n\n\n    def predict(self):\n        return tf.nn.softmax(self.outputs)\n'"
20-GCN/train.py,12,"b""import  time\nimport  tensorflow as tf\nfrom    tensorflow.keras import optimizers\n\nfrom    utils import *\nfrom    models import GCN, MLP\nfrom    config import args\n\nimport  os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nprint('tf version:', tf.__version__)\nassert tf.__version__.startswith('2.')\n\n\n\n# set random seed\nseed = 123\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n\n\n# load data\nadj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(args.dataset)\nprint('adj:', adj.shape)\nprint('features:', features.shape)\nprint('y:', y_train.shape, y_val.shape, y_test.shape)\nprint('mask:', train_mask.shape, val_mask.shape, test_mask.shape)\n\n\n\n# D^-1@X\nfeatures = preprocess_features(features) # [49216, 2], [49216], [2708, 1433]\nprint('features coordinates::', features[0].shape)\nprint('features data::', features[1].shape)\nprint('features shape::', features[2])\n\nif args.model == 'gcn':\n    # D^-0.5 A D^-0.5\n    support = [preprocess_adj(adj)]\n    num_supports = 1\n    model_func = GCN\nelif args.model == 'gcn_cheby':\n    support = chebyshev_polynomials(adj, args.max_degree)\n    num_supports = 1 + args.max_degree\n    model_func = GCN\nelif args.model == 'dense':\n    support = [preprocess_adj(adj)]  # Not used\n    num_supports = 1\n    model_func = MLP\nelse:\n    raise ValueError('Invalid argument for model: ' + str(args.model))\n\n\n\n# Create model\nmodel = GCN(input_dim=features[2][1], output_dim=y_train.shape[1], num_features_nonzero=features[1].shape) # [1433]\n\n\n\n\ntrain_label = tf.convert_to_tensor(y_train)\ntrain_mask = tf.convert_to_tensor(train_mask)\nval_label = tf.convert_to_tensor(y_val)\nval_mask = tf.convert_to_tensor(val_mask)\ntest_label = tf.convert_to_tensor(y_test)\ntest_mask = tf.convert_to_tensor(test_mask)\nfeatures = tf.SparseTensor(*features)\nsupport = [tf.cast(tf.SparseTensor(*support[0]), dtype=tf.float32)]\nnum_features_nonzero = features.values.shape\ndropout = args.dropout\n\n\noptimizer = optimizers.Adam(lr=1e-2)\n\n\n\nfor epoch in range(args.epochs):\n\n    with tf.GradientTape() as tape:\n        loss, acc = model((features, train_label, train_mask,support))\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    _, val_acc = model((features, val_label, val_mask, support), training=False)\n\n\n    if epoch % 20 == 0:\n\n        print(epoch, float(loss), float(acc), '\\tval:', float(val_acc))\n\n\n\ntest_loss, test_acc = model((features, test_label, test_mask, support), training=False)\n\n\nprint('\\ttest:', float(test_loss), float(test_acc))"""
20-GCN/utils.py,0,"b'import  numpy as np\nimport  pickle as pkl\nimport  networkx as nx\nimport  scipy.sparse as sp\nfrom    scipy.sparse.linalg.eigen.arpack import eigsh\nimport  sys\n\n\ndef parse_index_file(filename):\n    """"""\n    Parse index file.\n    """"""\n    index = []\n    for line in open(filename):\n        index.append(int(line.strip()))\n    return index\n\n\ndef sample_mask(idx, l):\n    """"""\n    Create mask.\n    """"""\n    mask = np.zeros(l)\n    mask[idx] = 1\n    return np.array(mask, dtype=np.bool)\n\n\ndef load_data(dataset_str):\n    """"""\n    Loads input data from gcn/data directory\n\n    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n        object;\n    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n\n    All objects above must be saved using python pickle module.\n\n    :param dataset_str: Dataset name\n    :return: All data input files loaded (as well the training/test data).\n    """"""\n    names = [\'x\', \'y\', \'tx\', \'ty\', \'allx\', \'ally\', \'graph\']\n    objects = []\n    for i in range(len(names)):\n        with open(""data/ind.{}.{}"".format(dataset_str, names[i]), \'rb\') as f:\n            if sys.version_info > (3, 0):\n                objects.append(pkl.load(f, encoding=\'latin1\'))\n            else:\n                objects.append(pkl.load(f))\n\n    x, y, tx, ty, allx, ally, graph = tuple(objects)\n    test_idx_reorder = parse_index_file(""data/ind.{}.test.index"".format(dataset_str))\n    test_idx_range = np.sort(test_idx_reorder)\n\n    if dataset_str == \'citeseer\':\n        # Fix citeseer dataset (there are some isolated nodes in the graph)\n        # Find isolated nodes, add them as zero-vecs into the right position\n        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n        tx = tx_extended\n        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n        ty = ty_extended\n\n    features = sp.vstack((allx, tx)).tolil()\n    features[test_idx_reorder, :] = features[test_idx_range, :]\n    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n\n    labels = np.vstack((ally, ty))\n    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n\n    idx_test = test_idx_range.tolist()\n    idx_train = range(len(y))\n    idx_val = range(len(y), len(y)+500)\n\n    train_mask = sample_mask(idx_train, labels.shape[0])\n    val_mask = sample_mask(idx_val, labels.shape[0])\n    test_mask = sample_mask(idx_test, labels.shape[0])\n\n    y_train = np.zeros(labels.shape)\n    y_val = np.zeros(labels.shape)\n    y_test = np.zeros(labels.shape)\n    y_train[train_mask, :] = labels[train_mask, :]\n    y_val[val_mask, :] = labels[val_mask, :]\n    y_test[test_mask, :] = labels[test_mask, :]\n\n    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n\n\ndef sparse_to_tuple(sparse_mx):\n    """"""\n    Convert sparse matrix to tuple representation.\n    """"""\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        coords = np.vstack((mx.row, mx.col)).transpose()\n        values = mx.data\n        shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx\n\n\ndef preprocess_features(features):\n    """"""\n    Row-normalize feature matrix and convert to tuple representation\n    """"""\n    rowsum = np.array(features.sum(1)) # get sum of each row, [2708, 1]\n    r_inv = np.power(rowsum, -1).flatten() # 1/rowsum, [2708]\n    r_inv[np.isinf(r_inv)] = 0. # zero inf data\n    r_mat_inv = sp.diags(r_inv) # sparse diagonal matrix, [2708, 2708]\n    features = r_mat_inv.dot(features) # D^-1:[2708, 2708]@X:[2708, 2708]\n    return sparse_to_tuple(features) # [coordinates, data, shape], []\n\n\ndef normalize_adj(adj):\n    """"""Symmetrically normalize adjacency matrix.""""""\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1)) # D\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # D^-0.5\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # D^-0.5\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # D^-0.5AD^0.5\n\n\ndef preprocess_adj(adj):\n    """"""Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.""""""\n    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n    return sparse_to_tuple(adj_normalized)\n\n\n\n\n\ndef chebyshev_polynomials(adj, k):\n    """"""\n    Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\n    """"""\n    print(""Calculating Chebyshev polynomials up to order {}..."".format(k))\n\n    adj_normalized = normalize_adj(adj)\n    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n    largest_eigval, _ = eigsh(laplacian, 1, which=\'LM\')\n    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n\n    t_k = list()\n    t_k.append(sp.eye(adj.shape[0]))\n    t_k.append(scaled_laplacian)\n\n    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n\n    for i in range(2, k+1):\n        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n\n    return sparse_to_tuple(t_k)\n'"
21-CN-EN-Translation-BERT/attention.py,17,"b'import  tensorflow as tf\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\n\n# ## Multi-head attention\n\n# In[ ]:\n\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        """"""Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        """"""\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention,\n                                        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n\ndef main():\n    temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n    y = tf.random.uniform((1, 60, 768))  # (batch_size, encoder_sequence, d_model)\n    q = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n    out, attn = temp_mha(y, k=y, q=q, mask=None)\n    out.shape, attn.shape\n\n\n\nif __name__ == \'__main__\':\n    main()'"
21-CN-EN-Translation-BERT/attlayer.py,18,"b""import tensorflow as tf\nfrom    attention import MultiHeadAttention\nfrom    utils import positional_encoding\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    ])\n\n\n\n\n\n# ## Decoder Layer and Decoder\n\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2\n\n# Each decoder layer consists of sublayers:\n#\n# 1. Masked multi-head attention (with look ahead mask and padding mask)\n#\n# 2. Multi-head attention (with padding mask). V (value) and K (key) receive the encoder output as inputs. Q (query) receives the output from the masked multi-head attention sublaye\n#\n# 3. Point wise feed forward networks\n#\n# Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n\n# In[ ]:\n\n\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2\n\n\n\n\n\n\n\ndef main():\n    # In[ ]:\n    sample_encoder_layer = EncoderLayer(512, 8, 2048)\n\n    sample_encoder_layer_output = sample_encoder_layer(\n        tf.random.uniform((64, 43, 512)), False, None)\n\n    sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)\n\n\n    sample_decoder_layer = DecoderLayer(512, 8, 2048)\n    sample_encoder_output = tf.random.uniform((64, 128, 768))\n\n    sample_decoder_layer_output, _, _ = sample_decoder_layer(\n        tf.random.uniform((64, 50, 512)), sample_encoder_output,\n        False, None, None)\n\n    sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)\n\n\nif __name__ == '__main__':\n    main()\n\n"""
21-CN-EN-Translation-BERT/bert_train.py,16,"b'import  tensorflow as tf\n\nimport  time\nimport  numpy as np\nimport  matplotlib.pyplot as plt\nimport  os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\nfrom    tokenizer import get_tokenizer\nfrom    bertmodel import Transformer, Config\nfrom    utils import CustomSchedule, create_masks\nfrom    test import Translator\n\n\n\nBUFFER_SIZE = 50000\nBATCH_SIZE = 64\nMAX_SEQ_LENGTH = 128\n\ntrain_dataset, val_dataset, tokenizer_en, tokenizer_zh = \\\n    get_tokenizer(MAX_SEQ_LENGTH, BATCH_SIZE)\n\n\nconfig = Config(num_layers=6, d_model=256, dff=1024, num_heads=8)\n\n\ntarget_vocab_size = tokenizer_en.vocab_size + 2\ndropout_rate = 0.1\n\n\n\n\nMODEL_DIR = ""chinese_L-12_H-768_A-12""\nbert_config_file = os.path.join(MODEL_DIR, ""bert_config.json"")\nbert_ckpt_file = os.path.join(MODEL_DIR, ""bert_model.ckpt"")\n\ntransformer = Transformer(config=config,\n                          target_vocab_size=target_vocab_size,\n                          bert_config_file=bert_config_file)\n\ninp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\ntar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\nfn_out, _ = transformer(inp, tar_inp,\n                        True,\n                        enc_padding_mask=None,\n                        look_ahead_mask=None,\n                        dec_padding_mask=None)\nprint(tar_inp.shape)  # (batch_size, tar_seq_len)\nprint(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size) \n\n# init bert pre-trained weights\ntransformer.restore_encoder(bert_ckpt_file)\ntransformer.summary()\n\n\n\nlearning_rate = CustomSchedule(config.d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\n\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction=\'none\')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\n\ntrain_loss = tf.keras.metrics.Mean(name=\'train_loss\')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n    name=\'train_accuracy\')\n\n\ncheckpoint_path = ""./zh-en/bert""\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\'Latest checkpoint restored!!\')\n\n\n\n@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp,\n                                     True,\n                                     enc_padding_mask,\n                                     combined_mask,\n                                     dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n\n# Chinese is used as the input language and English is the target language.\ntranslator = Translator(tokenizer_zh, tokenizer_en, transformer, MAX_SEQ_LENGTH)\n\nfor epoch in range(4):\n\n    res = translator.do(\'\xe8\x99\xbd\xe7\x84\xb6\xe7\xbb\xa7\xe6\x89\xbf\xe4\xba\x86\xe7\xa5\x96\xe8\x8d\xab\xef\xbc\x8c\xe4\xbd\x86\xe6\x9c\xb4\xe6\xa7\xbf\xe6\x83\xa0\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xaf\x81\xe6\x98\x8e\xe4\xba\x86\xe8\x87\xaa\xe5\xb7\xb1\xe6\x98\xaf\xe4\xb8\xaa\xe6\x9c\xba\xe6\x95\x8f\xe8\x80\x8c\xe8\x80\x81\xe7\xbb\x83\xe7\x9a\x84\xe6\x94\xbf\xe6\xb2\xbb\xe5\xae\xb6\xe3\x80\x82\')\n\n\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> chinese, tar -> english\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar)\n\n        if batch % 500 == 0:\n            print(\'Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\'.format(\n                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n\n    if (epoch + 1) % 1 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(\'Saving checkpoint for epoch {} at {}\'.format(epoch + 1,\n                                                            ckpt_save_path))\n\n    print(\'Epoch {} Loss {:.4f} Accuracy {:.4f}\'.format(epoch + 1,\n                                                        train_loss.result(),\n                                                        train_accuracy.result()))\n\n    print(\'Time taken for 1 epoch: {} secs\\n\'.format(time.time() - start))\n\n\n'"
21-CN-EN-Translation-BERT/bertmodel.py,10,"b'import tensorflow as tf\n\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, load_stock_weights\nfrom bert.loader import map_to_stock_variable_name\n\n\nfrom    utils import positional_encoding\nfrom    attlayer import DecoderLayer\n\nclass Config(object):\n    def __init__(self, num_layers, d_model, dff, num_heads):\n        self.num_layers = num_layers\n        self.d_model = d_model\n        self.dff = dff\n        self.num_heads = num_heads\n\n\n# In[ ]:\n\ndef build_encoder(config_file):\n    with tf.io.gfile.GFile(config_file, ""r"") as reader:\n        stock_params = StockBertConfig.from_json_string(reader.read())\n        bert_params = stock_params.to_bert_model_layer_params()\n\n    return BertModelLayer.from_params(bert_params, name=""bert"")\n\n\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n                 rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n\n            attention_weights[\'decoder_layer{}_block1\'.format(i + 1)] = block1\n            attention_weights[\'decoder_layer{}_block2\'.format(i + 1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n\n\n\nclass Transformer(tf.keras.Model):\n    def __init__(self, config,\n                 target_vocab_size,\n                 bert_config_file,\n                 bert_training=False,\n                 rate=0.1,\n                 name=\'transformer\'):\n        super(Transformer, self).__init__(name=name)\n\n        self.encoder = build_encoder(config_file=bert_config_file)\n        self.encoder.trainable = bert_training\n\n        self.decoder = Decoder(config.num_layers, config.d_model,\n                               config.num_heads, config.dff, target_vocab_size, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def load_stock_weights(self, bert: BertModelLayer, ckpt_file):\n        assert isinstance(bert, BertModelLayer), ""Expecting a BertModelLayer instance as first argument""\n        assert tf.compat.v1.train.checkpoint_exists(ckpt_file), ""Checkpoint does not exist: {}"".format(ckpt_file)\n        ckpt_reader = tf.train.load_checkpoint(ckpt_file)\n\n        bert_prefix = \'transformer/bert\'\n\n        weights = []\n        for weight in bert.weights:\n            stock_name = map_to_stock_variable_name(weight.name, bert_prefix)\n            if ckpt_reader.has_tensor(stock_name):\n                value = ckpt_reader.get_tensor(stock_name)\n                weights.append(value)\n            else:\n                raise ValueError(""No value for:[{}], i.e.:[{}] in:[{}]"".format(\n                    weight.name, stock_name, ckpt_file))\n        bert.set_weights(weights)\n        print(""Done loading {} BERT weights from: {} into {} (prefix:{})"".format(\n            len(weights), ckpt_file, bert, bert_prefix))\n\n    def restore_encoder(self, bert_ckpt_file):\n        # loading the original pre-trained weights into the BERT layer:\n        self.load_stock_weights(self.encoder, bert_ckpt_file)\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training=self.encoder.trainable)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n'"
21-CN-EN-Translation-BERT/test.py,11,"b'import  tensorflow as tf\n\nimport  time\nimport  numpy as np\nimport  matplotlib.pyplot as plt\nimport  os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\n\nfrom utils import create_masks\n\n\nclass Translator:\n\n    def __init__(self, tokenizer_zh, tokenize_en, model, MAX_SEQ_LENGTH):\n\n        self.tokenizer_zh = tokenizer_zh\n        self.tokenizer_en = tokenize_en\n        self.model = model\n        self.MAX_SEQ_LENGTH = MAX_SEQ_LENGTH\n\n\n    def encode_zh(self, zh):\n        tokens_zh = self.tokenizer_zh.tokenize(zh)\n        lang1 = self.tokenizer_zh.convert_tokens_to_ids([\'[CLS]\'] + tokens_zh + [\'[SEP]\'])\n\n        return lang1\n\n\n\n\n\n    def evaluate(self, inp_sentence):\n        # normalize input sentence\n        inp_sentence = self.encode_zh(inp_sentence)\n        encoder_input = tf.expand_dims(inp_sentence, 0)\n\n        # as the target is english, the first word to the transformer should be the\n        # english start token.\n        decoder_input = [self.tokenizer_en.vocab_size]\n        output = tf.expand_dims(decoder_input, 0)\n\n        for i in range(self.MAX_SEQ_LENGTH):\n            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n                                encoder_input, output)\n\n            # predictions.shape == (batch_size, seq_len, vocab_size)\n            predictions, attention_weights = self.model(encoder_input,\n                                                         output,\n                                                         False,\n                                                         enc_padding_mask,\n                                                         combined_mask,\n                                                         dec_padding_mask)\n\n            # select the last word from the seq_len dimension\n            predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n\n            predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n            # return the result if the predicted_id is equal to the end token\n            if tf.equal(predicted_id, self.tokenizer_en.vocab_size + 1):\n                return tf.squeeze(output, axis=0), attention_weights\n\n            # concatentate the predicted_id to the output which is given to the decoder\n            # as its input.\n            output = tf.concat([output, predicted_id], axis=-1)\n\n        return tf.squeeze(output, axis=0), attention_weights\n\n\n\n\n\n    def plot_attention_weights(self, attention, sentence, result, layer):\n        fig = plt.figure(figsize=(16, 8))\n\n        sentence_ids = self.encode_zh(sentence)\n\n        attention = tf.squeeze(attention[layer], axis=0)\n\n        for head in range(attention.shape[0]):\n            ax = fig.add_subplot(2, 4, head + 1)\n\n            # plot the attention weights\n            ax.matshow(attention[head][:-1, :], cmap=\'viridis\')\n\n            fontdict = {\'fontsize\': 10, \'family\': \'DFKai-SB\'}\n\n            ax.set_xticks(range(len(sentence_ids)))\n            ax.set_yticks(range(len(result)))\n\n            ax.set_ylim(len(result) - 1.5, -0.5)\n\n            ax.set_xticklabels(\n                self.tokenizer_zh.convert_ids_to_tokens(sentence_ids),\n                fontdict=fontdict, rotation=90)\n\n            ax.set_yticklabels([self.tokenizer_en.decode([i]) for i in result\n                                if i < self.tokenizer_en.vocab_size],\n                               fontdict=fontdict)\n\n            ax.set_xlabel(\'Head {}\'.format(head + 1))\n\n        plt.tight_layout()\n        plt.show()\n\n\n# In[ ]:\n\n\n    def do(self, sentence, plot=\'\'):\n        result, attention_weights = self.evaluate(sentence)\n\n        predicted_sentence = self.tokenizer_en.decode([i for i in result\n                                                  if i < self.tokenizer_en.vocab_size])\n\n        print(\'Chinese src: {}\'.format(sentence))\n        print(\'Translated : {}\'.format(predicted_sentence))\n\n        if plot:\n            self.plot_attention_weights(attention_weights, sentence, result, plot)\n\n\n\n\ndef main():\n    # In[42]:\n\n    sentence_ids = encode_zh(""\xe6\x88\x91\xe7\x88\xb1\xe4\xbd\xa0\xe5\x95\x8a"")\n    print(tokenizer_zh.convert_ids_to_tokens(sentence_ids))\n\n    # In[ ]:\n\n    # In[51]:\n\n    translate(transformer, \'\xe8\x99\xbd\xe7\x84\xb6\xe7\xbb\xa7\xe6\x89\xbf\xe4\xba\x86\xe7\xa5\x96\xe8\x8d\xab\xef\xbc\x8c\xe4\xbd\x86\xe6\x9c\xb4\xe6\xa7\xbf\xe6\x83\xa0\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xaf\x81\xe6\x98\x8e\xe4\xba\x86\xe8\x87\xaa\xe5\xb7\xb1\xe6\x98\xaf\xe4\xb8\xaa\xe6\x9c\xba\xe6\x95\x8f\xe8\x80\x8c\xe8\x80\x81\xe7\xbb\x83\xe7\x9a\x84\xe6\x94\xbf\xe6\xb2\xbb\xe5\xae\xb6\xe2\x80\x94\xe2\x80\x94\xe5\xa5\xb9\xe5\x8e\x86\xe7\xbb\x8f20\xe5\xb9\xb4\xe6\x89\x8d\xe7\x88\xac\xe4\xb8\x8a\xe9\x9f\xa9\xe5\x9b\xbd\xe5\xa4\xa7\xe5\x9b\xbd\xe5\xae\xb6\xe5\x85\x9a\xe6\x9c\x80\xe9\xab\x98\xe9\xa2\x86\xe5\xaf\xbc\xe5\xb1\x82\xe5\xb9\xb6\xe6\x88\x90\xe4\xb8\xba\xe5\x85\xa8\xe5\x9b\xbd\xe7\x9f\xa5\xe5\x90\x8d\xe4\xba\xba\xe7\x89\xa9\xe3\x80\x82\')\n    print(\n        \'Real translation: While Park derives some of her power from her family pedigree, she has proven to be an astute and seasoned politician \xe2\x80\x93&nbsp;one who climbed the Grand National Party\xe2\x80\x99s leadership ladder over the last two decades to emerge as a national figure.\')\n\n    # In[59]:\n\n    translate(transformer, ""\xe6\x88\x91\xe7\x88\xb1\xe4\xbd\xa0\xe6\x98\xaf\xe4\xb8\x80\xe4\xbb\xb6\xe5\xb9\xb8\xe7\xa6\x8f\xe7\x9a\x84\xe4\xba\x8b\xe6\x83\x85\xe3\x80\x82"")\n\n    # ## Save weights\n\n    # In[ ]:\n\n    transformer.save_weights(\'bert_nmt_ckpt\')\n\n    # In[49]:\n\n    new_transformer = Transformer(config=config,\n                                  target_vocab_size=target_vocab_size,\n                                  bert_config_file=bert_config_file)\n\n    fn_out, _ = new_transformer(inp, tar_inp,\n                                True,\n                                look_ahead_mask=None,\n                                dec_padding_mask=None)\n    new_transformer.load_weights(\'bert_nmt_ckpt\')\n\n    translate(new_transformer, \'\xe6\x88\x91\xe7\x88\xb1\xe4\xbd\xa0\')\n\nif __name__ == \'__main__\':\n    main()'"
21-CN-EN-Translation-BERT/tokenizer.py,10,"b'import  tensorflow as tf\nimport  tensorflow_datasets as tfds\n\n\nimport  collections\nimport  unicodedata\nimport  os,sys\nimport  numpy as np\n\n\n\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(""utf-8"", ""ignore"")\n    else:\n        raise ValueError(""Unsupported string type: %s"" % (type(text)))\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, ""r"") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\ndef convert_by_vocab(vocab, items):\n    """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenziation.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenziation.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n\n\n\n\ndef get_tokenizer(MAX_SEQ_LENGTH, BATCH_SIZE):\n    # ## Setup input pipleline\n\n    # Use TFDS to load the wmt2019 zh-en translation dataset.\n\n    if not os.path.exists(\'chinese_L-12_H-768_A-12\'):\n        # get_ipython().system(\'wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\')\n        # get_ipython().system(\'unzip chinese_L-12_H-768_A-12\')\n        print(\'download pretrained first!\')\n        sys.exit()\n\n    config = tfds.translate.wmt.WmtConfig(\n        description=""WMT 2019 translation task dataset."",\n        version=""0.0.3"",\n        language_pair=(""zh"", ""en""),\n        subsets={\n            tfds.Split.TRAIN: [""newscommentary_v13""],\n            tfds.Split.VALIDATION: [""newsdev2017""],\n        }\n    )\n\n    builder = tfds.builder(""wmt_translate"", config=config)\n    print(builder.info.splits)\n    builder.download_and_prepare()\n    datasets = builder.as_dataset(as_supervised=True)\n    print(\'datasets is {}\'.format(datasets))\n\n    # In[ ]:\n\n    train_examples = datasets[\'train\']\n    val_examples = datasets[\'validation\']\n\n    # In[ ]:\n\n    for zh, en in train_examples.take(1):\n        # print((zh))\n        print(tf.compat.as_text(zh.numpy()))\n        print(tf.compat.as_text(en.numpy()))\n\n    # Create a custom subwords tokenizer from the training dataset for the decoder.\n\n    # In[ ]:\n\n    vocab_file = \'vocab_en\'\n    if os.path.isfile(vocab_file + \'.subwords\'):\n        tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file(vocab_file)\n    else:\n        tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n            (en.numpy() for zh, en in train_examples), target_vocab_size=2 ** 13)\n        tokenizer_en.save_to_file(\'vocab_en\')\n\n    sample_string = \'Transformer is awesome.\'\n    tokenized_string = tokenizer_en.encode(sample_string)\n    for ts in tokenized_string:\n        print(\'{} ----> {}\'.format(ts, tokenizer_en.decode([ts])))\n\n    # The encoder uses BERT tokenizer.\n\n    # In[ ]:\n\n    tokenizer_zh = FullTokenizer(\n        vocab_file=\'chinese_L-12_H-768_A-12/vocab.txt\', do_lower_case=True)\n\n    test_tokens = tokenizer_zh.tokenize(\'\xe4\xbb\x8a\xe5\xa4\xa9\xe5\xa4\xa9\xe6\xb0\x94\xe4\xb8\x8d\xe9\x94\x99\xe9\xa2\x9d\xe3\x80\x82\')\n    test_ids = tokenizer_zh.convert_tokens_to_ids([\'[CLS]\'] + test_tokens + [\'[SEP]\'])\n    print(\'tokens:\', test_tokens)\n    print(\'ids:\', test_ids)\n    print(\'convert_ids_to_tokens:\', tokenizer_zh.convert_ids_to_tokens(test_ids))\n\n\n    def encode(zh, en, seq_length=MAX_SEQ_LENGTH):\n        tokens_zh = tokenizer_zh.tokenize(tf.compat.as_text(zh.numpy()))\n        lang1 = tokenizer_zh.convert_tokens_to_ids([\'[CLS]\'] + tokens_zh + [\'[SEP]\'])\n        if len(lang1) < seq_length:\n            lang1 = lang1 + list(np.zeros(seq_length - len(lang1), \'int32\'))\n\n        # insert SOS and EOS\n        lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n            tf.compat.as_text(en.numpy())) + [tokenizer_en.vocab_size + 1]\n        if len(lang2) < seq_length:\n            lang2 = lang2 + list(np.zeros(seq_length - len(lang2), \'int32\'))\n\n        return lang1, lang2\n\n\n\n    def filter_max_length(x, y, max_length=MAX_SEQ_LENGTH):\n        return tf.logical_and(tf.size(x) <= max_length,\n                              tf.size(y) <= max_length)\n\n    train_dataset = train_examples.map(\n        lambda zh, en: tf.py_function(encode, [zh, en], [tf.int32, tf.int32]))\n    train_dataset = train_dataset.filter(filter_max_length)\n\n    # cache the dataset to memory to get a speedup while reading from it.\n    train_dataset = train_dataset.cache()\n    train_dataset = train_dataset.shuffle(20000).padded_batch(\n        BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\n    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n    val_dataset = val_examples.map(\n        lambda zh, en: tf.py_function(encode, [zh, en], [tf.int32, tf.int32]))\n    val_dataset = val_dataset.filter(filter_max_length)\n    val_dataset = val_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n\n\n\n\n    return train_dataset, val_dataset, tokenizer_en, tokenizer_zh\n\n\nif __name__ == \'__main__\':\n    get_tokenizer(100, 64)'"
21-CN-EN-Translation-BERT/transformer.py,16,"b'import tensorflow as tf\nimport numpy as np\n\nfrom    utils import positional_encoding\nfrom    attlayer import EncoderLayer,DecoderLayer\n\n\n\nclass Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n                 rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        # adding embedding and position encoding.\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\n\n\n""""""### Decoder\n\nThe `Decoder` consists of:\n1.   Output Embedding\n2.   Positional Encoding\n3.   N decoder layers\n\nThe target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer.\n""""""\n\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n                 rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                   look_ahead_mask, padding_mask)\n\n            attention_weights[\'decoder_layer{}_block1\'.format(i + 1)] = block1\n            attention_weights[\'decoder_layer{}_block2\'.format(i + 1)] = block2\n\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights\n\n\n\n""""""## Create the Transformer\n\nTransformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.\n""""""\n\n\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n                 target_vocab_size, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n                               input_vocab_size, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n                               target_vocab_size, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output, attention_weights\n\n\n\n\n\nif __name__ == \'__main__\':\n    sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n                             dff=2048, input_vocab_size=8500)\n\n    sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)),\n                                           training=False, mask=None)\n\n    print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n\n    sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n                             dff=2048, target_vocab_size=8000)\n\n    output, attn = sample_decoder(tf.random.uniform((64, 26)),\n                                  enc_output=sample_encoder_output,\n                                  training=False, look_ahead_mask=None,\n                                  padding_mask=None)\n\n    output.shape, attn[\'decoder_layer2_block2\'].shape\n\n\n    sample_transformer = Transformer(\n        num_layers=2, d_model=512, num_heads=8, dff=2048,\n        input_vocab_size=8500, target_vocab_size=8000)\n\n    temp_input = tf.random.uniform((64, 62))\n    temp_target = tf.random.uniform((64, 26))\n\n    fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n                                   enc_padding_mask=None,\n                                   look_ahead_mask=None,\n                                   dec_padding_mask=None)\n\n    fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n\n'"
21-CN-EN-Translation-BERT/transformer_train.py,16,"b'import  tensorflow as tf\n\nimport  time\nimport  numpy as np\nimport  matplotlib.pyplot as plt\nimport  os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\ngpus = tf.config.experimental.list_physical_devices(\'GPU\')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\nfrom    tokenizer import get_tokenizer\nfrom    transformer import Transformer\nfrom    utils import CustomSchedule, create_masks\nfrom    test import Translator\n\n\n\n\nBUFFER_SIZE = 20000\nBATCH_SIZE = 64\nMAX_SEQ_LENGTH = 128\n\ntrain_dataset, val_dataset, tokenizer_en, tokenizer_zh = \\\n    get_tokenizer(MAX_SEQ_LENGTH, BATCH_SIZE)\n\n# Chinese -> English translation\ninput_vocab_size = 21128\ntarget_vocab_size = tokenizer_en.vocab_size + 2\ndropout_rate = 0.1\nnum_layers=4\nd_model=512\ndff=2048\nnum_heads=8\n\ntransformer = Transformer(num_layers, d_model, num_heads, dff,\n                          input_vocab_size, target_vocab_size, dropout_rate)\n\ninp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\ntar_inp = tf.random.uniform((BATCH_SIZE, MAX_SEQ_LENGTH))\n\nfn_out, _ = transformer(inp, tar_inp,\n                        True,\n                        enc_padding_mask=None,\n                        look_ahead_mask=None,\n                        dec_padding_mask=None)\nprint(tar_inp.shape)  # (batch_size, tar_seq_len)\nprint(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\ntransformer.summary()\n\n\n\nlearning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9)\n\n\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n                from_logits=True, reduction=\'none\')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\n\ntrain_loss = tf.keras.metrics.Mean(name=\'train_loss\')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n                    name=\'train_accuracy\')\n\n\ncheckpoint_path = ""./zh-en/transformer""\n\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\'Latest checkpoint restored!!\')\n\n\n\n@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp,\n                                     True,\n                                     enc_padding_mask,\n                                     combined_mask,\n                                     dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n\n\n# Chinese is used as the input language and English is the target language.\ntranslator = Translator(tokenizer_zh, tokenizer_en, transformer, MAX_SEQ_LENGTH)\n\nfor epoch in range(20):\n\n    (cn_code, en_code) = next(iter(val_dataset))\n    cn_code, en_code = cn_code[epoch].numpy(), en_code[epoch].numpy()\n    # print(cn_code)\n    # print(en_code)\n    en = tokenizer_en.decode([i for i in en_code if i < tokenizer_en.vocab_size])\n    cn_code = [int(i)\n                    for i in cn_code if (i!=101 and i!=102 and i!=1 and i!=0)]\n    # print(cn_code)\n    cn = tokenizer_zh.convert_ids_to_tokens(cn_code)\n    cn = """".join(cn)\n    translator.do(cn)\n    print(\'Real:\', en)\n    print(\'\\n\')\n\n\n\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> chinese, tar -> english\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(\'Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\'.format(\n                epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n\n    if (epoch + 1) % 3 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(\'Saving checkpoint for epoch {} at {}\'.format(epoch + 1,\n                                                            ckpt_save_path))\n\n    print(\'Epoch {} Loss {:.4f} Accuracy {:.4f}\'.format(epoch + 1,\n                                                        train_loss.result(),\n                                                        train_accuracy.result()))\n\n    print(\'Time taken for 1 epoch: {} secs\\n\'.format(time.time() - start))\n\n\n\n'"
21-CN-EN-Translation-BERT/utils.py,13,"b'\nimport  tensorflow as tf\nimport  numpy as np\nimport  matplotlib.pyplot as plt\n\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return pos * angle_rates\n\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n\n    # apply sin to even indices in the array; 2i\n    sines = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    cosines = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n\n    pos_encoding = pos_encoding[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\n\n# ## Masking\n\n# Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.\n\n# In[ ]:\n\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n\n    # add extra dimensions so that we can add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\n\n# The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n#\n# This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on.\n\n# In[ ]:\n\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\n\ndef create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by\n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return enc_padding_mask, combined_mask, dec_padding_mask\n\n# def create_masks2(inp, tar):\n#     # Used in the 2nd attention block in the decoder.\n#     # This padding mask is used to mask the encoder outputs.\n#     dec_padding_mask = create_padding_mask(inp)\n#\n#     # Used in the 1st attention block in the decoder.\n#     # It is used to pad and mask future tokens in the input received by\n#     # the decoder.\n#     look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n#     dec_target_padding_mask = create_padding_mask(tar)\n#     combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n#\n#     return combined_mask, dec_padding_mask\n\n\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n\n\n\ndef main():\n    # In[ ]:\n\n    temp_learning_rate_schedule = CustomSchedule(config.d_model)\n\n    plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n    plt.ylabel(""Learning Rate"")\n    plt.xlabel(""Train Step"")\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
19-BERT/embedding_similarity/__init__.py,0,b'from .embeddings import *\n'
19-BERT/embedding_similarity/embeddings.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n__all__ = [\'EmbeddingRet\', \'EmbeddingSim\', \'get_custom_objects\']\n\n\nclass EmbeddingRet(keras.layers.Embedding):\n    """"""Embedding layer with weights returned.""""""\n\n    def compute_output_shape(self, input_shape):\n        return [\n            super(EmbeddingRet, self).compute_output_shape(input_shape),\n            (self.input_dim, self.output_dim),\n        ]\n\n    def compute_mask(self, inputs, mask=None):\n        return [\n            super(EmbeddingRet, self).compute_mask(inputs, mask),\n            None,\n        ]\n\n    def call(self, inputs):\n        return [\n            super(EmbeddingRet, self).call(inputs),\n            self.embeddings,\n        ]\n\n\nclass EmbeddingSim(keras.layers.Layer):\n    """"""Calculate similarity between features and token embeddings with bias term.""""""\n\n    def __init__(self,\n                 use_bias=True,\n                 initializer=\'zeros\',\n                 regularizer=None,\n                 constraint=None,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param output_dim: Same as embedding output dimension.\n        :param use_bias: Whether to use bias term.\n        :param initializer: Initializer for bias.\n        :param regularizer: Regularizer for bias.\n        :param constraint: Constraint for bias.\n        :param kwargs: Arguments for parent class.\n        """"""\n        super(EmbeddingSim, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.use_bias = use_bias\n        self.initializer = keras.initializers.get(initializer)\n        self.regularizer = keras.regularizers.get(regularizer)\n        self.constraint = keras.constraints.get(constraint)\n        self.bias = None\n\n    def get_config(self):\n        config = {\n            \'use_bias\': self.use_bias,\n            \'initializer\': keras.initializers.serialize(self.initializer),\n            \'regularizer\': keras.regularizers.serialize(self.regularizer),\n            \'constraint\': keras.constraints.serialize(self.constraint),\n        }\n        base_config = super(EmbeddingSim, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        if self.use_bias:\n            embed_shape = input_shape[1]\n            token_num = embed_shape[0]\n            self.bias = self.add_weight(\n                shape=(token_num,),\n                initializer=self.initializer,\n                regularizer=self.regularizer,\n                constraint=self.constraint,\n                name=\'bias\',\n            )\n        super(EmbeddingSim, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        feature_shape, embed_shape = input_shape\n        token_num = embed_shape[0]\n        return feature_shape[:-1] + (token_num,)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask[0]\n\n    def call(self, inputs, mask=None, **kwargs):\n        inputs, embeddings = inputs\n        outputs = K.dot(inputs, K.transpose(embeddings))\n        if self.use_bias:\n            outputs = K.bias_add(outputs, self.bias)\n        return keras.activations.softmax(outputs)\n\n\ndef get_custom_objects():\n    return {\n        \'EmbeddingRet\': EmbeddingRet,\n        \'EmbeddingSim\': EmbeddingSim,\n    }\n'"
19-BERT/layer_normalization/__init__.py,0,b'from .layer_normalization import LayerNormalization\n'
19-BERT/layer_normalization/layer_normalization.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass LayerNormalization(keras.layers.Layer):\n\n    def __init__(self,\n                 center=True,\n                 scale=True,\n                 epsilon=None,\n                 gamma_initializer=\'ones\',\n                 beta_initializer=\'zeros\',\n                 gamma_regularizer=None,\n                 beta_regularizer=None,\n                 gamma_constraint=None,\n                 beta_constraint=None,\n                 **kwargs):\n        """"""Layer normalization layer\n\n        See: [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)\n\n        :param center: Add an offset parameter if it is True.\n        :param scale: Add a scale parameter if it is True.\n        :param epsilon: Epsilon for calculating variance.\n        :param gamma_initializer: Initializer for the gamma weight.\n        :param beta_initializer: Initializer for the beta weight.\n        :param gamma_regularizer: Optional regularizer for the gamma weight.\n        :param beta_regularizer: Optional regularizer for the beta weight.\n        :param gamma_constraint: Optional constraint for the gamma weight.\n        :param beta_constraint: Optional constraint for the beta weight.\n        :param kwargs:\n        """"""\n        super(LayerNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.center = center\n        self.scale = scale\n        if epsilon is None:\n            epsilon = K.epsilon() * K.epsilon()\n        self.epsilon = epsilon\n        self.gamma_initializer = keras.initializers.get(gamma_initializer)\n        self.beta_initializer = keras.initializers.get(beta_initializer)\n        self.gamma_regularizer = keras.regularizers.get(gamma_regularizer)\n        self.beta_regularizer = keras.regularizers.get(beta_regularizer)\n        self.gamma_constraint = keras.constraints.get(gamma_constraint)\n        self.beta_constraint = keras.constraints.get(beta_constraint)\n        self.gamma, self.beta = None, None\n\n    def get_config(self):\n        config = {\n            \'center\': self.center,\n            \'scale\': self.scale,\n            \'epsilon\': self.epsilon,\n            \'gamma_initializer\': keras.initializers.serialize(self.gamma_initializer),\n            \'beta_initializer\': keras.initializers.serialize(self.beta_initializer),\n            \'gamma_regularizer\': keras.regularizers.serialize(self.gamma_regularizer),\n            \'beta_regularizer\': keras.regularizers.serialize(self.beta_regularizer),\n            \'gamma_constraint\': keras.constraints.serialize(self.gamma_constraint),\n            \'beta_constraint\': keras.constraints.serialize(self.beta_constraint),\n        }\n        base_config = super(LayerNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        return input_mask\n\n    def build(self, input_shape):\n        self.input_spec = keras.layers.InputSpec(shape=input_shape)\n        shape = input_shape[-1:]\n        if self.scale:\n            self.gamma = self.add_weight(\n                shape=shape,\n                initializer=self.gamma_initializer,\n                regularizer=self.gamma_regularizer,\n                constraint=self.gamma_constraint,\n                name=\'gamma\',\n            )\n        if self.center:\n            self.beta = self.add_weight(\n                shape=shape,\n                initializer=self.beta_initializer,\n                regularizer=self.beta_regularizer,\n                constraint=self.beta_constraint,\n                name=\'beta\',\n            )\n        super(LayerNormalization, self).build(input_shape)\n\n    def call(self, inputs, training=None):\n        mean = K.mean(inputs, axis=-1, keepdims=True)\n        variance = K.mean(K.square(inputs - mean), axis=-1, keepdims=True)\n        std = K.sqrt(variance + self.epsilon)\n        outputs = (inputs - mean) / std\n        if self.scale:\n            outputs *= self.gamma\n        if self.center:\n            outputs += self.beta\n        return outputs\n'"
19-BERT/layers/__init__.py,0,"b'from .inputs import get_inputs\nfrom .embedding import get_embedding, TokenEmbedding, EmbeddingSimilarity\nfrom .masked import Masked\nfrom .extract import Extract\nfrom .pooling import MaskedGlobalMaxPool1D\nfrom .conv import MaskedConv1D\n'"
19-BERT/layers/conv.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass MaskedConv1D(keras.layers.Conv1D):\n\n    def __init__(self, **kwargs):\n        super(MaskedConv1D, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            inputs *= K.expand_dims(mask, axis=-1)\n        return super(MaskedConv1D, self).call(inputs)\n'"
19-BERT/layers/embedding.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom keras_pos_embd import PositionEmbedding\nfrom keras_layer_normalization import LayerNormalization\n\n\nclass TokenEmbedding(keras.layers.Embedding):\n    """"""Embedding layer with weights returned.""""""\n\n    def compute_output_shape(self, input_shape):\n        return [super(TokenEmbedding, self).compute_output_shape(input_shape), (self.input_dim, self.output_dim)]\n\n    def compute_mask(self, inputs, mask=None):\n        return [super(TokenEmbedding, self).compute_mask(inputs, mask), None]\n\n    def call(self, inputs):\n        return [super(TokenEmbedding, self).call(inputs), self.embeddings]\n\n\ndef get_embedding(inputs, token_num, pos_num, embed_dim, dropout_rate=0.1, trainable=True):\n    """"""Get embedding layer.\n\n    See: https://arxiv.org/pdf/1810.04805.pdf\n\n    :param inputs: Input layers.\n    :param token_num: Number of tokens.\n    :param pos_num: Maximum position.\n    :param embed_dim: The dimension of all embedding layers.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: The merged embedding layer and weights of token embedding.\n    """"""\n    embeddings = [\n        TokenEmbedding(\n            input_dim=token_num,\n            output_dim=embed_dim,\n            mask_zero=True,\n            trainable=trainable,\n            name=\'Embedding-Token\',\n        )(inputs[0]),\n        keras.layers.Embedding(\n            input_dim=2,\n            output_dim=embed_dim,\n            trainable=trainable,\n            name=\'Embedding-Segment\',\n        )(inputs[1]),\n    ]\n    embeddings[0], embed_weights = embeddings[0]\n    embed_layer = keras.layers.Add(name=\'Embedding-Token-Segment\')(embeddings)\n    embed_layer = PositionEmbedding(\n        input_dim=pos_num,\n        output_dim=embed_dim,\n        mode=PositionEmbedding.MODE_ADD,\n        trainable=trainable,\n        name=\'Embedding-Position\',\n    )(embed_layer)\n    if dropout_rate > 0.0:\n        dropout_layer = keras.layers.Dropout(\n            rate=dropout_rate,\n            name=\'Embedding-Dropout\',\n        )(embed_layer)\n    else:\n        dropout_layer = embed_layer\n    norm_layer = LayerNormalization(\n        trainable=trainable,\n        name=\'Embedding-Norm\',\n    )(dropout_layer)\n    return norm_layer, embed_weights\n\n\nclass EmbeddingSimilarity(keras.layers.Layer):\n    """"""Calculate similarity between features and token embeddings with bias term.""""""\n\n    def __init__(self,\n                 initializer=\'zeros\',\n                 regularizer=None,\n                 constraint=None,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param output_dim: Same as embedding output dimension.\n        :param initializer: Initializer for bias.\n        :param regularizer: Regularizer for bias.\n        :param constraint: Constraint for bias.\n        :param kwargs: Arguments for parent class.\n        """"""\n        super(EmbeddingSimilarity, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.initializer = keras.initializers.get(initializer)\n        self.regularizer = keras.regularizers.get(regularizer)\n        self.constraint = keras.constraints.get(constraint)\n        self.bias = None\n\n    def get_config(self):\n        config = {\n            \'initializer\': keras.initializers.serialize(self.initializer),\n            \'regularizer\': keras.regularizers.serialize(self.regularizer),\n            \'constraint\': keras.constraints.serialize(self.constraint),\n        }\n        base_config = super(EmbeddingSimilarity, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        self.bias = self.add_weight(\n            shape=(input_shape[1][0],),\n            initializer=self.initializer,\n            regularizer=self.regularizer,\n            constraint=self.constraint,\n            name=\'bias\',\n        )\n        super(EmbeddingSimilarity, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0][:2] + (input_shape[1][0],)\n\n    def compute_mask(self, inputs, mask=None):\n        return mask[0]\n\n    def call(self, inputs, mask=None, **kwargs):\n        inputs, embeddings = inputs\n        outputs = K.bias_add(K.dot(inputs, K.transpose(embeddings)), self.bias)\n        return keras.activations.softmax(outputs)\n'"
19-BERT/layers/extract.py,0,"b'from tensorflow import keras\n\n\nclass Extract(keras.layers.Layer):\n    """"""Extract from index.\n\n    See: https://arxiv.org/pdf/1810.04805.pdf\n    """"""\n\n    def __init__(self, index, **kwargs):\n        super(Extract, self).__init__(**kwargs)\n        self.index = index\n        self.supports_masking = True\n\n    def get_config(self):\n        config = {\n            \'index\': self.index,\n        }\n        base_config = super(Extract, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:1] + input_shape[2:]\n\n    def compute_mask(self, inputs, mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        return x[:, self.index]\n'"
19-BERT/layers/inputs.py,0,"b'from tensorflow import keras\n\n\ndef get_inputs(seq_len):\n    """"""Get input layers.\n\n    See: https://arxiv.org/pdf/1810.04805.pdf\n\n    :param seq_len: Length of the sequence or None.\n    """"""\n    names = [\'Token\', \'Segment\', \'Masked\']\n    return [keras.layers.Input(\n        shape=(seq_len,),\n        name=\'Input-%s\' % name,\n    ) for name in names]\n'"
19-BERT/layers/masked.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass Masked(keras.layers.Layer):\n    """"""Generate output mask based on the given mask.\n\n    The inputs for the layer is the original input layer and the masked locations.\n\n    See: https://arxiv.org/pdf/1810.04805.pdf\n    """"""\n\n    def __init__(self,\n                 return_masked=False,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param return_masked: Whether to return the merged mask.\n        :param kwargs: Arguments for parent class.\n        """"""\n        super(Masked, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.return_masked = return_masked\n\n    def get_config(self):\n        config = {\n            \'return_masked\': self.return_masked,\n        }\n        base_config = super(Masked, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        if self.return_masked:\n            return [input_shape[0], input_shape[0][:-1]]\n        return input_shape[0]\n\n    def compute_mask(self, inputs, mask=None):\n        token_mask = K.not_equal(inputs[1], 0)\n        return K.all(K.stack([token_mask, mask[0]], axis=0), axis=0)\n\n    def call(self, inputs, mask=None, **kwargs):\n        if self.return_masked:\n            return [inputs[0], K.cast(self.compute_mask(inputs, mask), K.floatx())]\n        return inputs[0]\n'"
19-BERT/layers/pooling.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass MaskedGlobalMaxPool1D(keras.layers.Layer):\n\n    def __init__(self, **kwargs):\n        super(MaskedGlobalMaxPool1D, self).__init__(**kwargs)\n        self.supports_masking = True\n\n    def compute_mask(self, inputs, mask=None):\n        return None\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[:-2] + (input_shape[-1],)\n\n    def call(self, inputs, mask=None):\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            inputs -= K.expand_dims((1.0 - mask) * 1e6, axis=-1)\n        return K.max(inputs, axis=-2)\n'"
19-BERT/multi_head_attention/__init__.py,0,b'from .multi_head import MultiHead\nfrom .multi_head_attention import MultiHeadAttention\n'
19-BERT/multi_head_attention/multi_head.py,0,"b'import copy\nfrom tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass MultiHead(keras.layers.Wrapper):\n\n    def __init__(self,\n                 layer,\n                 layer_num=1,\n                 hidden_dim=None,\n                 use_bias=True,\n                 reg_index=None,\n                 reg_slice=None,\n                 reg_factor=0.0,\n                 **kwargs):\n        """"""Initialize the wrapper layer.\n\n        :param layer: The layer to be duplicated or a list of layers.\n        :param layer_num: The number of duplicated layers.\n        :param hidden_dim: A linear transformation will be applied to the input data if provided, otherwise the original\n                           data will be feed to the sub-layers.\n        :param use_bias: Whether to use bias in the linear transformation.\n        :param reg_index: The index of weights to be regularized.\n        :param reg_slice: The slice indicates which part of the weight to be regularized.\n        :param reg_factor: The weights of the regularization.\n        :param kwargs: Arguments for parent.\n        """"""\n        if type(layer) is list:\n            self.layer = layer[0]\n            self.layers = layer\n            self.layer_num = len(self.layers)\n            self.rename = False\n        else:\n            self.layer = layer\n            self.layers = []\n            self.layer_num = layer_num\n            self.rename = True\n        self.hidden_dim = hidden_dim\n        self.use_bias = use_bias\n        if reg_index is None or type(reg_index) is list:\n            self.reg_index = reg_index\n        else:\n            self.reg_index = [reg_index]\n        if type(reg_slice) is list or reg_index is None:\n            self.reg_slice = reg_slice\n        else:\n            self.reg_slice = [reg_slice] * len(self.reg_index)\n        if reg_factor is None or type(reg_factor) is list or reg_index is None:\n            self.reg_weight = reg_factor\n        else:\n            self.reg_weight = [reg_factor] * len(self.reg_index)\n\n        self.W, self.b = None, None\n        self.supports_masking = self.layer.supports_masking\n        super(MultiHead, self).__init__(self.layer, **kwargs)\n\n    def get_config(self):\n        slices = None\n        if self.reg_slice:\n            slices = []\n            for interval in self.reg_slice:\n                if interval is None:\n                    slices.append(None)\n                elif type(interval) is slice:\n                    slices.append([interval.start, interval.stop, interval.step])\n                else:\n                    slices.append([])\n                    for sub in interval:\n                        slices[-1].append([sub.start, sub.stop, sub.step])\n        config = {\n            \'layers\': [],\n            \'hidden_dim\': self.hidden_dim,\n            \'use_bias\': self.use_bias,\n            \'reg_index\': self.reg_index,\n            \'reg_slice\': slices,\n            \'reg_factor\': self.reg_weight,\n        }\n        for layer in self.layers:\n            config[\'layers\'].append({\n                \'class_name\': layer.__class__.__name__,\n                \'config\': layer.get_config(),\n            })\n        base_config = super(MultiHead, self).get_config()\n        base_config.pop(\'layer\')\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config, custom_objects=None):\n        reg_slice = config.pop(\'reg_slice\')\n        if reg_slice is not None:\n            slices = []\n            for interval in reg_slice:\n                if interval is None:\n                    slices.append(None)\n                elif type(interval[0]) is list:\n                    slices.append([])\n                    for sub in interval:\n                        slices[-1].append(slice(sub[0], sub[1], sub[2]))\n                    slices[-1] = tuple(slices[-1])\n                else:\n                    slices.append(slice(interval[0], interval[1], interval[2]))\n            reg_slice = slices\n        layers = [keras.layers.deserialize(layer, custom_objects=custom_objects) for layer in config.pop(\'layers\')]\n        return cls(layers, reg_slice=reg_slice, **config)\n\n    def build(self, input_shape):\n        if type(input_shape) == list:\n            self.input_spec = list(map(lambda x: keras.engine.InputSpec(shape=x), input_shape))\n        else:\n            self.input_spec = keras.engine.InputSpec(shape=input_shape)\n        if not self.layers:\n            self.layers = [copy.deepcopy(self.layer) for _ in range(self.layer_num)]\n        if self.hidden_dim is not None:\n            self.W = self.add_weight(\n                shape=(input_shape[-1], self.hidden_dim * self.layer_num),\n                name=\'{}_W\'.format(self.name),\n                initializer=keras.initializers.get(\'uniform\'),\n            )\n            if self.use_bias:\n                self.b = self.add_weight(\n                    shape=(self.hidden_dim * self.layer_num,),\n                    name=\'{}_b\'.format(self.name),\n                    initializer=keras.initializers.get(\'zeros\'),\n                )\n            input_shape = input_shape[:-1] + (self.hidden_dim,)\n        for i, layer in enumerate(self.layers):\n            if not layer.built:\n                if self.rename:\n                    layer.name = layer.name + \'_%d\' % (i + 1)\n                layer.build(input_shape)\n        if self.reg_index:\n            for i, (index, interval, weight) in enumerate(zip(self.reg_index, self.reg_slice, self.reg_weight)):\n                weights = []\n                if type(interval) is slice:\n                    interval = (interval,)\n                for layer in self.layers:\n                    if interval is None:\n                        weights.append(K.flatten(layer.get_weights()[index]))\n                    else:\n                        weights.append(K.flatten(layer.get_weights()[index][interval]))\n                weights = K.stack(weights)\n                self.add_loss(weight * K.sum(K.square(K.dot(weights, K.transpose(weights)) - K.eye(len(self.layers)))))\n        super(MultiHead, self).build(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        if self.hidden_dim is not None:\n            input_shape = input_shape[:-1] + (self.hidden_dim,)\n        child_output_shape = self.layers[0].compute_output_shape(input_shape)\n        return child_output_shape + (self.layer_num,)\n\n    def compute_mask(self, inputs, mask=None):\n        return self.layers[0].compute_mask(inputs, mask)\n\n    def call(self, inputs, training=None, mask=None):\n        kwargs = {}\n        if keras.utils.generic_utils.has_arg(self.layer.call, \'training\'):\n            kwargs[\'training\'] = training\n        if keras.utils.generic_utils.has_arg(self.layer.call, \'mask\') and mask is not None:\n            kwargs[\'mask\'] = mask\n        if self.hidden_dim is None:\n            outputs = [K.expand_dims(layer.call(inputs, **kwargs)) for layer in self.layers]\n        else:\n            outputs = []\n            for i, layer in enumerate(self.layers):\n                begin = i * self.hidden_dim\n                end = begin + self.hidden_dim\n                transformed = K.dot(inputs, self.W[:, begin:end])\n                if self.use_bias:\n                    transformed += self.b[begin:end]\n                outputs.append(K.expand_dims(layer.call(transformed, **kwargs)))\n        return K.concatenate(outputs, axis=-1)\n\n    @property\n    def trainable_weights(self):\n        weights = self._trainable_weights[:]\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights\n\n    @property\n    def non_trainable_weights(self):\n        weights = self._non_trainable_weights[:]\n        for layer in self.layers:\n            weights += layer.non_trainable_weights\n        return weights\n\n    @property\n    def updates(self):\n        updates = self._updates\n        for layer in self.layers:\n            if hasattr(layer, \'updates\'):\n                updates += layer.updates\n        return []\n\n    def get_updates_for(self, inputs=None):\n        inner_inputs = inputs\n        if inputs is not None:\n            uid = keras.utils.generic_utils.object_list_uid(inputs)\n            if uid in self._input_map:\n                inner_inputs = self._input_map[uid]\n\n        updates = self._updates\n        for layer in self.layers:\n            layer_updates = layer.get_updates_for(inner_inputs)\n            layer_updates += super(MultiHead, self).get_updates_for(inputs)\n            updates += layer_updates\n        return updates\n\n    @property\n    def losses(self):\n        losses = self._losses\n        for layer in self.layers:\n            if hasattr(layer, \'losses\'):\n                losses += layer.losses\n        return losses\n\n    def get_losses_for(self, inputs=None):\n        if inputs is None:\n            losses = []\n            for layer in self.layers:\n                losses = layer.get_losses_for(None)\n            return losses + super(MultiHead, self).get_losses_for(None)\n        return super(MultiHead, self).get_losses_for(inputs)\n'"
19-BERT/multi_head_attention/multi_head_attention.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom keras_self_attention import ScaledDotProductAttention\n\n\nclass MultiHeadAttention(keras.layers.Layer):\n    """"""Multi-head attention layer.\n\n    See: https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n\n    def __init__(self,\n                 head_num,\n                 activation=\'relu\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_normal\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 history_only=False,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param head_num: Number of heads.\n        :param activation: Activations for linear mappings.\n        :param use_bias: Whether to use bias term.\n        :param kernel_initializer: Initializer for linear mappings.\n        :param bias_initializer: Initializer for linear mappings.\n        :param kernel_regularizer: Regularizer for linear mappings.\n        :param bias_regularizer: Regularizer for linear mappings.\n        :param kernel_constraint: Constraints for linear mappings.\n        :param bias_constraint: Constraints for linear mappings.\n        :param history_only: Whether to only use history in attention layer.\n        """"""\n        self.supports_masking = True\n        self.head_num = head_num\n        self.activation = keras.activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n        self.bias_constraint = keras.constraints.get(bias_constraint)\n        self.history_only = history_only\n\n        self.Wq, self.Wk, self.Wv, self.Wo = None, None, None, None\n        self.bq, self.bk, self.bv, self.bo = None, None, None, None\n        super(MultiHeadAttention, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            \'head_num\': self.head_num,\n            \'activation\': keras.activations.serialize(self.activation),\n            \'use_bias\': self.use_bias,\n            \'kernel_initializer\': keras.initializers.serialize(self.kernel_initializer),\n            \'bias_initializer\': keras.initializers.serialize(self.bias_initializer),\n            \'kernel_regularizer\': keras.regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': keras.regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': keras.constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': keras.constraints.serialize(self.bias_constraint),\n            \'history_only\': self.history_only,\n        }\n        base_config = super(MultiHeadAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            q, k, v = input_shape\n            return q[:-1] + (v[-1],)\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        if isinstance(input_mask, list):\n            return input_mask[0]\n        return input_mask\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            q, k, v = input_shape\n        else:\n            q = k = v = input_shape\n        feature_dim = v[-1]\n        if feature_dim % self.head_num != 0:\n            raise IndexError(\'Invalid head number %d with the given input dim %d\' % (self.head_num, feature_dim))\n        self.Wq = self.add_weight(\n            shape=(q[-1], feature_dim),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'%s_Wq\' % self.name,\n        )\n        if self.use_bias:\n            self.bq = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'%s_bq\' % self.name,\n            )\n        self.Wk = self.add_weight(\n            shape=(k[-1], feature_dim),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'%s_Wk\' % self.name,\n        )\n        if self.use_bias:\n            self.bk = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'%s_bk\' % self.name,\n            )\n        self.Wv = self.add_weight(\n            shape=(v[-1], feature_dim),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'%s_Wv\' % self.name,\n        )\n        if self.use_bias:\n            self.bv = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'%s_bv\' % self.name,\n            )\n        self.Wo = self.add_weight(\n            shape=(feature_dim, feature_dim),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'%s_Wo\' % self.name,\n        )\n        if self.use_bias:\n            self.bo = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'%s_bo\' % self.name,\n            )\n        super(MultiHeadAttention, self).build(input_shape)\n\n    @staticmethod\n    def _reshape_to_batches(x, head_num):\n        input_shape = K.shape(x)\n        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n        head_dim = feature_dim // head_num\n        x = K.reshape(x, (batch_size, seq_len, head_num, head_dim))\n        x = K.permute_dimensions(x, [0, 2, 1, 3])\n        return K.reshape(x, (batch_size * head_num, seq_len, head_dim))\n\n    @staticmethod\n    def _reshape_from_batches(x, head_num):\n        input_shape = K.shape(x)\n        batch_size, seq_len, feature_dim = input_shape[0], input_shape[1], input_shape[2]\n        x = K.reshape(x, (batch_size // head_num, head_num, seq_len, feature_dim))\n        x = K.permute_dimensions(x, [0, 2, 1, 3])\n        return K.reshape(x, (batch_size // head_num, seq_len, feature_dim * head_num))\n\n    @staticmethod\n    def _reshape_mask(mask, head_num):\n        if mask is None:\n            return mask\n        seq_len = K.shape(mask)[1]\n        mask = K.expand_dims(mask, axis=1)\n        mask = K.tile(mask, [1, head_num, 1])\n        return K.reshape(mask, (-1, seq_len))\n\n    def call(self, inputs, mask=None):\n        if isinstance(inputs, list):\n            q, k, v = inputs\n        else:\n            q = k = v = inputs\n        if isinstance(mask, list):\n            q_mask, k_mask, v_mask = mask\n        else:\n            q_mask = k_mask = v_mask = mask\n        q = K.dot(q, self.Wq)\n        k = K.dot(k, self.Wk)\n        v = K.dot(v, self.Wv)\n        if self.use_bias:\n            q += self.bq\n            k += self.bk\n            v += self.bv\n        if self.activation is not None:\n            q = self.activation(q)\n            k = self.activation(k)\n            v = self.activation(v)\n        y = ScaledDotProductAttention(\n            history_only=self.history_only,\n            name=\'%s-Attention\' % self.name,\n        )(\n            inputs=[\n                self._reshape_to_batches(q, self.head_num),\n                self._reshape_to_batches(k, self.head_num),\n                self._reshape_to_batches(v, self.head_num),\n            ],\n            mask=[\n                self._reshape_mask(q_mask, self.head_num),\n                self._reshape_mask(k_mask, self.head_num),\n                self._reshape_mask(v_mask, self.head_num),\n            ],\n        )\n        y = self._reshape_from_batches(y, self.head_num)\n        y = K.dot(y, self.Wo)\n        if self.use_bias:\n            y += self.bo\n        if self.activation is not None:\n            y = self.activation(y)\n        return y\n'"
19-BERT/pointwise_feedforward/__init__.py,0,b'from .feed_forward import FeedForward\n'
19-BERT/pointwise_feedforward/feed_forward.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass FeedForward(keras.layers.Layer):\n    """"""Point-wise feed-forward layer.\n\n    See: https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n\n    def __init__(self,\n                 units,\n                 activation=\'relu\',\n                 use_bias=True,\n                 kernel_initializer=\'glorot_normal\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param units: Dimension of hidden units.\n        :param activation: Activation for the first linear transformation.\n        :param use_bias: Whether to use the bias term.\n        :param kernel_initializer: Initializer for kernels.\n        :param bias_initializer: Initializer for kernels.\n        :param kernel_regularizer: Regularizer for kernels.\n        :param bias_regularizer: Regularizer for kernels.\n        :param kernel_constraint: Constraint for kernels.\n        :param bias_constraint: Constraint for kernels.\n        :param kwargs:\n        """"""\n        self.supports_masking = True\n        self.units = units\n        self.activation = keras.activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n        self.bias_constraint = keras.constraints.get(bias_constraint)\n        self.W1, self.b1 = None, None\n        self.W2, self.b2 = None, None\n        super(FeedForward, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            \'units\': self.units,\n            \'activation\': keras.activations.serialize(self.activation),\n            \'use_bias\': self.use_bias,\n            \'kernel_initializer\': keras.initializers.serialize(self.kernel_initializer),\n            \'bias_initializer\': keras.initializers.serialize(self.bias_initializer),\n            \'kernel_regularizer\': keras.regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': keras.regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': keras.constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': keras.constraints.serialize(self.bias_constraint),\n        }\n        base_config = super(FeedForward, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n    def compute_mask(self, inputs, input_mask=None):\n        return input_mask\n\n    def build(self, input_shape):\n        feature_dim = input_shape[-1]\n        self.W1 = self.add_weight(\n            shape=(feature_dim, self.units),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'{}_W1\'.format(self.name),\n        )\n        if self.use_bias:\n            self.b1 = self.add_weight(\n                shape=(self.units,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'{}_b1\'.format(self.name),\n            )\n        self.W2 = self.add_weight(\n            shape=(self.units, feature_dim),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n            name=\'{}_W2\'.format(self.name),\n        )\n        if self.use_bias:\n            self.b2 = self.add_weight(\n                shape=(feature_dim,),\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n                name=\'{}_b2\'.format(self.name),\n            )\n        super(FeedForward, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        h = K.dot(x, self.W1)\n        if self.use_bias:\n            h = K.bias_add(h, self.b1)\n        if self.activation is not None:\n            h = self.activation(h)\n        y = K.dot(h, self.W2)\n        if self.use_bias:\n            y = K.bias_add(y, self.b2)\n        return y\n'"
19-BERT/position_embedding/__init__.py,0,b'from .pos_embd import PositionEmbedding\nfrom .trig_pos_embd import TrigPosEmbedding\n'
19-BERT/position_embedding/pos_embd.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass PositionEmbedding(keras.layers.Layer):\n    """"""Turn integers (positions) into dense vectors of fixed size.\n    eg. [[-4], [10]] -> [[0.25, 0.1], [0.6, -0.2]]\n\n    Expand mode: negative integers (relative position) could be used in this mode.\n        # Input shape\n            2D tensor with shape: `(batch_size, sequence_length)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n\n    Add mode:\n        # Input shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n    Concat mode:\n        # Input shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n    """"""\n    MODE_EXPAND = \'expand\'\n    MODE_ADD = \'add\'\n    MODE_CONCAT = \'concat\'\n\n    def __init__(self,\n                 input_dim,\n                 output_dim,\n                 mode=MODE_EXPAND,\n                 embeddings_initializer=\'uniform\',\n                 embeddings_regularizer=None,\n                 activity_regularizer=None,\n                 embeddings_constraint=None,\n                 mask_zero=False,\n                 **kwargs):\n        """"""\n        :param input_dim: The maximum absolute value of positions.\n        :param output_dim: The embedding dimension.\n        :param embeddings_initializer:\n        :param embeddings_regularizer:\n        :param activity_regularizer:\n        :param embeddings_constraint:\n        :param mask_zero: The index that represents padding. Only works in `append` mode.\n        :param kwargs:\n        """"""\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.mode = mode\n        self.embeddings_initializer = keras.initializers.get(embeddings_initializer)\n        self.embeddings_regularizer = keras.regularizers.get(embeddings_regularizer)\n        self.activity_regularizer = keras.regularizers.get(activity_regularizer)\n        self.embeddings_constraint = keras.constraints.get(embeddings_constraint)\n        self.mask_zero = mask_zero\n        self.supports_masking = mask_zero is not False\n\n        self.embeddings = None\n        super(PositionEmbedding, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\'input_dim\': self.input_dim,\n                  \'output_dim\': self.output_dim,\n                  \'mode\': self.mode,\n                  \'embeddings_initializer\': keras.initializers.serialize(self.embeddings_initializer),\n                  \'embeddings_regularizer\': keras.regularizers.serialize(self.embeddings_regularizer),\n                  \'activity_regularizer\': keras.regularizers.serialize(self.activity_regularizer),\n                  \'embeddings_constraint\': keras.constraints.serialize(self.embeddings_constraint),\n                  \'mask_zero\': self.mask_zero}\n        base_config = super(PositionEmbedding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        if self.mode == self.MODE_EXPAND:\n            self.embeddings = self.add_weight(\n                shape=(self.input_dim * 2 + 1, self.output_dim),\n                initializer=self.embeddings_initializer,\n                name=\'embeddings\',\n                regularizer=self.embeddings_regularizer,\n                constraint=self.embeddings_constraint,\n            )\n        else:\n            self.embeddings = self.add_weight(\n                shape=(self.input_dim, self.output_dim),\n                initializer=self.embeddings_initializer,\n                name=\'embeddings\',\n                regularizer=self.embeddings_regularizer,\n                constraint=self.embeddings_constraint,\n            )\n        super(PositionEmbedding, self).build(input_shape)\n\n    def compute_mask(self, inputs, mask=None):\n        if self.mode == self.MODE_EXPAND:\n            if self.mask_zero:\n                output_mask = K.not_equal(inputs, self.mask_zero)\n            else:\n                output_mask = None\n        else:\n            output_mask = mask\n        return output_mask\n\n    def compute_output_shape(self, input_shape):\n        if self.mode == self.MODE_EXPAND:\n            return input_shape + (self.output_dim,)\n        if self.mode == self.MODE_CONCAT:\n            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n        return input_shape\n\n    def call(self, inputs, **kwargs):\n        if self.mode == self.MODE_EXPAND:\n            if K.dtype(inputs) != \'int32\':\n                inputs = K.cast(inputs, \'int32\')\n            return K.gather(\n                self.embeddings,\n                K.minimum(K.maximum(inputs, -self.input_dim), self.input_dim) + self.input_dim,\n            )\n        input_shape = K.shape(inputs)\n        if self.mode == self.MODE_ADD:\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n        else:\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n        pos_embeddings = K.tile(\n            K.expand_dims(self.embeddings[:seq_len, :self.output_dim], axis=0),\n            [batch_size, 1, 1],\n        )\n        if self.mode == self.MODE_ADD:\n            return inputs + pos_embeddings\n        return K.concatenate([inputs, pos_embeddings], axis=-1)\n'"
19-BERT/position_embedding/trig_pos_embd.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass TrigPosEmbedding(keras.layers.Layer):\n    """"""Position embedding use sine and cosine functions.\n\n    See: https://arxiv.org/pdf/1706.03762\n\n    Expand mode:\n        # Input shape\n            2D tensor with shape: `(batch_size, sequence_length)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n\n    Add mode:\n        # Input shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n    Concat mode:\n        # Input shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim)`.\n\n        # Output shape\n            3D tensor with shape: `(batch_size, sequence_length, feature_dim + output_dim)`.\n    """"""\n    MODE_EXPAND = \'expand\'\n    MODE_ADD = \'add\'\n    MODE_CONCAT = \'concat\'\n\n    def __init__(self,\n                 mode=MODE_ADD,\n                 output_dim=None,\n                 **kwargs):\n        """"""\n        :param output_dim: The embedding dimension.\n        :param kwargs:\n        """"""\n        if mode in [self.MODE_EXPAND, self.MODE_CONCAT]:\n            if output_dim is None:\n                raise NotImplementedError(\'`output_dim` is required in `%s` mode\' % mode)\n            if output_dim % 2 != 0:\n                raise NotImplementedError(\'It does not make sense to use an odd output dimension: %d\' % output_dim)\n        self.mode = mode\n        self.output_dim = output_dim\n        self.supports_masking = True\n        super(TrigPosEmbedding, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            \'mode\': self.mode,\n            \'output_dim\': self.output_dim,\n        }\n        base_config = super(TrigPosEmbedding, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_mask(self, inputs, mask=None):\n        return mask\n\n    def compute_output_shape(self, input_shape):\n        if self.mode == self.MODE_EXPAND:\n            return input_shape + (self.output_dim,)\n        if self.mode == self.MODE_CONCAT:\n            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)\n        return input_shape\n\n    def call(self, inputs, mask=None):\n        input_shape = K.shape(inputs)\n        if self.mode == self.MODE_ADD:\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], input_shape[2]\n            pos_input = K.tile(K.expand_dims(K.arange(seq_len), axis=0), [batch_size, 1])\n        elif self.mode == self.MODE_CONCAT:\n            batch_size, seq_len, output_dim = input_shape[0], input_shape[1], self.output_dim\n            pos_input = K.tile(K.expand_dims(K.arange(seq_len), axis=0), [batch_size, 1])\n        else:\n            output_dim = self.output_dim\n            pos_input = inputs\n        if K.dtype(pos_input) != K.floatx():\n            pos_input = K.cast(pos_input, K.floatx())\n        evens = K.arange(output_dim // 2) * 2\n        odds = K.arange(output_dim // 2) * 2 + 1\n        even_embd = K.sin(\n            K.dot(\n                K.expand_dims(pos_input, -1),\n                K.expand_dims(1.0 / K.pow(\n                    10000.0,\n                    K.cast(evens, K.floatx()) / K.cast(output_dim, K.floatx())\n                ), 0)\n            )\n        )\n        odd_embd = K.cos(\n            K.dot(\n                K.expand_dims(pos_input, -1),\n                K.expand_dims(1.0 / K.pow(\n                    10000.0, K.cast((odds - 1), K.floatx()) / K.cast(output_dim, K.floatx())\n                ), 0)\n            )\n        )\n        embd = K.stack([even_embd, odd_embd], axis=-1)\n        output = K.reshape(embd, [-1, K.shape(inputs)[1], output_dim])\n        if self.mode == self.MODE_CONCAT:\n            output = K.concatenate([inputs, output], axis=-1)\n        if self.mode == self.MODE_ADD:\n            output += inputs\n        return output\n'"
19-BERT/self_attention/__init__.py,0,b'from .seq_self_attention import SeqSelfAttention\nfrom .seq_weighted_attention import SeqWeightedAttention\nfrom .scaled_dot_attention import ScaledDotProductAttention\n'
19-BERT/self_attention/scaled_dot_attention.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass ScaledDotProductAttention(keras.layers.Layer):\n    """"""The attention layer that takes three inputs representing queries, keys and values.\n\n    \\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V\n\n    See: https://arxiv.org/pdf/1706.03762.pdf\n    """"""\n\n    def __init__(self,\n                 return_attention=False,\n                 history_only=False,\n                 **kwargs):\n        """"""Initialize the layer.\n\n        :param return_attention: Whether to return attention weights.\n        :param history_only: Whether to only use history data.\n        :param kwargs: Arguments for parent class.\n        """"""\n        self.supports_masking = True\n        self.return_attention = return_attention\n        self.history_only = history_only\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            \'return_attention\': self.return_attention,\n            \'history_only\': self.history_only,\n        }\n        base_config = super(ScaledDotProductAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            query_shape, key_shape, value_shape = input_shape\n        else:\n            query_shape = key_shape = value_shape = input_shape\n        output_shape = query_shape[:-1] + value_shape[-1:]\n        if self.return_attention:\n            attention_shape = query_shape[:2] + (key_shape[1],)\n            return [output_shape, attention_shape]\n        return output_shape\n\n    def compute_mask(self, inputs, mask=None):\n        if isinstance(mask, list):\n            mask = mask[0]\n        if self.return_attention:\n            return [mask, None]\n        return mask\n\n    def call(self, inputs, mask=None, **kwargs):\n        if isinstance(inputs, list):\n            query, key, value = inputs\n        else:\n            query = key = value = inputs\n        if isinstance(mask, list):\n            mask = mask[1]\n        feature_dim = K.shape(query)[-1]\n        e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(feature_dim, dtype=K.floatx()))\n        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n        if self.history_only:\n            query_len, key_len = K.shape(query)[1], K.shape(key)[1]\n            indices = K.tile(K.expand_dims(K.arange(key_len), axis=0), [query_len, 1])\n            upper = K.expand_dims(K.arange(key_len), axis=-1)\n            e *= K.expand_dims(K.cast(indices <= upper, K.floatx()), axis=0)\n        if mask is not None:\n            e *= K.cast(K.expand_dims(mask, axis=-2), K.floatx())\n        a = e / (K.sum(e, axis=-1, keepdims=True) + K.epsilon())\n        v = K.batch_dot(a, value)\n        if self.return_attention:\n            return [v, a]\n        return v\n'"
19-BERT/self_attention/seq_self_attention.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\n\n\nclass SeqSelfAttention(keras.layers.Layer):\n\n    ATTENTION_TYPE_ADD = \'additive\'\n    ATTENTION_TYPE_MUL = \'multiplicative\'\n\n    def __init__(self,\n                 units=32,\n                 attention_width=None,\n                 attention_type=ATTENTION_TYPE_ADD,\n                 return_attention=False,\n                 history_only=False,\n                 kernel_initializer=\'glorot_normal\',\n                 bias_initializer=\'zeros\',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 use_additive_bias=True,\n                 use_attention_bias=True,\n                 attention_activation=None,\n                 attention_regularizer_weight=0.0,\n                 **kwargs):\n        """"""Layer initialization.\n\n        For additive attention, see: https://arxiv.org/pdf/1806.01264.pdf\n\n        :param units: The dimension of the vectors that used to calculate the attention weights.\n        :param attention_width: The width of local attention.\n        :param attention_type: \'additive\' or \'multiplicative\'.\n        :param return_attention: Whether to return the attention weights for visualization.\n        :param history_only: Only use historical pieces of data.\n        :param kernel_initializer: The initializer for weight matrices.\n        :param bias_initializer: The initializer for biases.\n        :param kernel_regularizer: The regularization for weight matrices.\n        :param bias_regularizer: The regularization for biases.\n        :param kernel_constraint: The constraint for weight matrices.\n        :param bias_constraint: The constraint for biases.\n        :param use_additive_bias: Whether to use bias while calculating the relevance of inputs features\n                                  in additive mode.\n        :param use_attention_bias: Whether to use bias while calculating the weights of attention.\n        :param attention_activation: The activation used for calculating the weights of attention.\n        :param attention_regularizer_weight: The weights of attention regularizer.\n        :param kwargs: Parameters for parent class.\n        """"""\n        self.supports_masking = True\n        self.units = units\n        self.attention_width = attention_width\n        self.attention_type = attention_type\n        self.return_attention = return_attention\n        self.history_only = history_only\n        if history_only and attention_width is None:\n            self.attention_width = int(1e9)\n\n        self.use_additive_bias = use_additive_bias\n        self.use_attention_bias = use_attention_bias\n        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n        self.bias_initializer = keras.initializers.get(bias_initializer)\n        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n        self.kernel_constraint = keras.constraints.get(kernel_constraint)\n        self.bias_constraint = keras.constraints.get(bias_constraint)\n        self.attention_activation = keras.activations.get(attention_activation)\n        self.attention_regularizer_weight = attention_regularizer_weight\n        self._backend = keras.backend.backend()\n\n        if attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self.Wx, self.Wt, self.bh = None, None, None\n            self.Wa, self.ba = None, None\n        elif attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self.Wa, self.ba = None, None\n        else:\n            raise NotImplementedError(\'No implementation for attention type : \' + attention_type)\n\n        super(SeqSelfAttention, self).__init__(**kwargs)\n\n    def get_config(self):\n        config = {\n            \'units\': self.units,\n            \'attention_width\': self.attention_width,\n            \'attention_type\': self.attention_type,\n            \'return_attention\': self.return_attention,\n            \'history_only\': self.history_only,\n            \'use_additive_bias\': self.use_additive_bias,\n            \'use_attention_bias\': self.use_attention_bias,\n            \'kernel_initializer\': keras.regularizers.serialize(self.kernel_initializer),\n            \'bias_initializer\': keras.regularizers.serialize(self.bias_initializer),\n            \'kernel_regularizer\': keras.regularizers.serialize(self.kernel_regularizer),\n            \'bias_regularizer\': keras.regularizers.serialize(self.bias_regularizer),\n            \'kernel_constraint\': keras.constraints.serialize(self.kernel_constraint),\n            \'bias_constraint\': keras.constraints.serialize(self.bias_constraint),\n            \'attention_activation\': keras.activations.serialize(self.attention_activation),\n            \'attention_regularizer_weight\': self.attention_regularizer_weight,\n        }\n        base_config = super(SeqSelfAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            self._build_additive_attention(input_shape)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            self._build_multiplicative_attention(input_shape)\n        super(SeqSelfAttention, self).build(input_shape)\n\n    def _build_additive_attention(self, input_shape):\n        feature_dim = input_shape[2]\n\n        self.Wt = self.add_weight(shape=(feature_dim, self.units),\n                                  name=\'{}_Add_Wt\'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        self.Wx = self.add_weight(shape=(feature_dim, self.units),\n                                  name=\'{}_Add_Wx\'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_additive_bias:\n            self.bh = self.add_weight(shape=(self.units,),\n                                      name=\'{}_Add_bh\'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n        self.Wa = self.add_weight(shape=(self.units, 1),\n                                  name=\'{}_Add_Wa\'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name=\'{}_Add_ba\'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def _build_multiplicative_attention(self, input_shape):\n        feature_dim = input_shape[2]\n\n        self.Wa = self.add_weight(shape=(feature_dim, feature_dim),\n                                  name=\'{}_Mul_Wa\'.format(self.name),\n                                  initializer=self.kernel_initializer,\n                                  regularizer=self.kernel_regularizer,\n                                  constraint=self.kernel_constraint)\n        if self.use_attention_bias:\n            self.ba = self.add_weight(shape=(1,),\n                                      name=\'{}_Mul_ba\'.format(self.name),\n                                      initializer=self.bias_initializer,\n                                      regularizer=self.bias_regularizer,\n                                      constraint=self.bias_constraint)\n\n    def call(self, inputs, mask=None, **kwargs):\n        input_len = K.shape(inputs)[1]\n\n        if self.attention_type == SeqSelfAttention.ATTENTION_TYPE_ADD:\n            e = self._call_additive_emission(inputs)\n        elif self.attention_type == SeqSelfAttention.ATTENTION_TYPE_MUL:\n            e = self._call_multiplicative_emission(inputs)\n\n        if self.attention_activation is not None:\n            e = self.attention_activation(e)\n        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n        if self.attention_width is not None:\n            if self.history_only:\n                lower = K.arange(input_len) - (self.attention_width - 1)\n            else:\n                lower = K.arange(input_len) - self.attention_width // 2\n            lower = K.expand_dims(lower, axis=-1)\n            upper = lower + self.attention_width\n            indices = K.tile(K.expand_dims(K.arange(input_len), axis=0), [input_len, 1])\n            e = e * K.cast(lower <= indices, K.floatx()) * K.cast(indices < upper, K.floatx())\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            mask = K.expand_dims(mask)\n            e = K.permute_dimensions(K.permute_dimensions(e * mask, (0, 2, 1)) * mask, (0, 2, 1))\n\n        # a_{t} = \\text{softmax}(e_t)\n        s = K.sum(e, axis=-1, keepdims=True)\n        a = e / (s + K.epsilon())\n\n        # l_t = \\sum_{t\'} a_{t, t\'} x_{t\'}\n        v = K.batch_dot(a, inputs)\n        if self.attention_regularizer_weight > 0.0:\n            self.add_loss(self._attention_regularizer(a))\n\n        if self.return_attention:\n            return [v, a]\n        return v\n\n    def _call_additive_emission(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size, input_len = input_shape[0], input_shape[1]\n\n        # h_{t, t\'} = \\tanh(x_t^T W_t + x_{t\'}^T W_x + b_h)\n        q, k = K.dot(inputs, self.Wt), K.dot(inputs, self.Wx)\n        q = K.tile(K.expand_dims(q, 2), [1, 1, input_len, 1])\n        k = K.tile(K.expand_dims(k, 1), [1, input_len, 1, 1])\n        if self.use_additive_bias:\n            h = K.tanh(q + k + self.bh)\n        else:\n            h = K.tanh(q + k)\n\n        # e_{t, t\'} = W_a h_{t, t\'} + b_a\n        if self.use_attention_bias:\n            e = K.reshape(K.dot(h, self.Wa) + self.ba, (batch_size, input_len, input_len))\n        else:\n            e = K.reshape(K.dot(h, self.Wa), (batch_size, input_len, input_len))\n        return e\n\n    def _call_multiplicative_emission(self, inputs):\n        # e_{t, t\'} = x_t^T W_a x_{t\'} + b_a\n        e = K.batch_dot(K.dot(inputs, self.Wa), K.permute_dimensions(inputs, (0, 2, 1)))\n        if self.use_attention_bias:\n            e = e + self.ba\n        return e\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape, pos_shape = input_shape\n            output_shape = (input_shape[0], pos_shape[1], input_shape[2])\n        else:\n            output_shape = input_shape\n        if self.return_attention:\n            attention_shape = (input_shape[0], output_shape[1], input_shape[1])\n            return [output_shape, attention_shape]\n        return output_shape\n\n    def compute_mask(self, inputs, mask=None):\n        if isinstance(inputs, list):\n            mask = mask[1]\n        if self.return_attention:\n            return [mask, None]\n        return mask\n\n    def _attention_regularizer(self, attention):\n        batch_size = K.cast(K.shape(attention)[0], K.floatx())\n        input_len = K.shape(attention)[-1]\n        indices = K.tile(K.expand_dims(K.arange(input_len), axis=0), [input_len, 1])\n        diagonal = K.expand_dims(K.arange(input_len), axis=-1)\n        eye = K.cast(K.equal(indices, diagonal), K.floatx())\n        return self.attention_regularizer_weight * K.sum(K.square(K.batch_dot(\n            attention,\n            K.permute_dimensions(attention, (0, 2, 1))) - eye)) / batch_size\n\n    @staticmethod\n    def get_custom_objects():\n        return {\'SeqSelfAttention\': SeqSelfAttention}\n'"
19-BERT/self_attention/seq_weighted_attention.py,0,"b'from tensorflow import keras\nimport tensorflow.keras.backend as K\n\n\nclass SeqWeightedAttention(keras.layers.Layer):\n    """"""Y = \\text{softmax}(XW + b) X\n\n    See: https://arxiv.org/pdf/1708.00524.pdf\n    """"""\n\n    def __init__(self, use_bias=True, return_attention=False, **kwargs):\n        self.supports_masking = True\n        self.use_bias = use_bias\n        self.return_attention = return_attention\n        self.W, self.b = None, None\n        super(SeqWeightedAttention, self).__init__(** kwargs)\n\n    def get_config(self):\n        config = {\n            \'use_bias\': self.use_bias,\n            \'return_attention\': self.return_attention,\n        }\n        base_config = super(SeqWeightedAttention, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def build(self, input_shape):\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name=\'{}_W\'.format(self.name),\n                                 initializer=keras.initializers.get(\'uniform\'))\n        if self.use_bias:\n            self.b = self.add_weight(shape=(1,),\n                                     name=\'{}_b\'.format(self.name),\n                                     initializer=keras.initializers.get(\'zeros\'))\n        super(SeqWeightedAttention, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        logits = K.dot(x, self.W)\n        if self.use_bias:\n            logits += self.b\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return input_shape[0], output_len\n\n    def compute_mask(self, _, input_mask=None):\n        if self.return_attention:\n            return [None, None]\n        return None\n\n    @staticmethod\n    def get_custom_objects():\n        return {\'SeqWeightedAttention\': SeqWeightedAttention}\n'"
19-BERT/transformer/__init__.py,0,b'from .gelu import gelu\nfrom .transformer import *\n'
19-BERT/transformer/gelu.py,0,"b'import math\nimport tensorflow.keras.backend as K\n\n\ndef gelu(x):\n    """"""An approximation of gelu.\n\n    See: https://arxiv.org/pdf/1606.08415.pdf\n    """"""\n    return 0.5 * x * (1.0 + K.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n'"
19-BERT/transformer/transformer.py,0,"b'from tensorflow import keras\nimport numpy as np\nfrom keras_layer_normalization import LayerNormalization\nfrom keras_multi_head import MultiHeadAttention\nfrom keras_position_wise_feed_forward import FeedForward\nfrom keras_pos_embd import TrigPosEmbedding\nfrom keras_embed_sim import EmbeddingRet, EmbeddingSim\n\n\n__all__ = [\n    \'get_custom_objects\', \'get_encoders\', \'get_decoders\', \'get_model\', \'decode\',\n    \'attention_builder\', \'feed_forward_builder\', \'get_encoder_component\', \'get_decoder_component\',\n]\n\n\ndef get_custom_objects():\n    return {\n        \'LayerNormalization\': LayerNormalization,\n        \'MultiHeadAttention\': MultiHeadAttention,\n        \'FeedForward\': FeedForward,\n        \'TrigPosEmbedding\': TrigPosEmbedding,\n        \'EmbeddingRet\': EmbeddingRet,\n        \'EmbeddingSim\': EmbeddingSim,\n    }\n\n\ndef _wrap_layer(name, input_layer, build_func, dropout_rate=0.0, trainable=True):\n    """"""Wrap layers with residual, normalization and dropout.\n\n    :param name: Prefix of names for internal layers.\n    :param input_layer: Input layer.\n    :param build_func: A callable that takes the input tensor and generates the output tensor.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: Output layer.\n    """"""\n    build_output = build_func(input_layer)\n    if dropout_rate > 0.0:\n        dropout_layer = keras.layers.Dropout(\n            rate=dropout_rate,\n            name=\'%s-Dropout\' % name,\n        )(build_output)\n    else:\n        dropout_layer = build_output\n    if isinstance(input_layer, list):\n        input_layer = input_layer[0]\n    add_layer = keras.layers.Add(name=\'%s-Add\' % name)([input_layer, dropout_layer])\n    normal_layer = LayerNormalization(\n        trainable=trainable,\n        name=\'%s-Norm\' % name,\n    )(add_layer)\n    return normal_layer\n\n\ndef attention_builder(name, head_num, activation, history_only, trainable=True):\n    """"""Get multi-head self-attention builder.\n\n    :param name: Prefix of names for internal layers.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param activation: Activation for multi-head self-attention.\n    :param history_only: Only use history data.\n    :param trainable: Whether the layer is trainable.\n    :return:\n    """"""\n    def _attention_builder(x):\n        return MultiHeadAttention(\n            head_num=head_num,\n            activation=activation,\n            history_only=history_only,\n            trainable=trainable,\n            name=name,\n        )(x)\n    return _attention_builder\n\n\ndef feed_forward_builder(name, hidden_dim, activation, trainable=True):\n    """"""Get position-wise feed-forward layer builder.\n\n    :param name: Prefix of names for internal layers.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param activation: Activation for feed-forward layer.\n    :param trainable: Whether the layer is trainable.\n    :return:\n    """"""\n    def _feed_forward_builder(x):\n        return FeedForward(\n            units=hidden_dim,\n            activation=activation,\n            trainable=trainable,\n            name=name,\n        )(x)\n    return _feed_forward_builder\n\n\ndef get_encoder_component(name,\n                          input_layer,\n                          head_num,\n                          hidden_dim,\n                          attention_activation=None,\n                          feed_forward_activation=\'relu\',\n                          dropout_rate=0.0,\n                          trainable=True):\n    """"""Multi-head self-attention and feed-forward layer.\n\n    :param name: Prefix of names for internal layers.\n    :param input_layer: Input layer.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param attention_activation: Activation for multi-head self-attention.\n    :param feed_forward_activation: Activation for feed-forward layer.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: Output layer.\n    """"""\n    attention_name = \'%s-MultiHeadSelfAttention\' % name\n    feed_forward_name = \'%s-FeedForward\' % name\n    attention_layer = _wrap_layer(\n        name=attention_name,\n        input_layer=input_layer,\n        build_func=attention_builder(\n            name=attention_name,\n            head_num=head_num,\n            activation=attention_activation,\n            history_only=False,\n            trainable=trainable,\n        ),\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    feed_forward_layer = _wrap_layer(\n        name=feed_forward_name,\n        input_layer=attention_layer,\n        build_func=feed_forward_builder(\n            name=feed_forward_name,\n            hidden_dim=hidden_dim,\n            activation=feed_forward_activation,\n            trainable=trainable,\n        ),\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    return feed_forward_layer\n\n\ndef get_decoder_component(name,\n                          input_layer,\n                          encoded_layer,\n                          head_num,\n                          hidden_dim,\n                          attention_activation=None,\n                          feed_forward_activation=\'relu\',\n                          dropout_rate=0.0,\n                          trainable=True):\n    """"""Multi-head self-attention, multi-head query attention and feed-forward layer.\n\n    :param name: Prefix of names for internal layers.\n    :param input_layer: Input layer.\n    :param encoded_layer: Encoded layer from encoder.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param attention_activation: Activation for multi-head self-attention.\n    :param feed_forward_activation: Activation for feed-forward layer.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: Output layer.\n    """"""\n    self_attention_name = \'%s-MultiHeadSelfAttention\' % name\n    query_attention_name = \'%s-MultiHeadQueryAttention\' % name\n    feed_forward_name = \'%s-FeedForward\' % name\n    self_attention_layer = _wrap_layer(\n        name=self_attention_name,\n        input_layer=input_layer,\n        build_func=attention_builder(\n            name=self_attention_name,\n            head_num=head_num,\n            activation=attention_activation,\n            history_only=True,\n            trainable=trainable,\n        ),\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    query_attention_layer = _wrap_layer(\n        name=query_attention_name,\n        input_layer=[self_attention_layer, encoded_layer, encoded_layer],\n        build_func=attention_builder(\n            name=query_attention_name,\n            head_num=head_num,\n            activation=attention_activation,\n            history_only=False,\n            trainable=trainable,\n        ),\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    feed_forward_layer = _wrap_layer(\n        name=feed_forward_name,\n        input_layer=query_attention_layer,\n        build_func=feed_forward_builder(\n            name=feed_forward_name,\n            hidden_dim=hidden_dim,\n            activation=feed_forward_activation,\n            trainable=trainable,\n        ),\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    return feed_forward_layer\n\n\ndef get_encoders(encoder_num,\n                 input_layer,\n                 head_num,\n                 hidden_dim,\n                 attention_activation=None,\n                 feed_forward_activation=\'relu\',\n                 dropout_rate=0.0,\n                 trainable=True):\n    """"""Get encoders.\n\n    :param encoder_num: Number of encoder components.\n    :param input_layer: Input layer.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param attention_activation: Activation for multi-head self-attention.\n    :param feed_forward_activation: Activation for feed-forward layer.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: Output layer.\n    """"""\n    last_layer = input_layer\n    for i in range(encoder_num):\n        last_layer = get_encoder_component(\n            name=\'Encoder-%d\' % (i + 1),\n            input_layer=last_layer,\n            head_num=head_num,\n            hidden_dim=hidden_dim,\n            attention_activation=attention_activation,\n            feed_forward_activation=feed_forward_activation,\n            dropout_rate=dropout_rate,\n            trainable=trainable,\n        )\n    return last_layer\n\n\ndef get_decoders(decoder_num,\n                 input_layer,\n                 encoded_layer,\n                 head_num,\n                 hidden_dim,\n                 attention_activation=None,\n                 feed_forward_activation=\'relu\',\n                 dropout_rate=0.0,\n                 trainable=True):\n    """"""Get decoders.\n\n    :param decoder_num: Number of decoder components.\n    :param input_layer: Input layer.\n    :param encoded_layer: Encoded layer from encoder.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param attention_activation: Activation for multi-head self-attention.\n    :param feed_forward_activation: Activation for feed-forward layer.\n    :param dropout_rate: Dropout rate.\n    :param trainable: Whether the layers are trainable.\n    :return: Output layer.\n    """"""\n    last_layer = input_layer\n    for i in range(decoder_num):\n        last_layer = get_decoder_component(\n            name=\'Decoder-%d\' % (i + 1),\n            input_layer=last_layer,\n            encoded_layer=encoded_layer,\n            head_num=head_num,\n            hidden_dim=hidden_dim,\n            attention_activation=attention_activation,\n            feed_forward_activation=feed_forward_activation,\n            dropout_rate=dropout_rate,\n            trainable=trainable,\n        )\n    return last_layer\n\n\ndef get_model(token_num,\n              embed_dim,\n              encoder_num,\n              decoder_num,\n              head_num,\n              hidden_dim,\n              attention_activation=None,\n              feed_forward_activation=\'relu\',\n              dropout_rate=0.0,\n              use_same_embed=True,\n              embed_weights=None,\n              embed_trainable=None,\n              trainable=True):\n    """"""Get full model without compilation.\n\n    :param token_num: Number of distinct tokens.\n    :param embed_dim: Dimension of token embedding.\n    :param encoder_num: Number of encoder components.\n    :param decoder_num: Number of decoder components.\n    :param head_num: Number of heads in multi-head self-attention.\n    :param hidden_dim: Hidden dimension of feed forward layer.\n    :param attention_activation: Activation for multi-head self-attention.\n    :param feed_forward_activation: Activation for feed-forward layer.\n    :param dropout_rate: Dropout rate.\n    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n                           `embed_trainable` should be lists of two elements if it is False.\n    :param embed_weights: Initial weights of token embedding.\n    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n                            value is None when embedding weights has been provided.\n    :param trainable: Whether the layers are trainable.\n    :return: Keras model.\n    """"""\n    if not isinstance(token_num, list):\n        token_num = [token_num, token_num]\n    encoder_token_num, decoder_token_num = token_num\n\n    if not isinstance(embed_weights, list):\n        embed_weights = [embed_weights, embed_weights]\n    encoder_embed_weights, decoder_embed_weights = embed_weights\n    if encoder_embed_weights is not None:\n        encoder_embed_weights = [encoder_embed_weights]\n    if decoder_embed_weights is not None:\n        decoder_embed_weights = [decoder_embed_weights]\n\n    if not isinstance(embed_trainable, list):\n        embed_trainable = [embed_trainable, embed_trainable]\n    encoder_embed_trainable, decoder_embed_trainable = embed_trainable\n    if encoder_embed_trainable is None:\n        encoder_embed_trainable = encoder_embed_weights is None\n    if decoder_embed_trainable is None:\n        decoder_embed_trainable = decoder_embed_weights is None\n\n    if use_same_embed:\n        encoder_embed_layer = decoder_embed_layer = EmbeddingRet(\n            input_dim=encoder_token_num,\n            output_dim=embed_dim,\n            mask_zero=True,\n            weights=encoder_embed_weights,\n            trainable=encoder_embed_trainable,\n            name=\'Token-Embedding\',\n        )\n    else:\n        encoder_embed_layer = EmbeddingRet(\n            input_dim=encoder_token_num,\n            output_dim=embed_dim,\n            mask_zero=True,\n            weights=encoder_embed_weights,\n            trainable=encoder_embed_trainable,\n            name=\'Encoder-Token-Embedding\',\n        )\n        decoder_embed_layer = EmbeddingRet(\n            input_dim=decoder_token_num,\n            output_dim=embed_dim,\n            mask_zero=True,\n            weights=decoder_embed_weights,\n            trainable=decoder_embed_trainable,\n            name=\'Decoder-Token-Embedding\',\n        )\n    encoder_input = keras.layers.Input(shape=(None,), name=\'Encoder-Input\')\n    encoder_embed = TrigPosEmbedding(\n        mode=TrigPosEmbedding.MODE_ADD,\n        name=\'Encoder-Embedding\',\n    )(encoder_embed_layer(encoder_input)[0])\n    encoded_layer = get_encoders(\n        encoder_num=encoder_num,\n        input_layer=encoder_embed,\n        head_num=head_num,\n        hidden_dim=hidden_dim,\n        attention_activation=attention_activation,\n        feed_forward_activation=feed_forward_activation,\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    decoder_input = keras.layers.Input(shape=(None,), name=\'Decoder-Input\')\n    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n    decoder_embed = TrigPosEmbedding(\n        mode=TrigPosEmbedding.MODE_ADD,\n        name=\'Decoder-Embedding\',\n    )(decoder_embed)\n    decoded_layer = get_decoders(\n        decoder_num=decoder_num,\n        input_layer=decoder_embed,\n        encoded_layer=encoded_layer,\n        head_num=head_num,\n        hidden_dim=hidden_dim,\n        attention_activation=attention_activation,\n        feed_forward_activation=feed_forward_activation,\n        dropout_rate=dropout_rate,\n        trainable=trainable,\n    )\n    dense_layer = EmbeddingSim(\n        trainable=trainable,\n        name=\'Output\',\n    )([decoded_layer, decoder_embed_weights])\n    return keras.models.Model(inputs=[encoder_input, decoder_input], outputs=dense_layer)\n\n\ndef _get_max_suffix_repeat_times(tokens, max_len):\n    detect_len = min(max_len, len(tokens))\n    next = [-1] * detect_len\n    k = -1\n    for i in range(1, detect_len):\n        while k >= 0 and tokens[len(tokens) - i - 1] != tokens[len(tokens) - k - 2]:\n            k = next[k]\n        if tokens[len(tokens) - i - 1] == tokens[len(tokens) - k - 2]:\n            k += 1\n        next[i] = k\n    max_repeat = 1\n    for i in range(2, detect_len):\n        if next[i] >= 0 and (i + 1) % (i - next[i]) == 0:\n            max_repeat = max(max_repeat, (i + 1) // (i - next[i]))\n    return max_repeat\n\n\ndef decode(model, tokens, start_token, end_token, pad_token, max_len=10000, max_repeat=10, max_repeat_block=10):\n    """"""Decode with the given model and input tokens.\n\n    :param model: The trained model.\n    :param tokens: The input tokens of encoder.\n    :param start_token: The token that represents the start of a sentence.\n    :param end_token: The token that represents the end of a sentence.\n    :param pad_token: The token that represents padding.\n    :param max_len: Maximum length of decoded list.\n    :param max_repeat: Maximum number of repeating blocks.\n    :param max_repeat_block: Maximum length of the repeating block.\n    :return: Decoded tokens.\n    """"""\n    is_single = not isinstance(tokens[0], list)\n    if is_single:\n        tokens = [tokens]\n    batch_size = len(tokens)\n    decoder_inputs = [[start_token] for _ in range(batch_size)]\n    outputs = [None for _ in range(batch_size)]\n    output_len = 1\n    while len(list(filter(lambda x: x is None, outputs))) > 0:\n        output_len += 1\n        batch_inputs, batch_outputs = [], []\n        max_input_len = 0\n        index_map = {}\n        for i in range(batch_size):\n            if outputs[i] is None:\n                index_map[len(batch_inputs)] = i\n                batch_inputs.append(tokens[i][:])\n                batch_outputs.append(decoder_inputs[i])\n                max_input_len = max(max_input_len, len(tokens[i]))\n        for i in range(len(batch_inputs)):\n            batch_inputs[i] += [pad_token] * (max_input_len - len(batch_inputs[i]))\n        predicts = model.predict([np.asarray(batch_inputs), np.asarray(batch_outputs)])\n        for i in range(len(predicts)):\n            last_token = np.argmax(predicts[i][-1])\n            decoder_inputs[index_map[i]].append(last_token)\n            if last_token == end_token or\\\n                    (max_len is not None and output_len >= max_len) or\\\n                    _get_max_suffix_repeat_times(decoder_inputs, max_repeat * max_repeat_block) >= max_repeat:\n                outputs[index_map[i]] = decoder_inputs[index_map[i]]\n    if is_single:\n        outputs = outputs[0]\n    return outputs\n'"
TensorFlow-PPT/lesson01-TensorFlow2.0/autograd.py,5,"b'import tensorflow as tf \n\n\nx = tf.constant(1.)\na = tf.constant(2.)\nb = tf.constant(3.)\nc = tf.constant(4.)\n\n\nwith tf.GradientTape() as tape:\n\ttape.watch([a, b, c])\n\ty = a**2 * x + b * x + c\n\n\n[dy_da, dy_db, dy_dc] = tape.gradient(y, [a, b, c])\nprint(dy_da, dy_db, dy_dc)\n\n'"
TensorFlow-PPT/lesson01-TensorFlow2.0/gpu_accelerate.py,10,"b""import tensorflow as tf\nimport timeit\n\n\nwith tf.device('/cpu:0'):\n\tcpu_a = tf.random.normal([10000, 1000])\n\tcpu_b = tf.random.normal([1000, 2000])\n\tprint(cpu_a.device, cpu_b.device)\n\nwith tf.device('/gpu:0'):\n\tgpu_a = tf.random.normal([10000, 1000])\n\tgpu_b = tf.random.normal([1000, 2000])\n\tprint(gpu_a.device, gpu_b.device)\n\ndef cpu_run():\n\twith tf.device('/cpu:0'):\n\t\tc = tf.matmul(cpu_a, cpu_b)\n\treturn c \n\ndef gpu_run():\n\twith tf.device('/gpu:0'):\n\t\tc = tf.matmul(gpu_a, gpu_b)\n\treturn c \n\n\n# warm up\ncpu_time = timeit.timeit(cpu_run, number=10)\ngpu_time = timeit.timeit(gpu_run, number=10)\nprint('warmup:', cpu_time, gpu_time)\n\n\ncpu_time = timeit.timeit(cpu_run, number=10)\ngpu_time = timeit.timeit(gpu_run, number=10)\nprint('run time:', cpu_time, gpu_time)"""
TensorFlow-PPT/lesson02-/test.py,3,"b""import os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n\r\na = tf.constant(1.)\r\nb = tf.constant(2.)\r\nprint(a+b)\r\n\r\nprint('GPU:', tf.test.is_gpu_available())"""
TensorFlow-PPT/lesson04-/linear_regression.py,0,"b'import numpy as np\r\n\r\n\r\n\r\n\r\n# y = wx + b\r\ndef compute_error_for_line_given_points(b, w, points):\r\n    totalError = 0\r\n    for i in range(0, len(points)):\r\n        x = points[i, 0]\r\n        y = points[i, 1]\r\n        # computer mean-squared-error\r\n        totalError += (y - (w * x + b)) ** 2\r\n    # average loss for each point\r\n    return totalError / float(len(points))\r\n\r\n\r\n\r\ndef step_gradient(b_current, w_current, points, learningRate):\r\n    b_gradient = 0\r\n    w_gradient = 0\r\n    N = float(len(points))\r\n    for i in range(0, len(points)):\r\n        x = points[i, 0]\r\n        y = points[i, 1]\r\n        # grad_b = 2(wx+b-y)\r\n        b_gradient += (2/N) * ((w_current * x + b_current) - y)\r\n        # grad_w = 2(wx+b-y)*x\r\n        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)\r\n    # update w\'\r\n    new_b = b_current - (learningRate * b_gradient)\r\n    new_w = w_current - (learningRate * w_gradient)\r\n    return [new_b, new_w]\r\n\r\ndef gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):\r\n    b = starting_b\r\n    w = starting_w\r\n    # update for several times\r\n    for i in range(num_iterations):\r\n        b, w = step_gradient(b, w, np.array(points), learning_rate)\r\n    return [b, w]\r\n\r\n\r\ndef run():\r\n\t\r\n    points = np.genfromtxt(""data.csv"", delimiter="","")\r\n    learning_rate = 0.0001\r\n    initial_b = 0 # initial y-intercept guess\r\n    initial_w = 0 # initial slope guess\r\n    num_iterations = 1000\r\n    print(""Starting gradient descent at b = {0}, w = {1}, error = {2}""\r\n          .format(initial_b, initial_w,\r\n                  compute_error_for_line_given_points(initial_b, initial_w, points))\r\n          )\r\n    print(""Running..."")\r\n    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\r\n    print(""After {0} iterations b = {1}, w = {2}, error = {3}"".\r\n          format(num_iterations, b, w,\r\n                 compute_error_for_line_given_points(b, w, points))\r\n          )\r\n\r\nif __name__ == \'__main__\':\r\n    run()'"
TensorFlow-PPT/lesson06-/main.py,7,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\n\n\n(x, y), (x_val, y_val) = datasets.mnist.load_data() \nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\ny = tf.convert_to_tensor(y, dtype=tf.int32)\ny = tf.one_hot(y, depth=10)\nprint(x.shape, y.shape)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x, y))\ntrain_dataset = train_dataset.batch(200)\n\n \n\n\nmodel = keras.Sequential([ \n    layers.Dense(512, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10)])\n\noptimizer = optimizers.SGD(learning_rate=0.001)\n\n\ndef train_epoch(epoch):\n\n    # Step4.loop\n    for step, (x, y) in enumerate(train_dataset):\n\n\n        with tf.GradientTape() as tape:\n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # Step1. compute output\n            # [b, 784] => [b, 10]\n            out = model(x)\n            # Step2. compute loss\n            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]\n\n        # Step3. optimize and update w1, w2, w3, b1, b2, b3\n        grads = tape.gradient(loss, model.trainable_variables)\n        # w' = w - lr * grad\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', loss.numpy())\n\n\n\ndef train():\n\n    for epoch in range(30):\n\n        train_epoch(epoch)\n\n\n\n\n\n\nif __name__ == '__main__':\n    train()"""
TensorFlow-PPT/lesson13--/forward.py,19,"b""import  tensorflow as tf\r\nfrom    tensorflow import keras\r\nfrom    tensorflow.keras import datasets\r\nimport  os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n# x: [60k, 28, 28],\r\n# y: [60k]\r\n(x, y), _ = datasets.mnist.load_data()\r\n# x: [0~255] => [0~1.]\r\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\r\ny = tf.convert_to_tensor(y, dtype=tf.int32)\r\n\r\nprint(x.shape, y.shape, x.dtype, y.dtype)\r\nprint(tf.reduce_min(x), tf.reduce_max(x))\r\nprint(tf.reduce_min(y), tf.reduce_max(y))\r\n\r\n\r\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\r\ntrain_iter = iter(train_db)\r\nsample = next(train_iter)\r\nprint('batch:', sample[0].shape, sample[1].shape)\r\n\r\n\r\n# [b, 784] => [b, 256] => [b, 128] => [b, 10]\r\n# [dim_in, dim_out], [dim_out]\r\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\r\nb1 = tf.Variable(tf.zeros([256]))\r\nw2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\r\nb2 = tf.Variable(tf.zeros([128]))\r\nw3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\r\nb3 = tf.Variable(tf.zeros([10]))\r\n\r\nlr = 1e-3\r\n\r\nfor epoch in range(10): # iterate db for 10\r\n    for step, (x, y) in enumerate(train_db): # for every batch\r\n        # x:[128, 28, 28]\r\n        # y: [128]\r\n\r\n        # [b, 28, 28] => [b, 28*28]\r\n        x = tf.reshape(x, [-1, 28*28])\r\n\r\n        with tf.GradientTape() as tape: # tf.Variable\r\n            # x: [b, 28*28]\r\n            # h1 = x@w1 + b1\r\n            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\r\n            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\r\n            h1 = tf.nn.relu(h1)\r\n            # [b, 256] => [b, 128]\r\n            h2 = h1@w2 + b2\r\n            h2 = tf.nn.relu(h2)\r\n            # [b, 128] => [b, 10]\r\n            out = h2@w3 + b3\r\n\r\n            # compute loss\r\n            # out: [b, 10]\r\n            # y: [b] => [b, 10]\r\n            y_onehot = tf.one_hot(y, depth=10)\r\n\r\n            # mse = mean(sum(y-out)^2)\r\n            # [b, 10]\r\n            loss = tf.square(y_onehot - out)\r\n            # mean: scalar\r\n            loss = tf.reduce_mean(loss)\r\n\r\n        # compute gradients\r\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\r\n        # print(grads)\r\n        # w1 = w1 - lr * w1_grad\r\n        w1.assign_sub(lr * grads[0])\r\n        b1.assign_sub(lr * grads[1])\r\n        w2.assign_sub(lr * grads[2])\r\n        b2.assign_sub(lr * grads[3])\r\n        w3.assign_sub(lr * grads[4])\r\n        b3.assign_sub(lr * grads[5])\r\n\r\n\r\n        if step % 100 == 0:\r\n            print(epoch, step, 'loss:', float(loss))\r\n"""
TensorFlow-PPT/lesson16-/topk.py,11,"b""import  tensorflow as tf\r\nimport  os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\ntf.random.set_seed(2467)\r\n\r\ndef accuracy(output, target, topk=(1,)):\r\n    maxk = max(topk)\r\n    batch_size = target.shape[0]\r\n\r\n    pred = tf.math.top_k(output, maxk).indices\r\n    pred = tf.transpose(pred, perm=[1, 0])\r\n    target_ = tf.broadcast_to(target, pred.shape)\r\n    # [10, b]\r\n    correct = tf.equal(pred, target_)\r\n\r\n    res = []\r\n    for k in topk:\r\n        correct_k = tf.cast(tf.reshape(correct[:k], [-1]), dtype=tf.float32)\r\n        correct_k = tf.reduce_sum(correct_k)\r\n        acc = float(correct_k* (100.0 / batch_size) )\r\n        res.append(acc)\r\n\r\n    return res\r\n\r\n\r\n\r\noutput = tf.random.normal([10, 6])\r\noutput = tf.math.softmax(output, axis=1)\r\ntarget = tf.random.uniform([10], maxval=6, dtype=tf.int32)\r\nprint('prob:', output.numpy())\r\npred = tf.argmax(output, axis=1)\r\nprint('pred:', pred.numpy())\r\nprint('label:', target.numpy())\r\n\r\nacc = accuracy(output, target, topk=(1,2,3,4,5,6))\r\nprint('top-1-6 acc:', acc)"""
TensorFlow-PPT/lesson18-/main.py,19,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nprint(tf.__version__)\n\n(x, y), _ = datasets.mnist.load_data()\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 50.\ny = tf.convert_to_tensor(y)\ny = tf.one_hot(y, depth=10)\nprint('x:', x.shape, 'y:', y.shape)\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128).repeat(30)\nx,y = next(iter(train_db))\nprint('sample:', x.shape, y.shape)\n# print(x[0], y[0])\n\n\n\ndef main():\n\n    # 784 => 512\n    w1, b1 = tf.Variable(tf.random.truncated_normal([784, 512], stddev=0.1)), tf.Variable(tf.zeros([512]))\n    # 512 => 256\n    w2, b2 = tf.Variable(tf.random.truncated_normal([512, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))\n    # 256 => 10\n    w3, b3 = tf.Variable(tf.random.truncated_normal([256, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))\n\n\n\n    optimizer = optimizers.SGD(lr=0.01)\n\n\n    for step, (x,y) in enumerate(train_db):\n\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 784))\n\n        with tf.GradientTape() as tape:\n\n            # layer1.\n            h1 = x @ w1 + b1\n            h1 = tf.nn.relu(h1)\n            # layer2\n            h2 = h1 @ w2 + b2\n            h2 = tf.nn.relu(h2)\n            # output\n            out = h2 @ w3 + b3\n            # out = tf.nn.relu(out)\n\n            # compute loss\n            # [b, 10] - [b, 10]\n            loss = tf.square(y-out)\n            # [b, 10] => [b]\n            loss = tf.reduce_mean(loss, axis=1)\n            # [b] => scalar\n            loss = tf.reduce_mean(loss)\n\n\n\n        # compute gradient\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n        # print('==before==')\n        # for g in grads:\n        #     print(tf.norm(g))\n        \n        grads,  _ = tf.clip_by_global_norm(grads, 15)\n\n        # print('==after==')\n        # for g in grads:\n        #     print(tf.norm(g))\n        # update w' = w - lr*grad\n        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w3, b3]))\n\n\n\n        if step % 100 == 0:\n            print(step, 'loss:', float(loss))\n\n\n\n\nif __name__ == '__main__':\n    main()"""
TensorFlow-PPT/lesson19-OP/meshgrid.py,6,"b'import tensorflow as tf\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndef func(x):\r\n    """"""\r\n\r\n    :param x: [b, 2]\r\n    :return:\r\n    """"""\r\n    z = tf.math.sin(x[...,0]) + tf.math.sin(x[...,1])\r\n\r\n    return z\r\n\r\n\r\nx = tf.linspace(0., 2*3.14, 500)\r\ny = tf.linspace(0., 2*3.14, 500)\r\n# [50, 50]\r\npoint_x, point_y = tf.meshgrid(x, y)\r\n# [50, 50, 2]\r\npoints = tf.stack([point_x, point_y], axis=2)\r\n# points = tf.reshape(points, [-1, 2])\r\nprint(\'points:\', points.shape)\r\nz = func(points)\r\nprint(\'z:\', z.shape)\r\n\r\nplt.figure(\'plot 2d func value\')\r\nplt.imshow(z, origin=\'lower\', interpolation=\'none\')\r\nplt.colorbar()\r\n\r\nplt.figure(\'plot 2d func contour\')\r\nplt.contour(point_x, point_y, z)\r\nplt.colorbar()\r\nplt.show()'"
TensorFlow-PPT/lesson21--/forward.py,30,"b""import  tensorflow as tf\r\nfrom    tensorflow import keras\r\nfrom    tensorflow.keras import datasets\r\nimport  os\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\n# x: [60k, 28, 28], [10, 28, 28]\r\n# y: [60k], [10k]\r\n(x, y), (x_test, y_test) = datasets.mnist.load_data()\r\n# x: [0~255] => [0~1.]\r\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\r\ny = tf.convert_to_tensor(y, dtype=tf.int32)\r\nx_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\r\ny_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\r\n\r\nprint(x.shape, y.shape, x.dtype, y.dtype)\r\nprint(tf.reduce_min(x), tf.reduce_max(x))\r\nprint(tf.reduce_min(y), tf.reduce_max(y))\r\n\r\n\r\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\r\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(128)\r\ntrain_iter = iter(train_db)\r\nsample = next(train_iter)\r\nprint('batch:', sample[0].shape, sample[1].shape)\r\n\r\n\r\n# [b, 784] => [b, 256] => [b, 128] => [b, 10]\r\n# [dim_in, dim_out], [dim_out]\r\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\r\nb1 = tf.Variable(tf.zeros([256]))\r\nw2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\r\nb2 = tf.Variable(tf.zeros([128]))\r\nw3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\r\nb3 = tf.Variable(tf.zeros([10]))\r\n\r\nlr = 1e-3\r\n\r\nfor epoch in range(100): # iterate db for 10\r\n    for step, (x, y) in enumerate(train_db): # for every batch\r\n        # x:[128, 28, 28]\r\n        # y: [128]\r\n\r\n        # [b, 28, 28] => [b, 28*28]\r\n        x = tf.reshape(x, [-1, 28*28])\r\n\r\n        with tf.GradientTape() as tape: # tf.Variable\r\n            # x: [b, 28*28]\r\n            # h1 = x@w1 + b1\r\n            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\r\n            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\r\n            h1 = tf.nn.relu(h1)\r\n            # [b, 256] => [b, 128]\r\n            h2 = h1@w2 + b2\r\n            h2 = tf.nn.relu(h2)\r\n            # [b, 128] => [b, 10]\r\n            out = h2@w3 + b3\r\n\r\n            # compute loss\r\n            # out: [b, 10]\r\n            # y: [b] => [b, 10]\r\n            y_onehot = tf.one_hot(y, depth=10)\r\n\r\n            # mse = mean(sum(y-out)^2)\r\n            # [b, 10]\r\n            loss = tf.square(y_onehot - out)\r\n            # mean: scalar\r\n            loss = tf.reduce_mean(loss)\r\n\r\n        # compute gradients\r\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\r\n        # print(grads)\r\n        # w1 = w1 - lr * w1_grad\r\n        w1.assign_sub(lr * grads[0])\r\n        b1.assign_sub(lr * grads[1])\r\n        w2.assign_sub(lr * grads[2])\r\n        b2.assign_sub(lr * grads[3])\r\n        w3.assign_sub(lr * grads[4])\r\n        b3.assign_sub(lr * grads[5])\r\n\r\n\r\n        if step % 100 == 0:\r\n            print(epoch, step, 'loss:', float(loss))\r\n\r\n\r\n    # test/evluation\r\n    # [w1, b1, w2, b2, w3, b3]\r\n    total_correct, total_num = 0, 0\r\n    for step, (x,y) in enumerate(test_db):\r\n\r\n        # [b, 28, 28] => [b, 28*28]\r\n        x = tf.reshape(x, [-1, 28*28])\r\n\r\n        # [b, 784] => [b, 256] => [b, 128] => [b, 10]\r\n        h1 = tf.nn.relu(x@w1 + b1)\r\n        h2 = tf.nn.relu(h1@w2 + b2)\r\n        out = h2@w3 +b3\r\n\r\n        # out: [b, 10] ~ R\r\n        # prob: [b, 10] ~ [0, 1]\r\n        prob = tf.nn.softmax(out, axis=1)\r\n        # [b, 10] => [b]\r\n        # int64!!!\r\n        pred = tf.argmax(prob, axis=1)\r\n        pred = tf.cast(pred, dtype=tf.int32)\r\n        # y: [b]\r\n        # [b], int32\r\n        # print(pred.dtype, y.dtype)\r\n        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\r\n        correct = tf.reduce_sum(correct)\r\n\r\n        total_correct += int(correct)\r\n        total_num += x.shape[0]\r\n\r\n    acc = total_correct / total_num\r\n    print('test acc:', acc)\r\n"""
TensorFlow-PPT/lesson21--/mnist_tensor.py,25,"b'import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  os\n\n\n\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\nprint(tf.__version__)\n\n\ndef preprocess(x, y):\n    """"""\n\n    :param x:\n    :param y:\n    :return:\n    """"""\n    # [b, 28, 28], [b]\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [-1, 28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n\n    return x,y\n\n\n(x, y), (x_test, y_test) = datasets.mnist.load_data()\nprint(\'x:\', x.shape, \'y:\', y.shape, \'x test:\', x_test.shape, \'y test:\', y_test)\ntrain_db = tf.data.Dataset.from_tensor_slices((x, y))\ntrain_db = train_db.shuffle(60000).batch(128).map(preprocess).repeat(30)\n\ntest_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_db = test_db.shuffle(10000).batch(128).map(preprocess)\nx,y = next(iter(train_db))\nprint(\'train sample:\', x.shape, y.shape)\n# print(x[0], y[0])\n\n\n\n\n\n\n\n\ndef main():\n\n    # learning rate\n    lr = 1e-3\n\n\n    # 784 => 512\n    w1, b1 = tf.Variable(tf.random.truncated_normal([784, 512], stddev=0.1)), tf.Variable(tf.zeros([512]))\n    # 512 => 256\n    w2, b2 = tf.Variable(tf.random.truncated_normal([512, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))\n    # 256 => 10\n    w3, b3 = tf.Variable(tf.random.truncated_normal([256, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))\n\n\n\n\n\n    for step, (x,y) in enumerate(train_db):\n\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 784))\n\n        with tf.GradientTape() as tape:\n\n            # layer1.\n            h1 = x @ w1 + b1\n            h1 = tf.nn.relu(h1)\n            # layer2\n            h2 = h1 @ w2 + b2\n            h2 = tf.nn.relu(h2)\n            # output\n            out = h2 @ w3 + b3\n            # out = tf.nn.relu(out)\n\n            # compute loss\n            # [b, 10] - [b, 10]\n            loss = tf.square(y-out)\n            # [b, 10] => [b]\n            loss = tf.reduce_mean(loss, axis=1)\n            # [b] => scalar\n            loss = tf.reduce_mean(loss)\n\n\n\n        # compute gradient\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n        # for g in grads:\n        #     print(tf.norm(g))\n        # update w\' = w - lr*grad\n        for p, g in zip([w1, b1, w2, b2, w3, b3], grads):\n            p.assign_sub(lr * g)\n\n\n\n        # print\n        if step % 100 == 0:\n            print(step, \'loss:\', float(loss))\n\n        # evaluate\n        if step % 500 == 0:\n            total, total_correct = 0., 0\n\n            for step, (x, y) in enumerate(test_db):\n                # layer1.\n                h1 = x @ w1 + b1\n                h1 = tf.nn.relu(h1)\n                # layer2\n                h2 = h1 @ w2 + b2\n                h2 = tf.nn.relu(h2)\n                # output\n                out = h2 @ w3 + b3\n                # [b, 10] => [b]\n                pred = tf.argmax(out, axis=1)\n                # convert one_hot y to number y\n                y = tf.argmax(y, axis=1)\n                # bool type\n                correct = tf.equal(pred, y)\n                # bool tensor => int tensor => numpy\n                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n                total += x.shape[0]\n\n            print(step, \'Evaluate Acc:\', total_correct/total)\n\n\nif __name__ == \'__main__\':\n    main()'"
TensorFlow-PPT/lesson22-/mlp.py,1,"b""import tensorflow as tf \nfrom \ttensorflow import keras\n\n\n\n\n\nx = tf.random.normal([2, 3])\n\nmodel = keras.Sequential([\n\t\tkeras.layers.Dense(2, activation='relu'),\n\t\tkeras.layers.Dense(2, activation='relu'),\n\t\tkeras.layers.Dense(2)\n\t])\nmodel.build(input_shape=[None, 3])\nmodel.summary()\n\nfor p in model.trainable_variables:\n\tprint(p.name, p.shape)\n"""
TensorFlow-PPT/lesson24-/loss.py,7,"b'import tensorflow as tf \n\n\n\n\ny = tf.constant([1, 2, 3, 0, 2])\ny = tf.one_hot(y, depth=4)\ny = tf.cast(y, dtype=tf.float32)\n\nout = tf.random.normal([5, 4])\n\n\nloss1 = tf.reduce_mean(tf.square(y-out))\n\nloss2 = tf.square(tf.norm(y-out))/(5*4)\n\nloss3 = tf.reduce_mean(tf.losses.MSE(y, out)) # VS MeanSquaredError is a class\n\n\nprint(loss1)\nprint(loss2)\nprint(loss3)'"
TensorFlow-PPT/lesson25-/2nd_derivative.py,5,"b'import tensorflow as tf\r\n\r\nw = tf.Variable(1.0)\r\nb = tf.Variable(2.0)\r\nx = tf.Variable(3.0)\r\n\r\nwith tf.GradientTape() as t1:\r\n  with tf.GradientTape() as t2:\r\n    y = x * w + b\r\n  dy_dw, dy_db = t2.gradient(y, [w, b])\r\nd2y_dw2 = t1.gradient(dy_dw, w)\r\n\r\nprint(dy_dw)\r\nprint(dy_db)\r\nprint(d2y_dw2)\r\n\r\nassert dy_dw.numpy() == 3.0\r\nassert d2y_dw2 is None'"
TensorFlow-PPT/lesson25-/chain_rule.py,6,"b'import tensorflow as tf \n\nx = tf.constant(1.)\nw1 = tf.constant(2.)\nb1 = tf.constant(1.)\nw2 = tf.constant(2.)\nb2 = tf.constant(1.)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n\n\ttape.watch([w1, b1, w2, b2])\n\n\ty1 = x * w1 + b1\n\ty2 = y1 * w2 + b2\n\ndy2_dy1 = tape.gradient(y2, [y1])[0]\ndy1_dw1 = tape.gradient(y1, [w1])[0]\ndy2_dw1 = tape.gradient(y2, [w1])[0]\n\n\nprint(dy2_dy1 * dy1_dw1)\nprint(dy2_dw1)'"
TensorFlow-PPT/lesson25-/crossentropy_loss.py,7,"b""import tensorflow as tf \n\n\ntf.random.set_seed(4323)\n\nx=tf.random.normal([1,3])\n\nw=tf.random.normal([3,2])\n\nb=tf.random.normal([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = (x@w+b)\n\tloss = tf.reduce_mean(tf.losses.categorical_crossentropy(y, logits, from_logits=True))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])"""
TensorFlow-PPT/lesson25-/mse_grad.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,2])\n\nb=tf.ones([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
TensorFlow-PPT/lesson25-/multi_output_perceptron.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,2])\n\nb=tf.ones([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
TensorFlow-PPT/lesson25-/sigmoid_grad.py,3,"b""import tensorflow as tf \n\n\na = tf.linspace(-10., 10., 10)\n\nwith tf.GradientTape() as tape:\n\ttape.watch(a)\n\ty = tf.sigmoid(a)\n\n\ngrads = tape.gradient(y, [a])\nprint('x:', a.numpy())\nprint('y:', y.numpy())\nprint('grad:', grads[0].numpy())\n"""
TensorFlow-PPT/lesson25-/single_output_perceptron.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,1])\n\nb=tf.ones([1])\n\ny = tf.constant([1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
TensorFlow-PPT/lesson26-/himmelblau.py,2,"b""import  numpy as np\nfrom    mpl_toolkits.mplot3d import Axes3D\nfrom    matplotlib import pyplot as plt\nimport  tensorflow as tf\n\n\n\ndef himmelblau(x):\n    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n\n\nx = np.arange(-6, 6, 0.1)\ny = np.arange(-6, 6, 0.1)\nprint('x,y range:', x.shape, y.shape)\nX, Y = np.meshgrid(x, y)\nprint('X,Y maps:', X.shape, Y.shape)\nZ = himmelblau([X, Y])\n\nfig = plt.figure('himmelblau')\nax = fig.gca(projection='3d')\nax.plot_surface(X, Y, Z)\nax.view_init(60, -30)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n\n\n# [1., 0.], [-4, 0.], [4, 0.]\nx = tf.constant([4., 0.])\n\nfor step in range(200):\n\n    with tf.GradientTape() as tape:\n        tape.watch([x])\n        y = himmelblau(x)\n\n    grads = tape.gradient(y, [x])[0] \n    x -= 0.01*grads\n\n    \n\n    if step % 20 == 0:\n        print ('step {}: x = {}, f(x) = {}'\n               .format(step, x.numpy(), y.numpy()))"""
TensorFlow-PPT/lesson27--/fashionmnist_layer.py,21,"b""import  os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n\r\nimport tensorflow as tf\r\nfrom    tensorflow import keras\r\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\r\n\r\nassert tf.__version__.startswith('2.')\r\n\r\ndef preprocess(x, y):\r\n\r\n    x = tf.cast(x, dtype=tf.float32) / 255.\r\n    y = tf.cast(y, dtype=tf.int32)\r\n    return x,y\r\n\r\n\r\n(x, y), (x_test, y_test) = datasets.fashion_mnist.load_data()\r\nprint(x.shape, y.shape)\r\n\r\n\r\nbatchsz = 128\r\n\r\ndb = tf.data.Dataset.from_tensor_slices((x,y))\r\ndb = db.map(preprocess).shuffle(10000).batch(batchsz)\r\n\r\ndb_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\r\ndb_test = db_test.map(preprocess).batch(batchsz)\r\n\r\ndb_iter = iter(db)\r\nsample = next(db_iter)\r\nprint('batch:', sample[0].shape, sample[1].shape)\r\n\r\n\r\nmodel = Sequential([\r\n    layers.Dense(256, activation=tf.nn.relu), # [b, 784] => [b, 256]\r\n    layers.Dense(128, activation=tf.nn.relu), # [b, 256] => [b, 128]\r\n    layers.Dense(64, activation=tf.nn.relu), # [b, 128] => [b, 64]\r\n    layers.Dense(32, activation=tf.nn.relu), # [b, 64] => [b, 32]\r\n    layers.Dense(10) # [b, 32] => [b, 10], 330 = 32*10 + 10\r\n])\r\nmodel.build(input_shape=[None, 28*28])\r\nmodel.summary()\r\n# w = w - lr*grad\r\noptimizer = optimizers.Adam(lr=1e-3)\r\n\r\ndef main():\r\n\r\n\r\n    for epoch in range(30):\r\n\r\n\r\n        for step, (x,y) in enumerate(db):\r\n\r\n            # x: [b, 28, 28] => [b, 784]\r\n            # y: [b]\r\n            x = tf.reshape(x, [-1, 28*28])\r\n\r\n            with tf.GradientTape() as tape:\r\n                # [b, 784] => [b, 10]\r\n                logits = model(x)\r\n                y_onehot = tf.one_hot(y, depth=10)\r\n                # [b]\r\n                loss_mse = tf.reduce_mean(tf.losses.MSE(y_onehot, logits))\r\n                loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\r\n                loss_ce = tf.reduce_mean(loss_ce)\r\n\r\n            grads = tape.gradient(loss_ce, model.trainable_variables)\r\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n\r\n            if step % 100 == 0:\r\n                print(epoch, step, 'loss:', float(loss_ce), float(loss_mse))\r\n\r\n\r\n        # test\r\n        total_correct = 0\r\n        total_num = 0\r\n        for x,y in db_test:\r\n\r\n            # x: [b, 28, 28] => [b, 784]\r\n            # y: [b]\r\n            x = tf.reshape(x, [-1, 28*28])\r\n            # [b, 10]\r\n            logits = model(x)\r\n            # logits => prob, [b, 10]\r\n            prob = tf.nn.softmax(logits, axis=1)\r\n            # [b, 10] => [b], int64\r\n            pred = tf.argmax(prob, axis=1)\r\n            pred = tf.cast(pred, dtype=tf.int32)\r\n            # pred:[b]\r\n            # y: [b]\r\n            # correct: [b], True: equal, False: not equal\r\n            correct = tf.equal(pred, y)\r\n            correct = tf.reduce_sum(tf.cast(correct, dtype=tf.int32))\r\n\r\n            total_correct += int(correct)\r\n            total_num += x.shape[0]\r\n\r\n        acc = total_correct / total_num\r\n        print(epoch, 'test acc:', acc)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()"""
TensorFlow-PPT/lesson28-/main.py,25,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\nimport  datetime\nfrom    matplotlib import pyplot as plt\nimport  io\n\nassert tf.__version__.startswith(\'2.\')\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\ndef plot_to_image(figure):\n  """"""Converts the matplotlib plot specified by \'figure\' to a PNG image and\n  returns it. The supplied figure is closed and inaccessible after this call.""""""\n  # Save the plot to a PNG in memory.\n  buf = io.BytesIO()\n  plt.savefig(buf, format=\'png\')\n  # Closing the figure prevents it from being displayed directly inside\n  # the notebook.\n  plt.close(figure)\n  buf.seek(0)\n  # Convert PNG buffer to TF image\n  image = tf.image.decode_png(buf.getvalue(), channels=4)\n  # Add the batch dimension\n  image = tf.expand_dims(image, 0)\n  return image\n\ndef image_grid(images):\n  """"""Return a 5x5 grid of the MNIST images as a matplotlib figure.""""""\n  # Create a figure to contain the plot.\n  figure = plt.figure(figsize=(10,10))\n  for i in range(25):\n    # Start next subplot.\n    plt.subplot(5, 5, i + 1, title=\'name\')\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(images[i], cmap=plt.cm.binary)\n  \n  return figure\n\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz, drop_remainder=True) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\n\n\ncurrent_time = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")\nlog_dir = \'logs/\' + current_time\nsummary_writer = tf.summary.create_file_writer(log_dir) \n\n# get x from (x,y)\nsample_img = next(iter(db))[0]\n# get first image instance\nsample_img = sample_img[0]\nsample_img = tf.reshape(sample_img, [1, 28, 28, 1])\nwith summary_writer.as_default():\n    tf.summary.image(""Training sample:"", sample_img, step=0)\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, \'loss:\', float(loss))\n        with summary_writer.as_default(): \n            tf.summary.scalar(\'train-loss\', float(loss), step=step) \n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n\n        for _, (x, y) in enumerate(ds_val):  \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10]\n            out = network(x) \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, \'Evaluate Acc:\', total_correct/total)\n\n        \n        # print(x.shape) \n        val_images = x[:25]\n        val_images = tf.reshape(val_images, [-1, 28, 28, 1])\n        with summary_writer.as_default():\n            tf.summary.scalar(\'test-acc\', float(total_correct/total), step=step)\n            tf.summary.image(""val-onebyone-images:"", val_images, max_outputs=25, step=step)\n            \n            val_images = tf.reshape(val_images, [-1, 28, 28])\n            figure  = image_grid(val_images)\n            tf.summary.image(\'val-images:\', plot_to_image(figure), step=step)'"
TensorFlow-PPT/lesson30-KerasAPI/compile_fit.py,9,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
TensorFlow-PPT/lesson30-KerasAPI/layer_model.py,13,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\nfrom \ttensorflow import keras\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\nclass MyDense(layers.Layer):\n\n\tdef __init__(self, inp_dim, outp_dim):\n\t\tsuper(MyDense, self).__init__()\n\n\t\tself.kernel = self.add_weight(\'w\', [inp_dim, outp_dim])\n\t\tself.bias = self.add_weight(\'b\', [outp_dim])\n\n\tdef call(self, inputs, training=None):\n\n\t\tout = inputs @ self.kernel + self.bias\n\n\t\treturn out \n\nclass MyModel(keras.Model):\n\n\tdef __init__(self):\n\t\tsuper(MyModel, self).__init__()\n\n\t\tself.fc1 = MyDense(28*28, 256)\n\t\tself.fc2 = MyDense(256, 128)\n\t\tself.fc3 = MyDense(128, 64)\n\t\tself.fc4 = MyDense(64, 32)\n\t\tself.fc5 = MyDense(32, 10)\n\n\tdef call(self, inputs, training=None):\n\n\t\tx = self.fc1(inputs)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc2(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc3(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc4(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc5(x) \n\n\t\treturn x\n\n\nnetwork = MyModel()\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val,\n              validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
TensorFlow-PPT/lesson30-KerasAPI/metrics.py,13,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\nacc_meter = metrics.Accuracy()\nloss_meter = metrics.Mean()\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n        loss_meter.update_state(loss)\n\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', loss_meter.result().numpy()) \n        loss_meter.reset_states()\n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n        acc_meter.reset_states()\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10]\n            out = network(x) \n\n\n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n            acc_meter.update_state(y, pred)\n\n\n        print(step, 'Evaluate Acc:', total_correct/total, acc_meter.result().numpy())\n"""
TensorFlow-PPT/lesson31-Keras/save_load_model.py,14,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nnetwork.save(\'model.h5\')\nprint(\'saved total model.\')\ndel network\n\nprint(\'loaded model from file.\')\nnetwork = tf.keras.models.load_model(\'model.h5\', compile=False)\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=[\'accuracy\']\n    )\nx_val = tf.cast(x_val, dtype=tf.float32) / 255.\nx_val = tf.reshape(x_val, [-1, 28*28])\ny_val = tf.cast(y_val, dtype=tf.int32)\ny_val = tf.one_hot(y_val, depth=10)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(128)\nnetwork.evaluate(ds_val)\n'"
TensorFlow-PPT/lesson31-Keras/save_load_weight.py,8,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nnetwork.save_weights(\'weights.ckpt\')\nprint(\'saved weights.\')\ndel network\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\nnetwork.load_weights(\'weights.ckpt\')\nprint(\'loaded weights!\')\nnetwork.evaluate(ds_val)\n'"
TensorFlow-PPT/lesson32-Keras/keras_train.py,15,"b'import  os\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\r\n\r\nimport  tensorflow as tf\r\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\r\nfrom \ttensorflow import keras\r\n\r\n\r\n\r\ndef preprocess(x, y):\r\n    # [0~255] => [-1~1]\r\n    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1.\r\n    y = tf.cast(y, dtype=tf.int32)\r\n    return x,y\r\n\r\n\r\nbatchsz = 128\r\n# [50k, 32, 32, 3], [10k, 1]\r\n(x, y), (x_val, y_val) = datasets.cifar10.load_data()\r\ny = tf.squeeze(y)\r\ny_val = tf.squeeze(y_val)\r\ny = tf.one_hot(y, depth=10) # [50k, 10]\r\ny_val = tf.one_hot(y_val, depth=10) # [10k, 10]\r\nprint(\'datasets:\', x.shape, y.shape, x_val.shape, y_val.shape, x.min(), x.max())\r\n\r\n\r\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\r\ntrain_db = train_db.map(preprocess).shuffle(10000).batch(batchsz)\r\ntest_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))\r\ntest_db = test_db.map(preprocess).batch(batchsz)\r\n\r\n\r\nsample = next(iter(train_db))\r\nprint(\'batch:\', sample[0].shape, sample[1].shape)\r\n\r\n\r\nclass MyDense(layers.Layer):\r\n    # to replace standard layers.Dense()\r\n    def __init__(self, inp_dim, outp_dim):\r\n        super(MyDense, self).__init__()\r\n\r\n        self.kernel = self.add_variable(\'w\', [inp_dim, outp_dim])\r\n        # self.bias = self.add_variable(\'b\', [outp_dim])\r\n\r\n    def call(self, inputs, training=None):\r\n\r\n        x = inputs @ self.kernel\r\n        return x\r\n\r\nclass MyNetwork(keras.Model):\r\n\r\n    def __init__(self):\r\n        super(MyNetwork, self).__init__()\r\n\r\n        self.fc1 = MyDense(32*32*3, 256)\r\n        self.fc2 = MyDense(256, 128)\r\n        self.fc3 = MyDense(128, 64)\r\n        self.fc4 = MyDense(64, 32)\r\n        self.fc5 = MyDense(32, 10)\r\n\r\n\r\n\r\n    def call(self, inputs, training=None):\r\n        """"""\r\n\r\n        :param inputs: [b, 32, 32, 3]\r\n        :param training:\r\n        :return:\r\n        """"""\r\n        x = tf.reshape(inputs, [-1, 32*32*3])\r\n        # [b, 32*32*3] => [b, 256]\r\n        x = self.fc1(x)\r\n        x = tf.nn.relu(x)\r\n        # [b, 256] => [b, 128]\r\n        x = self.fc2(x)\r\n        x = tf.nn.relu(x)\r\n        # [b, 128] => [b, 64]\r\n        x = self.fc3(x)\r\n        x = tf.nn.relu(x)\r\n        # [b, 64] => [b, 32]\r\n        x = self.fc4(x)\r\n        x = tf.nn.relu(x)\r\n        # [b, 32] => [b, 10]\r\n        x = self.fc5(x)\r\n\r\n        return x\r\n\r\n\r\nnetwork = MyNetwork()\r\nnetwork.compile(optimizer=optimizers.Adam(lr=1e-3),\r\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\r\n                metrics=[\'accuracy\'])\r\nnetwork.fit(train_db, epochs=15, validation_data=test_db, validation_freq=1)\r\n\r\nnetwork.evaluate(test_db)\r\nnetwork.save_weights(\'ckpt/weights.ckpt\')\r\ndel network\r\nprint(\'saved to ckpt/weights.ckpt\')\r\n\r\n\r\nnetwork = MyNetwork()\r\nnetwork.compile(optimizer=optimizers.Adam(lr=1e-3),\r\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\r\n                metrics=[\'accuracy\'])\r\nnetwork.load_weights(\'ckpt/weights.ckpt\')\r\nprint(\'loaded weights from file.\')\r\nnetwork.evaluate(test_db)'"
TensorFlow-PPT/lesson34-/compile_fit.py,9,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val,\n              validation_steps=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
TensorFlow-PPT/lesson34-/train_evalute_test.py,14,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_test, y_test) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\nidx = tf.range(60000)\nidx = tf.random.shuffle(idx)\nx_train, y_train = tf.gather(x, idx[:50000]), tf.gather(y, idx[:50000])\nx_val, y_val = tf.gather(x, idx[-10000:]) , tf.gather(y, idx[-10000:])\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\ndb_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\ndb_train = db_train.map(preprocess).shuffle(50000).batch(batchsz)\n\ndb_val = tf.data.Dataset.from_tensor_slices((x_val,y_val))\ndb_val = db_val.map(preprocess).shuffle(10000).batch(batchsz)\n\n\n\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.map(preprocess).batch(batchsz) \n\nsample = next(iter(db_train))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db_train, epochs=6, validation_data=db_val, validation_freq=2)\n\nprint(\'Test performance:\') \nnetwork.evaluate(db_test)\n \n\nsample = next(iter(db_test))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
TensorFlow-PPT/lesson35-Regularization/regularization.py,15,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n\n        loss_regularization = []\n        for p in network.trainable_variables:\n            loss_regularization.append(tf.nn.l2_loss(p))\n        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n\n        loss = loss + 0.0001 * loss_regularization\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10]\n            out = network(x) \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc:', total_correct/total)"""
"TensorFlow-PPT/lesson37-Early Stopping, Dropout/dropout.py",20,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dropout(0.5), # 0.5 rate to drop\n                     layers.Dense(128, activation='relu'),\n                     layers.Dropout(0.5), # 0.5 rate to drop\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x, training=True)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n\n        loss_regularization = []\n        for p in network.trainable_variables:\n            loss_regularization.append(tf.nn.l2_loss(p))\n        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n\n        loss = loss + 0.0001 * loss_regularization\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10] \n            out = network(x, training=True)  \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc with drop:', total_correct/total)\n\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10] \n            out = network(x, training=False)  \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc without drop:', total_correct/total)"""
TensorFlow-PPT/lesson40-CIFARVGG/cifar100_train.py,31,"b'import  os\r\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\r\n\r\nimport  tensorflow as tf\r\nfrom    tensorflow.keras import layers, optimizers, datasets, Sequential\r\n\r\ntf.random.set_seed(2345)\r\n\r\nconv_layers = [ # 5 units of conv + max pooling\r\n    # unit 1\r\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\r\n\r\n    # unit 2\r\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\r\n\r\n    # unit 3\r\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\r\n\r\n    # unit 4\r\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\r\n\r\n    # unit 5\r\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\r\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\')\r\n\r\n]\r\n\r\n\r\n\r\ndef preprocess(x, y):\r\n    # [0~1]\r\n    x = tf.cast(x, dtype=tf.float32) / 255.\r\n    y = tf.cast(y, dtype=tf.int32)\r\n    return x,y\r\n\r\n\r\n(x,y), (x_test, y_test) = datasets.cifar100.load_data()\r\ny = tf.squeeze(y, axis=1)\r\ny_test = tf.squeeze(y_test, axis=1)\r\nprint(x.shape, y.shape, x_test.shape, y_test.shape)\r\n\r\n\r\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\r\ntrain_db = train_db.shuffle(1000).map(preprocess).batch(128)\r\n\r\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\r\ntest_db = test_db.map(preprocess).batch(64)\r\n\r\nsample = next(iter(train_db))\r\nprint(\'sample:\', sample[0].shape, sample[1].shape,\r\n      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\r\n\r\n\r\ndef main():\r\n\r\n    # [b, 32, 32, 3] => [b, 1, 1, 512]\r\n    conv_net = Sequential(conv_layers)\r\n\r\n    fc_net = Sequential([\r\n        layers.Dense(256, activation=tf.nn.relu),\r\n        layers.Dense(128, activation=tf.nn.relu),\r\n        layers.Dense(100, activation=None),\r\n    ])\r\n\r\n    conv_net.build(input_shape=[None, 32, 32, 3])\r\n    fc_net.build(input_shape=[None, 512])\r\n    optimizer = optimizers.Adam(lr=1e-4)\r\n\r\n    # [1, 2] + [3, 4] => [1, 2, 3, 4]\r\n    variables = conv_net.trainable_variables + fc_net.trainable_variables\r\n\r\n    for epoch in range(50):\r\n\r\n        for step, (x,y) in enumerate(train_db):\r\n\r\n            with tf.GradientTape() as tape:\r\n                # [b, 32, 32, 3] => [b, 1, 1, 512]\r\n                out = conv_net(x)\r\n                # flatten, => [b, 512]\r\n                out = tf.reshape(out, [-1, 512])\r\n                # [b, 512] => [b, 100]\r\n                logits = fc_net(out)\r\n                # [b] => [b, 100]\r\n                y_onehot = tf.one_hot(y, depth=100)\r\n                # compute loss\r\n                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\r\n                loss = tf.reduce_mean(loss)\r\n\r\n            grads = tape.gradient(loss, variables)\r\n            optimizer.apply_gradients(zip(grads, variables))\r\n\r\n            if step %100 == 0:\r\n                print(epoch, step, \'loss:\', float(loss))\r\n\r\n\r\n\r\n        total_num = 0\r\n        total_correct = 0\r\n        for x,y in test_db:\r\n\r\n            out = conv_net(x)\r\n            out = tf.reshape(out, [-1, 512])\r\n            logits = fc_net(out)\r\n            prob = tf.nn.softmax(logits, axis=1)\r\n            pred = tf.argmax(prob, axis=1)\r\n            pred = tf.cast(pred, dtype=tf.int32)\r\n\r\n            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\r\n            correct = tf.reduce_sum(correct)\r\n\r\n            total_num += x.shape[0]\r\n            total_correct += int(correct)\r\n\r\n        acc = total_correct / total_num\r\n        print(epoch, \'acc:\', acc)\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
TensorFlow-PPT/lesson42-BatchNorm/main.py,3,"b""import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers\n\n\n# 2 images with 4x4 size, 3 channels\n# we explicitly enforce the mean and stddev to N(1, 0.5)\nx = tf.random.normal([2,4,4,3], mean=1.,stddev=0.5)\n\nnet = layers.BatchNormalization(axis=-1, center=True, scale=True,\n                                trainable=True)\n\nout = net(x)\nprint('forward in test mode:', net.variables)\n\n\nout = net(x, training=True)\nprint('forward in train mode(1 step):', net.variables)\n\nfor i in range(100):\n    out = net(x, training=True)\nprint('forward in train mode(100 steps):', net.variables)\n\n\noptimizer = optimizers.SGD(lr=1e-2)\nfor i in range(10):\n    with tf.GradientTape() as tape:\n        out = net(x, training=True)\n        loss = tf.reduce_mean(tf.pow(out,2)) - 1\n\n    grads = tape.gradient(loss, net.trainable_variables)\n    optimizer.apply_gradients(zip(grads, net.trainable_variables))\nprint('backward(10 steps):', net.variables)\n\n\n\n\n"""
TensorFlow-PPT/lesson43-ResNet/resnet.py,1,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, Sequential\n\n\n\nclass BasicBlock(layers.Layer):\n\n    def __init__(self, filter_num, stride=1):\n        super(BasicBlock, self).__init__()\n\n        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')\n        self.bn1 = layers.BatchNormalization()\n        self.relu = layers.Activation('relu')\n\n        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if stride != 1:\n            self.downsample = Sequential()\n            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n        else:\n            self.downsample = lambda x:x\n\n\n\n    def call(self, inputs, training=None):\n\n        # [b, h, w, c]\n        out = self.conv1(inputs)\n        out = self.bn1(out,training=training)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out,training=training)\n\n        identity = self.downsample(inputs)\n\n        output = layers.add([out, identity])\n        output = tf.nn.relu(output)\n\n        return output\n\n\nclass ResNet(keras.Model):\n\n\n    def __init__(self, layer_dims, num_classes=100): # [2, 2, 2, 2]\n        super(ResNet, self).__init__()\n\n        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n                                layers.BatchNormalization(),\n                                layers.Activation('relu'),\n                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n                                ])\n\n        self.layer1 = self.build_resblock(64,  layer_dims[0])\n        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n\n        # output: [b, 512, h, w],\n        self.avgpool = layers.GlobalAveragePooling2D()\n        self.fc = layers.Dense(num_classes)\n\n\n\n\n\n    def call(self, inputs, training=None):\n\n        x = self.stem(inputs,training=training)\n\n        x = self.layer1(x,training=training)\n        x = self.layer2(x,training=training)\n        x = self.layer3(x,training=training)\n        x = self.layer4(x,training=training)\n\n        # [b, c]\n        x = self.avgpool(x)\n        # [b, 100]\n        x = self.fc(x)\n\n        return x\n\n\n\n    def build_resblock(self, filter_num, blocks, stride=1):\n\n        res_blocks = Sequential()\n        # may down sample\n        res_blocks.add(BasicBlock(filter_num, stride))\n\n        for _ in range(1, blocks):\n            res_blocks.add(BasicBlock(filter_num, stride=1))\n\n        return res_blocks\n\n\ndef resnet18():\n    return ResNet([2, 2, 2, 2])\n\n\ndef resnet34():\n    return ResNet([3, 4, 6, 3])\n"""
TensorFlow-PPT/lesson43-ResNet/resnet18_train.py,17,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import layers, optimizers, datasets, Sequential \nfrom    resnet import resnet18 \n\ntf.random.set_seed(2345)\n\n\n\n\n\ndef preprocess(x, y):\n    # [-1~1]\n    x = tf.cast(x, dtype=tf.float32) / 255. - 0.5\n    y = tf.cast(y, dtype=tf.int32)\n    return x,y\n\n\n(x,y), (x_test, y_test) = datasets.cifar100.load_data()\ny = tf.squeeze(y, axis=1)\ny_test = tf.squeeze(y_test, axis=1)\nprint(x.shape, y.shape, x_test.shape, y_test.shape)\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db = train_db.shuffle(1000).map(preprocess).batch(512)\n\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\ntest_db = test_db.map(preprocess).batch(512)\n\nsample = next(iter(train_db))\nprint('sample:', sample[0].shape, sample[1].shape,\n      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n\n\ndef main():\n\n    # [b, 32, 32, 3] => [b, 1, 1, 512]\n    model = resnet18()\n    model.build(input_shape=(None, 32, 32, 3))\n    model.summary()\n    optimizer = optimizers.Adam(lr=1e-3)\n\n    for epoch in range(500):\n\n        for step, (x,y) in enumerate(train_db):\n\n            with tf.GradientTape() as tape:\n                # [b, 32, 32, 3] => [b, 100]\n                logits = model(x,training=True)\n                # [b] => [b, 100]\n                y_onehot = tf.one_hot(y, depth=100)\n                # compute loss\n                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n                loss = tf.reduce_mean(loss)\n\n            grads = tape.gradient(loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            if step %50 == 0:\n                print(epoch, step, 'loss:', float(loss))\n\n\n\n        total_num = 0\n        total_correct = 0\n        for x,y in test_db:\n\n            logits = model(x,training=False)\n            prob = tf.nn.softmax(logits, axis=1)\n            pred = tf.argmax(prob, axis=1)\n            pred = tf.cast(pred, dtype=tf.int32)\n\n            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n            correct = tf.reduce_sum(correct)\n\n            total_num += x.shape[0]\n            total_correct += int(correct)\n\n        acc = total_correct / total_num\n        print(epoch, 'acc:', acc)\n\n\n\nif __name__ == '__main__':\n    main()\n"""
TensorFlow-PPT/lesson45-RNN/sentiment_analysis_cell.py,10,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n        # [b, 64]\n        self.state0 = [tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units])]\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        # RNN: cell1 ,cell2, cell3\n        # SimpleRNN\n        self.rnn_cell0 = layers.SimpleRNNCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.SimpleRNNCell(units, dropout=0.5)\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # [b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100]\n            # h1 = x*wxh+h0*whh\n            # out0: [b, 64]\n            out0, state0 = self.rnn_cell0(word, state0, training)\n            # out1: [b, 64]\n            out1, state1 = self.rnn_cell1(out0, state1, training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(out1)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    model = MyRNN(units)\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'],experimental_run_tf_function=False)\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson45-RNN/sentiment_analysis_layer.py,7,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        self.rnn = keras.Sequential([\n            layers.SimpleRNN(units, dropout=0.5, return_sequences=True, unroll=True),\n            layers.SimpleRNN(units, dropout=0.5, unroll=True)\n        ])\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # x: [b, 80, 100] => [b, 64]\n        x = self.rnn(x,training=training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(x)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    model = MyRNN(units)\n    # model.build(input_shape=(4,80))\n    # model.summary()\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson47-LSTM/gru_sentiment_analysis_cell.py,10,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n        # [b, 64]\n        self.state0 = [tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units])]\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        # RNN: cell1 ,cell2, cell3\n        # SimpleRNN\n        # self.rnn_cell0 = layers.SimpleRNNCell(units, dropout=0.5)\n        # self.rnn_cell1 = layers.SimpleRNNCell(units, dropout=0.5)\n        self.rnn_cell0 = layers.GRUCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.GRUCell(units, dropout=0.5)\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # [b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100]\n            # h1 = x*wxh+h0*whh\n            # out0: [b, 64]\n            out0, state0 = self.rnn_cell0(word, state0, training)\n            # out1: [b, 64]\n            out1, state1 = self.rnn_cell1(out0, state1, training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(out1)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    import time\n\n    t0 = time.time()\n\n    model = MyRNN(units)\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'], experimental_run_tf_function=False)\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n    t1 = time.time()\n    # LSTM: 64.3 seconds, 83.4%\n    # GRU:  96.7s, 83.4%\n    print(\'total time cost:\', t1-t0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson47-LSTM/gru_sentiment_analysis_layer.py,7,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        self.rnn = keras.Sequential([\n            # layers.SimpleRNN(units, dropout=0.5, return_sequences=True, unroll=True),\n            # layers.SimpleRNN(units, dropout=0.5, unroll=True)\n\n            # unroll: Boolean (default False). If True, the network will be unrolled,\n            # else a symbolic loop will be used.\n            # Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n            # Unrolling is only suitable for short sequences.\n            layers.GRU(units, dropout=0.5, return_sequences=True, unroll=True),\n            layers.GRU(units, dropout=0.5, unroll=True)\n        ])\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # x: [b, 80, 100] => [b, 64]\n        x = self.rnn(x,training=training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(x)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    import time\n\n\n    t0 = time.time()\n    model = MyRNN(units)\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n\n    t1 = time.time()\n    # Unroll=True\n    # LSTM: 69.3 secnods, 83%\n    # GRU: 100 seconds, 83.4%\n\n    # Unroll=False\n    # LSTM\xef\xbc\x9a23.71\xef\xbc\x8c 81.24\n    # GRU 23.05\xef\xbc\x8c 83.11\n    print(\'total time cost:\', t1-t0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson47-LSTM/lstm_sentiment_analysis_cell.py,10,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n        # [b, 64]\n        self.state0 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        # RNN: cell1 ,cell2, cell3\n        # SimpleRNN\n        # self.rnn_cell0 = layers.SimpleRNNCell(units, dropout=0.5)\n        # self.rnn_cell1 = layers.SimpleRNNCell(units, dropout=0.5)\n        self.rnn_cell0 = layers.LSTMCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.LSTMCell(units, dropout=0.5)\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # [b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100]\n            # h1 = x*wxh+h0*whh\n            # out0: [b, 64]\n            out0, state0 = self.rnn_cell0(word, state0, training)\n            # out1: [b, 64]\n            out1, state1 = self.rnn_cell1(out0, state1, training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(out1)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    import time\n\n    t0 = time.time()\n\n    model = MyRNN(units)\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'],experimental_run_tf_function=False)\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n    t1 = time.time()\n    # 64.3 seconds, 83.4%\n    print(\'total time cost:\', t1-t0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson47-LSTM/lstm_sentiment_analysis_layer.py,7,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128\n\n# the most frequest words\ntotal_words = 10000\nmax_review_len = 80\nembedding_len = 100\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\n# x_train:[b, 80]\n# x_test: [b, 80]\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n\n\nclass MyRNN(keras.Model):\n\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n\n\n        # transform text to embedding representation\n        # [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n\n        # [b, 80, 100] , h_dim: 64\n        self.rnn = keras.Sequential([\n            # layers.SimpleRNN(units, dropout=0.5, return_sequences=True, unroll=True),\n            # layers.SimpleRNN(units, dropout=0.5, unroll=True)\n\n            layers.LSTM(units, dropout=0.5, return_sequences=True, unroll=True),\n            layers.LSTM(units, dropout=0.5, unroll=True)\n        ])\n\n\n        # fc, [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = layers.Dense(1)\n\n    def call(self, inputs, training=None):\n        """"""\n        net(x) net(x, training=True) :train mode\n        net(x, training=False): test\n        :param inputs: [b, 80]\n        :param training:\n        :return:\n        """"""\n        # [b, 80]\n        x = inputs\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute\n        # x: [b, 80, 100] => [b, 64]\n        x = self.rnn(x,training=training)\n\n        # out: [b, 64] => [b, 1]\n        x = self.outlayer(x)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64\n    epochs = 4\n\n    import time\n\n    t0 = time.time()\n\n    model = MyRNN(units)\n    model.compile(optimizer = keras.optimizers.Adam(0.001),\n                  loss = tf.losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n\n    model.evaluate(db_test)\n\n\n    t1 = time.time()\n    # 69.3 secnods, 83%\n    print(\'total time cost:\', t1-t0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
TensorFlow-PPT/lesson49-VAE/autoencoder.py,17,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import Sequential, layers\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith('2.')\n\n\ndef save_images(imgs, name):\n    new_im = Image.new('L', (280, 280))\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = imgs[index]\n            im = Image.fromarray(im, mode='L')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(name)\n\n\nh_dim = 20\nbatchsz = 512\nlr = 1e-3\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n# we do not need label\ntrain_db = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_db = train_db.shuffle(batchsz * 5).batch(batchsz)\ntest_db = tf.data.Dataset.from_tensor_slices(x_test)\ntest_db = test_db.batch(batchsz)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n\nclass AE(keras.Model):\n\n    def __init__(self):\n        super(AE, self).__init__()\n\n        # Encoders\n        self.encoder = Sequential([\n            layers.Dense(256, activation=tf.nn.relu),\n            layers.Dense(128, activation=tf.nn.relu),\n            layers.Dense(h_dim)\n        ])\n\n        # Decoders\n        self.decoder = Sequential([\n            layers.Dense(128, activation=tf.nn.relu),\n            layers.Dense(256, activation=tf.nn.relu),\n            layers.Dense(784)\n        ])\n\n\n    def call(self, inputs, training=None):\n        # [b, 784] => [b, 10]\n        h = self.encoder(inputs)\n        # [b, 10] => [b, 784]\n        x_hat = self.decoder(h)\n\n        return x_hat\n\n\n\nmodel = AE()\nmodel.build(input_shape=(None, 784))\nmodel.summary()\n\noptimizer = tf.optimizers.Adam(lr=lr)\n\nfor epoch in range(100):\n\n    for step, x in enumerate(train_db):\n\n        #[b, 28, 28] => [b, 784]\n        x = tf.reshape(x, [-1, 784])\n\n        with tf.GradientTape() as tape:\n            x_rec_logits = model(x)\n\n            rec_loss = tf.losses.binary_crossentropy(x, x_rec_logits, from_logits=True)\n            rec_loss = tf.reduce_mean(rec_loss)\n\n        grads = tape.gradient(rec_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n        if step % 100 ==0:\n            print(epoch, step, float(rec_loss))\n\n\n        # evaluation\n        x = next(iter(test_db))\n        logits = model(tf.reshape(x, [-1, 784]))\n        x_hat = tf.sigmoid(logits)\n        # [b, 784] => [b, 28, 28]\n        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n\n        # [b, 28, 28] => [2b, 28, 28]\n        x_concat = tf.concat([x, x_hat], axis=0)\n        x_concat = x_hat\n        x_concat = x_concat.numpy() * 255.\n        x_concat = x_concat.astype(np.uint8)\n        save_images(x_concat, 'ae_images/rec_epoch_%d.png'%epoch)\n"""
TensorFlow-PPT/lesson49-VAE/vae.py,21,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import Sequential, layers\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22) \nassert tf.__version__.startswith('2.')\n\n\ndef save_images(imgs, name):\n    new_im = Image.new('L', (280, 280))\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = imgs[index]\n            im = Image.fromarray(im, mode='L')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(name)\n\n\nh_dim = 20\nbatchsz = 512\nlr = 1e-3\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n# we do not need label\ntrain_db = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_db = train_db.shuffle(batchsz * 5).batch(batchsz)\ntest_db = tf.data.Dataset.from_tensor_slices(x_test)\ntest_db = test_db.batch(batchsz)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nz_dim = 10\n\nclass VAE(keras.Model):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.fc1 = layers.Dense(128)\n        self.fc2 = layers.Dense(z_dim) # get mean prediction\n        self.fc3 = layers.Dense(z_dim)\n\n        # Decoder\n        self.fc4 = layers.Dense(128)\n        self.fc5 = layers.Dense(784)\n\n    def encoder(self, x):\n\n        h = tf.nn.relu(self.fc1(x))\n        # get mean\n        mu = self.fc2(h)\n        # get variance\n        log_var = self.fc3(h)\n\n        return mu, log_var\n\n    def decoder(self, z):\n\n        out = tf.nn.relu(self.fc4(z))\n        out = self.fc5(out)\n\n        return out\n\n    def reparameterize(self, mu, log_var):\n\n        eps = tf.random.normal(log_var.shape)\n\n        std = tf.exp(log_var)**0.5\n\n        z = mu + std * eps\n        return z\n\n    def call(self, inputs, training=None):\n\n        # [b, 784] => [b, z_dim], [b, z_dim]\n        mu, log_var = self.encoder(inputs)\n        # reparameterization trick\n        z = self.reparameterize(mu, log_var)\n\n        x_hat = self.decoder(z)\n\n        return x_hat, mu, log_var\n\n\nmodel = VAE()\nmodel.build(input_shape=(4, 784))\noptimizer = tf.optimizers.Adam(lr)\n\nfor epoch in range(1000):\n\n    for step, x in enumerate(train_db):\n\n        x = tf.reshape(x, [-1, 784])\n\n        with tf.GradientTape() as tape:\n            x_rec_logits, mu, log_var = model(x)\n\n            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n\n            # compute kl divergence (mu, var) ~ N (0, 1)\n            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n            kl_div = -0.5 * (log_var + 1 - mu**2 - tf.exp(log_var))\n            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n\n            loss = rec_loss + 1. * kl_div\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n        if step % 100 == 0:\n            print(epoch, step, 'kl div:', float(kl_div), 'rec loss:', float(rec_loss))\n\n\n    # evaluation\n    z = tf.random.normal((batchsz, z_dim))\n    logits = model.decoder(z)\n    x_hat = tf.sigmoid(logits)\n    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n    x_hat = x_hat.astype(np.uint8)\n    save_images(x_hat, 'vae_images/sampled_epoch%d.png'%epoch)\n\n    x = next(iter(test_db))\n    x = tf.reshape(x, [-1, 784])\n    x_hat_logits, _, _ = model(x)\n    x_hat = tf.sigmoid(x_hat_logits)\n    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n    x_hat = x_hat.astype(np.uint8)\n    save_images(x_hat, 'vae_images/rec_epoch%d.png'%epoch)\n\n"""
TensorFlow-PPT/lesson51-WGAN/dataset.py,6,"b'import multiprocessing\n\nimport tensorflow as tf\n\n\ndef make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n    @tf.function\n    def _map_fn(img):\n        img = tf.image.resize(img, [resize, resize])\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1\n        return img\n\n    dataset = disk_image_batch_dataset(img_paths,\n                                          batch_size,\n                                          drop_remainder=drop_remainder,\n                                          map_fn=_map_fn,\n                                          shuffle=shuffle,\n                                          repeat=repeat)\n    img_shape = (resize, resize, 3)\n    len_dataset = len(img_paths) // batch_size\n\n    return dataset, img_shape, len_dataset\n\n\ndef batch_dataset(dataset,\n                  batch_size,\n                  drop_remainder=True,\n                  n_prefetch_batch=1,\n                  filter_fn=None,\n                  map_fn=None,\n                  n_map_threads=None,\n                  filter_after_map=False,\n                  shuffle=True,\n                  shuffle_buffer_size=None,\n                  repeat=None):\n    # set defaults\n    if n_map_threads is None:\n        n_map_threads = multiprocessing.cpu_count()\n    if shuffle and shuffle_buffer_size is None:\n        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n\n    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n\n    if not filter_after_map:\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n    else:  # [*] this is slower\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n\n    return dataset\n\n\ndef memory_data_batch_dataset(memory_data,\n                              batch_size,\n                              drop_remainder=True,\n                              n_prefetch_batch=1,\n                              filter_fn=None,\n                              map_fn=None,\n                              n_map_threads=None,\n                              filter_after_map=False,\n                              shuffle=True,\n                              shuffle_buffer_size=None,\n                              repeat=None):\n    """"""Batch dataset of memory data.\n\n    Parameters\n    ----------\n    memory_data : nested structure of tensors/ndarrays/lists\n\n    """"""\n    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n    dataset = batch_dataset(dataset,\n                            batch_size,\n                            drop_remainder=drop_remainder,\n                            n_prefetch_batch=n_prefetch_batch,\n                            filter_fn=filter_fn,\n                            map_fn=map_fn,\n                            n_map_threads=n_map_threads,\n                            filter_after_map=filter_after_map,\n                            shuffle=shuffle,\n                            shuffle_buffer_size=shuffle_buffer_size,\n                            repeat=repeat)\n    return dataset\n\n\ndef disk_image_batch_dataset(img_paths,\n                             batch_size,\n                             labels=None,\n                             drop_remainder=True,\n                             n_prefetch_batch=1,\n                             filter_fn=None,\n                             map_fn=None,\n                             n_map_threads=None,\n                             filter_after_map=False,\n                             shuffle=True,\n                             shuffle_buffer_size=None,\n                             repeat=None):\n    """"""Batch dataset of disk image for PNG and JPEG.\n\n    Parameters\n    ----------\n        img_paths : 1d-tensor/ndarray/list of str\n        labels : nested structure of tensors/ndarrays/lists\n\n    """"""\n    if labels is None:\n        memory_data = img_paths\n    else:\n        memory_data = (img_paths, labels)\n\n    def parse_fn(path, *label):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_png(img, 3)  # fix channels to 3\n        return (img,) + label\n\n    if map_fn:  # fuse `map_fn` and `parse_fn`\n        def map_fn_(*args):\n            return map_fn(*parse_fn(*args))\n    else:\n        map_fn_ = parse_fn\n\n    dataset = memory_data_batch_dataset(memory_data,\n                                        batch_size,\n                                        drop_remainder=drop_remainder,\n                                        n_prefetch_batch=n_prefetch_batch,\n                                        filter_fn=filter_fn,\n                                        map_fn=map_fn_,\n                                        n_map_threads=n_map_threads,\n                                        filter_after_map=filter_after_map,\n                                        shuffle=shuffle,\n                                        shuffle_buffer_size=shuffle_buffer_size,\n                                        repeat=repeat)\n\n    return dataset\n'"
TensorFlow-PPT/lesson51-WGAN/gan.py,10,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\n\n\n\nclass Generator(keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n        self.fc = layers.Dense(3*3*512)\n\n        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n        self.bn1 = layers.BatchNormalization()\n\n        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n\n    def call(self, inputs, training=None):\n        # [z, 100] => [z, 3*3*512]\n        x = self.fc(inputs)\n        x = tf.reshape(x, [-1, 3, 3, 512])\n        x = tf.nn.leaky_relu(x)\n\n        #\n        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = self.conv3(x)\n        x = tf.tanh(x)\n\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # [b, 64, 64, 3] => [b, 1]\n        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n\n        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n        self.bn3 = layers.BatchNormalization()\n\n        # [b, h, w ,c] => [b, -1]\n        self.flatten = layers.Flatten()\n        self.fc = layers.Dense(1)\n\n\n    def call(self, inputs, training=None):\n\n        x = tf.nn.leaky_relu(self.conv1(inputs))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n\n        # [b, h, w, c] => [b, -1]\n        x = self.flatten(x)\n        # [b, -1] => [b, 1]\n        logits = self.fc(x)\n\n        return logits\n\ndef main():\n\n    d = Discriminator()\n    g = Generator()\n\n\n    x = tf.random.normal([2, 64, 64, 3])\n    z = tf.random.normal([2, 100])\n\n    prob = d(x)\n    print(prob)\n    x_hat = g(z)\n    print(x_hat.shape)\n\n\n\n\nif __name__ == '__main__':\n    main()"""
TensorFlow-PPT/lesson51-WGAN/gan_train.py,17,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras \nfrom\tPIL import Image\nimport  glob\nfrom    gan import Generator, Discriminator\n\nfrom    dataset import make_anime_dataset\n\n\ndef save_result(val_out, val_block_size, image_path, color_mode):\n    def preprocess(img):\n        img = ((img + 1.0) * 127.5).astype(np.uint8)\n        # img = img.astype(np.uint8)\n        return img\n\n    preprocesed = preprocess(val_out)\n    final_image = np.array([])\n    single_row = np.array([])\n    for b in range(val_out.shape[0]):\n        # concat image into a row\n        if single_row.size == 0:\n            single_row = preprocesed[b, :, :, :]\n        else:\n            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n\n        # concat image row to final_image\n        if (b+1) % val_block_size == 0:\n            if final_image.size == 0:\n                final_image = single_row\n            else:\n                final_image = np.concatenate((final_image, single_row), axis=0)\n\n            # reset single row\n            single_row = np.array([])\n\n    if final_image.shape[2] == 1:\n        final_image = np.squeeze(final_image, axis=2) \n    Image.fromarray(final_image).save(image_path)\n\n\ndef celoss_ones(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n                                                   labels=tf.ones_like(logits))\n    return tf.reduce_mean(loss)\n\n\ndef celoss_zeros(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n                                                   labels=tf.zeros_like(logits))\n    return tf.reduce_mean(loss)\n\ndef d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n    # 1. treat real image as real\n    # 2. treat generated image as fake\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    d_real_logits = discriminator(batch_x, is_training)\n\n    d_loss_real = celoss_ones(d_real_logits)\n    d_loss_fake = celoss_zeros(d_fake_logits)\n\n    loss = d_loss_fake + d_loss_real\n\n    return loss\n\n\ndef g_loss_fn(generator, discriminator, batch_z, is_training):\n\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    loss = celoss_ones(d_fake_logits)\n\n    return loss\n\ndef main():\n\n    tf.random.set_seed(22)\n    np.random.seed(22)\n    assert tf.__version__.startswith('2.')\n\n\n    # hyper parameters\n    z_dim = 100\n    epochs = 3000000\n    batch_size = 512\n    learning_rate = 0.002\n    is_training = True\n\n\n    img_path = glob.glob(r'C:\\Users\\Jackie\\Downloads\\faces\\*.jpg')\n    assert len(img_path) > 0\n\n    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n    print(dataset, img_shape)\n    sample = next(iter(dataset))\n    print(sample.shape, tf.reduce_max(sample).numpy(),\n          tf.reduce_min(sample).numpy())\n    dataset = dataset.repeat()\n    db_iter = iter(dataset)\n\n\n    generator = Generator()\n    generator.build(input_shape = (None, z_dim))\n    discriminator = Discriminator()\n    discriminator.build(input_shape=(None, 64, 64, 3))\n\n    g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n    d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n\n\n    for epoch in range(epochs):\n\n        batch_z = tf.random.normal([batch_size, z_dim])\n        batch_x = next(db_iter)\n        # train D\n        with tf.GradientTape() as tape:\n            d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n        grads = tape.gradient(d_loss, discriminator.trainable_variables)\n        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n\n\n        batch_z = tf.random.normal([batch_size, z_dim])\n        with tf.GradientTape() as tape:\n            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n        grads = tape.gradient(g_loss, generator.trainable_variables)\n        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n\n        if epoch % 100 == 0:\n            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss))\n\n            z = tf.random.normal([100, z_dim])\n            fake_image = generator(z, training=False)\n            img_path = os.path.join('images', 'gan-%d.png'%epoch)\n            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n\n\n\nif __name__ == '__main__':\n    main()"""
TensorFlow-PPT/lesson51-WGAN/wgan.py,10,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\n\n\n\nclass Generator(keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n        self.fc = layers.Dense(3*3*512)\n\n        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n        self.bn1 = layers.BatchNormalization()\n\n        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n\n    def call(self, inputs, training=None):\n        # [z, 100] => [z, 3*3*512]\n        x = self.fc(inputs)\n        x = tf.reshape(x, [-1, 3, 3, 512])\n        x = tf.nn.leaky_relu(x)\n\n        #\n        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = self.conv3(x)\n        x = tf.tanh(x)\n\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # [b, 64, 64, 3] => [b, 1]\n        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n\n        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n        self.bn3 = layers.BatchNormalization()\n\n        # [b, h, w ,c] => [b, -1]\n        self.flatten = layers.Flatten()\n        self.fc = layers.Dense(1)\n\n\n    def call(self, inputs, training=None):\n\n        x = tf.nn.leaky_relu(self.conv1(inputs))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n\n        # [b, h, w, c] => [b, -1]\n        x = self.flatten(x)\n        # [b, -1] => [b, 1]\n        logits = self.fc(x)\n\n        return logits\n\ndef main():\n\n    d = Discriminator()\n    g = Generator()\n\n\n    x = tf.random.normal([2, 64, 64, 3])\n    z = tf.random.normal([2, 100])\n\n    prob = d(x)\n    print(prob)\n    x_hat = g(z)\n    print(x_hat.shape)\n\n\n\n\nif __name__ == '__main__':\n    main()"""
TensorFlow-PPT/lesson51-WGAN/wgan_train.py,24,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\n\nfrom    PIL import Image\nimport  glob\nfrom    gan import Generator, Discriminator\n\nfrom    dataset import make_anime_dataset\n\n\ndef save_result(val_out, val_block_size, image_path, color_mode):\n    def preprocess(img):\n        img = ((img + 1.0) * 127.5).astype(np.uint8)\n        # img = img.astype(np.uint8)\n        return img\n\n    preprocesed = preprocess(val_out)\n    final_image = np.array([])\n    single_row = np.array([])\n    for b in range(val_out.shape[0]):\n        # concat image into a row\n        if single_row.size == 0:\n            single_row = preprocesed[b, :, :, :]\n        else:\n            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n\n        # concat image row to final_image\n        if (b+1) % val_block_size == 0:\n            if final_image.size == 0:\n                final_image = single_row\n            else:\n                final_image = np.concatenate((final_image, single_row), axis=0)\n\n            # reset single row\n            single_row = np.array([])\n\n    if final_image.shape[2] == 1:\n        final_image = np.squeeze(final_image, axis=2) \n    Image.fromarray(final_image).save(image_path)\n\n\ndef celoss_ones(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    # loss = tf.keras.losses.categorical_crossentropy(y_pred=logits,\n    #                                                y_true=tf.ones_like(logits))\n    return - tf.reduce_mean(logits)\n\n\ndef celoss_zeros(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    # loss = tf.keras.losses.categorical_crossentropy(y_pred=logits,\n    #                                                y_true=tf.zeros_like(logits))\n    return tf.reduce_mean(logits)\n\n\ndef gradient_penalty(discriminator, batch_x, fake_image):\n\n    batchsz = batch_x.shape[0]\n\n    # [b, h, w, c]\n    t = tf.random.uniform([batchsz, 1, 1, 1])\n    # [b, 1, 1, 1] => [b, h, w, c]\n    t = tf.broadcast_to(t, batch_x.shape)\n\n    interplate = t * batch_x + (1 - t) * fake_image\n\n    with tf.GradientTape() as tape:\n        tape.watch([interplate])\n        d_interplote_logits = discriminator(interplate, training=True)\n    grads = tape.gradient(d_interplote_logits, interplate)\n\n    # grads:[b, h, w, c] => [b, -1]\n    grads = tf.reshape(grads, [grads.shape[0], -1])\n    gp = tf.norm(grads, axis=1) #[b]\n    gp = tf.reduce_mean( (gp-1)**2 )\n\n    return gp\n\n\n\ndef d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n    # 1. treat real image as real\n    # 2. treat generated image as fake\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    d_real_logits = discriminator(batch_x, is_training)\n\n    d_loss_real = celoss_ones(d_real_logits)\n    d_loss_fake = celoss_zeros(d_fake_logits)\n    gp = gradient_penalty(discriminator, batch_x, fake_image)\n\n    loss = d_loss_real + d_loss_fake + 10. * gp\n\n    return loss, gp\n\n\ndef g_loss_fn(generator, discriminator, batch_z, is_training):\n\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    loss = celoss_ones(d_fake_logits)\n\n    return loss\n\n\ndef main():\n\n    tf.random.set_seed(233)\n    np.random.seed(233)\n    assert tf.__version__.startswith('2.')\n\n\n    # hyper parameters\n    z_dim = 100\n    epochs = 3000000\n    batch_size = 512\n    learning_rate = 0.0005\n    is_training = True\n\n\n    img_path = glob.glob(r'C:\\Users\\Jackie\\Downloads\\faces\\*.jpg')\n    assert len(img_path) > 0\n    \n\n    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n    print(dataset, img_shape)\n    sample = next(iter(dataset))\n    print(sample.shape, tf.reduce_max(sample).numpy(),\n          tf.reduce_min(sample).numpy())\n    dataset = dataset.repeat()\n    db_iter = iter(dataset)\n\n\n    generator = Generator() \n    generator.build(input_shape = (None, z_dim))\n    discriminator = Discriminator()\n    discriminator.build(input_shape=(None, 64, 64, 3))\n    z_sample = tf.random.normal([100, z_dim])\n\n\n    g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n    d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n\n\n    for epoch in range(epochs):\n\n        for _ in range(5):\n            batch_z = tf.random.normal([batch_size, z_dim])\n            batch_x = next(db_iter)\n\n            # train D\n            with tf.GradientTape() as tape:\n                d_loss, gp = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n            grads = tape.gradient(d_loss, discriminator.trainable_variables)\n            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n        \n        batch_z = tf.random.normal([batch_size, z_dim])\n\n        with tf.GradientTape() as tape:\n            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n        grads = tape.gradient(g_loss, generator.trainable_variables)\n        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n\n        if epoch % 100 == 0:\n            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss),\n                  'gp:', float(gp))\n\n            z = tf.random.normal([100, z_dim])\n            fake_image = generator(z, training=False)\n            img_path = os.path.join('images', 'wgan-%d.png'%epoch)\n            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n\n\n\nif __name__ == '__main__':\n    main()"""
TensorFlow-PPT/lesson52-/pokemon.py,13,"b'import  os, glob\nimport  random, csv\n\nimport tensorflow as tf\n\n\n\ndef load_csv(root, filename, name2label):\n    # root:\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\n    # filename:csv\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\n    # name2label:\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\n    if not os.path.exists(os.path.join(root, filename)):\n        images = []\n        for name in name2label.keys():\n            # \'pokemon\\\\mewtwo\\\\00001.png\n            images += glob.glob(os.path.join(root, name, \'*.png\'))\n            images += glob.glob(os.path.join(root, name, \'*.jpg\'))\n            images += glob.glob(os.path.join(root, name, \'*.jpeg\'))\n\n        # 1167, \'pokemon\\\\bulbasaur\\\\00000000.png\'\n        print(len(images), images)\n\n        random.shuffle(images)\n        with open(os.path.join(root, filename), mode=\'w\', newline=\'\') as f:\n            writer = csv.writer(f)\n            for img in images:  # \'pokemon\\\\bulbasaur\\\\00000000.png\'\n                name = img.split(os.sep)[-2]\n                label = name2label[name]\n                # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n                writer.writerow([img, label])\n            print(\'written into csv file:\', filename)\n\n    # read from csv file\n    images, labels = [], []\n    with open(os.path.join(root, filename)) as f:\n        reader = csv.reader(f)\n        for row in reader:\n            # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n            img, label = row\n            label = int(label)\n\n            images.append(img)\n            labels.append(label)\n\n    assert len(images) == len(labels)\n\n    return images, labels\n\n\ndef load_pokemon(root, mode=\'train\'):\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\n    name2label = {}  # ""sq..."":0\n    for name in sorted(os.listdir(os.path.join(root))):\n        if not os.path.isdir(os.path.join(root, name)):\n            continue\n        # \xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\xbc\x96\xe7\xa0\x81\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\n        name2label[name] = len(name2label.keys())\n\n    # \xe8\xaf\xbb\xe5\x8f\x96Label\xe4\xbf\xa1\xe6\x81\xaf\n    # [file1,file2,], [3,1]\n    images, labels = load_csv(root, \'images.csv\', name2label)\n\n    if mode == \'train\':  # 60%\n        images = images[:int(0.6 * len(images))]\n        labels = labels[:int(0.6 * len(labels))]\n    elif mode == \'val\':  # 20% = 60%->80%\n        images = images[int(0.6 * len(images)):int(0.8 * len(images))]\n        labels = labels[int(0.6 * len(labels)):int(0.8 * len(labels))]\n    else:  # 20% = 80%->100%\n        images = images[int(0.8 * len(images)):]\n        labels = labels[int(0.8 * len(labels)):]\n\n    return images, labels, name2label\n\n\nimg_mean = tf.constant([0.485, 0.456, 0.406])\nimg_std = tf.constant([0.229, 0.224, 0.225])\ndef normalize(x, mean=img_mean, std=img_std):\n    # x: [224, 224, 3]\n    # mean: [224, 224, 3], std: [3]\n    x = (x - mean)/std\n    return x\n\ndef denormalize(x, mean=img_mean, std=img_std):\n    x = x * std + mean\n    return x\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\n    x = tf.io.read_file(x)\n    x = tf.image.decode_jpeg(x, channels=3) # RGBA\n    x = tf.image.resize(x, [244, 244])\n\n    # data augmentation, 0~255\n    # x = tf.image.random_flip_up_down(x)\n    x= tf.image.random_flip_left_right(x)\n    x = tf.image.random_crop(x, [224, 224, 3])\n\n    # x: [0,255]=> 0~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    # 0~1 => D(0,1)\n    x = normalize(x)\n\n    y = tf.convert_to_tensor(y)\n\n    return x, y\n\n\ndef main():\n    import  time\n\n\n\n\n    images, labels, table = load_pokemon(\'pokemon\', \'train\')\n    print(\'images\', len(images), images)\n    print(\'labels\', len(labels), labels)\n    print(table)\n\n    # images: string path\n    # labels: number\n    db = tf.data.Dataset.from_tensor_slices((images, labels))\n    db = db.shuffle(1000).map(preprocess).batch(32)\n\n    writter = tf.summary.create_file_writer(\'logs\')\n\n    for step, (x,y) in enumerate(db):\n\n        # x: [32, 224, 224, 3]\n        # y: [32]\n        with writter.as_default():\n            x = denormalize(x)\n            tf.summary.image(\'img\',x,step=step,max_outputs=9)\n            time.sleep(5)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
TensorFlow-PPT/lesson52-/resnet.py,9,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\n\nclass ResnetBlock(keras.Model):\n\n    def __init__(self, channels, strides=1):\n        super(ResnetBlock, self).__init__()\n\n        self.channels = channels\n        self.strides = strides\n\n        self.conv1 = layers.Conv2D(channels, 3, strides=strides,\n                                   padding=[[0,0],[1,1],[1,1],[0,0]])\n        self.bn1 = keras.layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(channels, 3, strides=1,\n                                   padding=[[0,0],[1,1],[1,1],[0,0]])\n        self.bn2 = keras.layers.BatchNormalization()\n\n        if strides!=1:\n            self.down_conv = layers.Conv2D(channels, 1, strides=strides, padding='valid')\n            self.down_bn = tf.keras.layers.BatchNormalization()\n\n    def call(self, inputs, training=None):\n        residual = inputs\n\n        x = self.conv1(inputs)\n        x = tf.nn.relu(x)\n        x = self.bn1(x, training=training)\n        x = self.conv2(x)\n        x = tf.nn.relu(x)\n        x = self.bn2(x, training=training)\n\n        # \xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5\n        if self.strides!=1:\n            residual = self.down_conv(inputs)\n            residual = tf.nn.relu(residual)\n            residual = self.down_bn(residual, training=training)\n\n        x = x + residual\n        x = tf.nn.relu(x)\n        return x\n\n\nclass ResNet(keras.Model):\n\n    def __init__(self, num_classes, initial_filters=16, **kwargs):\n        super(ResNet, self).__init__(**kwargs)\n\n        self.stem = layers.Conv2D(initial_filters, 3, strides=3, padding='valid')\n\n        self.blocks = keras.models.Sequential([\n            ResnetBlock(initial_filters * 2, strides=3),\n            ResnetBlock(initial_filters * 2, strides=1),\n            # layers.Dropout(rate=0.5),\n\n            ResnetBlock(initial_filters * 4, strides=3),\n            ResnetBlock(initial_filters * 4, strides=1),\n\n            ResnetBlock(initial_filters * 8, strides=2),\n            ResnetBlock(initial_filters * 8, strides=1),\n\n            ResnetBlock(initial_filters * 16, strides=2),\n            ResnetBlock(initial_filters * 16, strides=1),\n        ])\n\n        self.final_bn = layers.BatchNormalization()\n        self.avg_pool = layers.GlobalMaxPool2D()\n        self.fc = layers.Dense(num_classes)\n\n    def call(self, inputs, training=None):\n        # print('x:',inputs.shape)\n        out = self.stem(inputs\xef\xbc\x8ctraining=training)\n        out = tf.nn.relu(out)\n\n        # print('stem:',out.shape)\n\n        out = self.blocks(out, training=training)\n        # print('res:',out.shape)\n\n        out = self.final_bn(out, training=training)\n        # out = tf.nn.relu(out)\n\n        out = self.avg_pool(out)\n\n        # print('avg_pool:',out.shape)\n        out = self.fc(out)\n\n        # print('out:',out.shape)\n\n        return out\n\n\n\ndef main():\n    num_classes = 5\n\n    resnet18 = ResNet(5)\n    resnet18.build(input_shape=(4,224,224,3))\n    resnet18.summary()\n\n\n\n\n\n\nif __name__ == '__main__':\n    main()\n"""
TensorFlow-PPT/lesson52-/train_scratch.py,19,"b'import  os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    tensorflow.keras.callbacks import EarlyStopping\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nassert tf.__version__.startswith(\'2.\')\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe6\x98\xbe\xe5\xad\x98\xe6\x8c\x89\xe9\x9c\x80\xe5\x88\x86\xe9\x85\x8d\ngpus = tf.config.experimental.list_physical_devices(\'GPU\') \nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices(\'GPU\')\n    print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n    \n\nfrom    pokemon import load_pokemon, normalize, denormalize\nfrom    resnet import ResNet\n\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\n    x = tf.io.read_file(x)\n    x = tf.image.decode_jpeg(x, channels=3) # RGBA\n    # \xe5\x9b\xbe\xe7\x89\x87\xe7\xbc\xa9\xe6\x94\xbe\n    # x = tf.image.resize(x, [244, 244])\n    # \xe5\x9b\xbe\xe7\x89\x87\xe6\x97\x8b\xe8\xbd\xac\n    # x = tf.image.rot90(x,2)\n    # \xe9\x9a\x8f\xe6\x9c\xba\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\n    x = tf.image.random_flip_left_right(x)\n    # \xe9\x9a\x8f\xe6\x9c\xba\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\n    # x = tf.image.random_flip_up_down(x)\n    \n    # \xe5\x9b\xbe\xe7\x89\x87\xe5\x85\x88\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe7\xa8\x8d\xe5\xa4\xa7\xe5\xb0\xba\xe5\xaf\xb8\n    x = tf.image.resize(x, [244, 244])\n    # \xe5\x86\x8d\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\xe5\x88\xb0\xe5\x90\x88\xe9\x80\x82\xe5\xb0\xba\xe5\xaf\xb8\n    x = tf.image.random_crop(x, [224,224,3])\n\n    # x: [0,255]=> -1~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = normalize(x)\n    y = tf.convert_to_tensor(y)\n    y = tf.one_hot(y, depth=5)\n\n    return x, y\n\n\nbatchsz = 256\n\n# creat train db\nimages, labels, table = load_pokemon(\'pokemon\',mode=\'train\')\ndb_train = tf.data.Dataset.from_tensor_slices((images, labels))\ndb_train = db_train.shuffle(1000).map(preprocess).batch(batchsz)\n# crate validation db\nimages2, labels2, table = load_pokemon(\'pokemon\',mode=\'val\')\ndb_val = tf.data.Dataset.from_tensor_slices((images2, labels2))\ndb_val = db_val.map(preprocess).batch(batchsz)\n# create test db\nimages3, labels3, table = load_pokemon(\'pokemon\',mode=\'test\')\ndb_test = tf.data.Dataset.from_tensor_slices((images3, labels3))\ndb_test = db_test.map(preprocess).batch(batchsz)\n\n\nresnet = keras.Sequential([\n    layers.Conv2D(16,5,3),\n    layers.MaxPool2D(3,3),\n    layers.ReLU(),\n    layers.Conv2D(64,5,3),\n    layers.MaxPool2D(2,2),\n    layers.ReLU(),\n    layers.Flatten(),\n    layers.Dense(64),\n    layers.ReLU(),\n    layers.Dense(5)\n])\n\n\nresnet = ResNet(5)\nresnet.build(input_shape=(4, 224, 224, 3))\nresnet.summary()\n\nearly_stopping = EarlyStopping(\n    monitor=\'val_accuracy\',\n    min_delta=0.001,\n    patience=5\n)\n\nresnet.compile(optimizer=optimizers.Adam(lr=1e-3),\n               loss=losses.CategoricalCrossentropy(from_logits=True),\n               metrics=[\'accuracy\'])\nresnet.fit(db_train, validation_data=db_val, validation_freq=1, epochs=100,\n           callbacks=[early_stopping])\nresnet.evaluate(db_test)'"
TensorFlow-PPT/lesson52-/train_transfer.py,14,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    tensorflow.keras.callbacks import EarlyStopping\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\nfrom pokemon import  load_pokemon,normalize\n\n\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\n    x = tf.io.read_file(x)\n    x = tf.image.decode_jpeg(x, channels=3) # RGBA\n    x = tf.image.resize(x, [244, 244])\n\n    # x = tf.image.random_flip_left_right(x)\n    x = tf.image.random_flip_up_down(x)\n    x = tf.image.random_crop(x, [224,224,3])\n\n    # x: [0,255]=> -1~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = normalize(x)\n    y = tf.convert_to_tensor(y)\n    y = tf.one_hot(y, depth=5)\n\n    return x, y\n\n\nbatchsz = 128\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages, labels, table = load_pokemon('pokemon',mode='train')\ndb_train = tf.data.Dataset.from_tensor_slices((images, labels))\ndb_train = db_train.shuffle(1000).map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages2, labels2, table = load_pokemon('pokemon',mode='val')\ndb_val = tf.data.Dataset.from_tensor_slices((images2, labels2))\ndb_val = db_val.map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages3, labels3, table = load_pokemon('pokemon',mode='test')\ndb_test = tf.data.Dataset.from_tensor_slices((images3, labels3))\ndb_test = db_test.map(preprocess).batch(batchsz)\n\n# \nnet = keras.applications.VGG19(weights='imagenet', include_top=False,\n                               pooling='max')\nnet.trainable = False\nnewnet = keras.Sequential([\n    net,\n    layers.Dense(5)\n])\nnewnet.build(input_shape=(4,224,224,3))\nnewnet.summary()\n\n\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.001,\n    patience=5\n)\n\nnewnet.compile(optimizer=optimizers.Adam(lr=1e-3),\n               loss=losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\nnewnet.fit(db_train, validation_data=db_val, validation_freq=1, epochs=100,\n           callbacks=[early_stopping])\nnewnet.evaluate(db_test)\n"""
16-fasterRCNN/detection/datasets/coco.py,0,"b'import os.path as osp\nimport cv2\nimport numpy as np\nfrom pycocotools.coco import COCO\n\nfrom detection.datasets import transforms, utils\n\nclass CocoDataSet(object):\n    def __init__(self, dataset_dir, subset,\n                 flip_ratio=0,\n                 pad_mode=\'fixed\',\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 scale=(1024, 800),\n                 debug=False):\n        \'\'\'Load a subset of the COCO dataset.\n        \n        Attributes\n        ---\n            dataset_dir: The root directory of the COCO dataset.\n            subset: What to load (train, val).\n            flip_ratio: Float. The ratio of flipping an image and its bounding boxes.\n            pad_mode: Which padded method to use (fixed, non-fixed)\n            mean: Tuple. Image mean.\n            std: Tuple. Image standard deviation.\n            scale: Tuple of two integers.\n        \'\'\'\n        \n        if subset not in [\'train\', \'val\']:\n            raise AssertionError(\'subset must be ""train"" or ""val"".\')\n            \n\n        self.coco = COCO(""{}/annotations/instances_{}2017.json"".format(dataset_dir, subset))\n\n        # get the mapping from original category ids to labels\n        self.cat_ids = self.coco.getCatIds()\n        self.cat2label = {\n            cat_id: i + 1\n            for i, cat_id in enumerate(self.cat_ids)\n        }\n        \n        self.img_ids, self.img_infos = self._filter_imgs()\n        \n        if debug:\n            self.img_ids, self.img_infos = self.img_ids[:50], self.img_infos[:50]\n            \n        self.image_dir = ""{}/{}2017"".format(dataset_dir, subset)\n        \n        self.flip_ratio = flip_ratio\n        \n        if pad_mode in [\'fixed\', \'non-fixed\']:\n            self.pad_mode = pad_mode\n        elif subset == \'train\':\n            self.pad_mode = \'fixed\'\n        else:\n            self.pad_mode = \'non-fixed\'\n        \n        self.img_transform = transforms.ImageTransform(scale, mean, std, pad_mode)\n        self.bbox_transform = transforms.BboxTransform()\n        \n        \n    def _filter_imgs(self, min_size=32):\n        \'\'\'Filter images too small or without ground truths.\n        \n        Args\n        ---\n            min_size: the minimal size of the image.\n        \'\'\'\n        # Filter images without ground truths.\n        all_img_ids = list(set([_[\'image_id\'] for _ in self.coco.anns.values()]))\n        # Filter images too small.\n        img_ids = []\n        img_infos = []\n        for i in all_img_ids:\n            info = self.coco.loadImgs(i)[0]\n            \n            ann_ids = self.coco.getAnnIds(imgIds=i)\n            ann_info = self.coco.loadAnns(ann_ids)\n            ann = self._parse_ann_info(ann_info)\n            \n            if min(info[\'width\'], info[\'height\']) >= min_size and ann[\'labels\'].shape[0] != 0:\n                img_ids.append(i)\n                img_infos.append(info)\n        return img_ids, img_infos\n        \n    def _load_ann_info(self, idx):\n        img_id = self.img_ids[idx]\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        ann_info = self.coco.loadAnns(ann_ids)\n        return ann_info\n\n    def _parse_ann_info(self, ann_info):\n        \'\'\'Parse bbox annotation.\n        \n        Args\n        ---\n            ann_info (list[dict]): Annotation info of an image.\n            \n        Returns\n        ---\n            dict: A dict containing the following keys: bboxes, \n                bboxes_ignore, labels.\n        \'\'\'\n        gt_bboxes = []\n        gt_labels = []\n        gt_bboxes_ignore = []\n\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if ann[\'area\'] <= 0 or w < 1 or h < 1:\n                continue\n            bbox = [y1, x1, y1 + h - 1, x1 + w - 1]\n            if ann[\'iscrowd\']:\n                gt_bboxes_ignore.append(bbox)\n            else:\n                gt_bboxes.append(bbox)\n                gt_labels.append(self.cat2label[ann[\'category_id\']])\n\n        if gt_bboxes:\n            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n            gt_labels = np.array(gt_labels, dtype=np.int64)\n        else:\n            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n            gt_labels = np.array([], dtype=np.int64)\n\n        if gt_bboxes_ignore:\n            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n        else:\n            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        ann = dict(\n            bboxes=gt_bboxes, labels=gt_labels, bboxes_ignore=gt_bboxes_ignore)\n\n\n        return ann\n    \n    def __len__(self):\n        return len(self.img_infos)\n    \n    def __getitem__(self, idx):\n        \'\'\'Load the image and its bboxes for the given index.\n        \n        Args\n        ---\n            idx: the index of images.\n            \n        Returns\n        ---\n            tuple: A tuple containing the following items: image, \n                bboxes, labels.\n        \'\'\'\n        img_info = self.img_infos[idx]\n        ann_info = self._load_ann_info(idx)\n        \n        # load the image.\n        img = cv2.imread(osp.join(self.image_dir, img_info[\'file_name\']), cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        ori_shape = img.shape\n        \n        # Load the annotation.\n        ann = self._parse_ann_info(ann_info)\n        bboxes = ann[\'bboxes\']\n        labels = ann[\'labels\']\n        \n        flip = True if np.random.rand() < self.flip_ratio else False\n        \n        # Handle the image\n        img, img_shape, scale_factor = self.img_transform(img, flip)\n\n        pad_shape = img.shape\n        \n        # Handle the annotation.\n        bboxes, labels = self.bbox_transform(\n            bboxes, labels, img_shape, scale_factor, flip)\n        \n        # Handle the meta info.\n        img_meta_dict = dict({\n            \'ori_shape\': ori_shape,\n            \'img_shape\': img_shape,\n            \'pad_shape\': pad_shape,\n            \'scale_factor\': scale_factor,\n            \'flip\': flip\n        })\n\n        img_meta = utils.compose_image_meta(img_meta_dict)\n        \n        return img, img_meta, bboxes, labels\n    \n    def get_categories(self):\n        \'\'\'Get list of category names. \n        \n        Returns\n        ---\n            list: A list of category names.\n            \n        Note that the first item \'bg\' means background.\n        \'\'\'\n        return [\'bg\'] + [self.coco.loadCats(i)[0][""name""] for i in self.cat2label.keys()]\n\n'"
16-fasterRCNN/detection/datasets/data_generator.py,0,"b'import numpy as np\n\n\n\nclass DataGenerator:\n\n    def __init__(self, dataset, shuffle=False):\n        self.dataset = dataset\n        self.shuffle = shuffle\n    \n    def __call__(self):\n        indices = np.arange(len(self.dataset))\n        if self.shuffle:\n            np.random.shuffle(indices)\n\n        for img_idx in indices:\n            img, img_meta, bbox, label = self.dataset[img_idx]\n            yield img, img_meta, bbox, label\n'"
16-fasterRCNN/detection/datasets/transforms.py,0,"b""import numpy as np\n\nfrom detection.datasets.utils import *\n\nclass ImageTransform(object):\n    '''Preprocess the image.\n    \n        1. rescale the image to expected size\n        2. normalize the image\n        3. flip the image (if needed)\n        4. pad the image (if needed)\n    '''\n    def __init__(self,\n                 scale=(800, 1333),\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 pad_mode='fixed'):\n        self.scale = scale\n        self.mean = mean\n        self.std = std\n        self.pad_mode = pad_mode\n\n        self.impad_size = max(scale) if pad_mode == 'fixed' else 64\n\n    def __call__(self, img, flip=False):\n        img, scale_factor = imrescale(img, self.scale)\n        img_shape = img.shape\n        img = imnormalize(img, self.mean, self.std)\n          \n        if flip:\n            img = img_flip(img)\n        if self.pad_mode == 'fixed':\n            img = impad_to_square(img, self.impad_size)\n\n        else: # 'non-fixed'\n            img = impad_to_multiple(img, self.impad_size)\n        \n        return img, img_shape, scale_factor\n\nclass BboxTransform(object):\n    '''Preprocess ground truth bboxes.\n    \n        1. rescale bboxes according to image size\n        2. flip bboxes (if needed)\n    '''\n    def __init__(self):\n        pass\n    \n    def __call__(self, bboxes, labels, \n                 img_shape, scale_factor, flip=False):\n \n        bboxes = bboxes * scale_factor\n        if flip:\n            bboxes = bbox_flip(bboxes, img_shape)\n            \n        bboxes[:, 0::2] = np.clip(bboxes[:, 0::2], 0, img_shape[0])\n        bboxes[:, 1::2] = np.clip(bboxes[:, 1::2], 0, img_shape[1])\n            \n        return bboxes, labels\n"""
16-fasterRCNN/detection/datasets/utils.py,0,"b""import cv2\nimport numpy as np\n\n###########################################\n#\n# Utility Functions for \n# Image Preprocessing and Data Augmentation\n#\n###########################################\n\ndef img_flip(img):\n    '''Flip the image horizontally\n    \n    Args\n    ---\n        img: [height, width, channel]\n    \n    Returns\n    ---\n        np.ndarray: the flipped image.\n    '''\n    return np.fliplr(img)\n\ndef bbox_flip(bboxes, img_shape):\n    '''Flip bboxes horizontally.\n    \n    Args\n    ---\n        bboxes: [..., 4]\n        img_shape: Tuple. (height, width)\n    \n    Returns\n    ---\n        np.ndarray: the flipped bboxes.\n    '''\n    w = img_shape[1]\n    flipped = bboxes.copy()\n    flipped[..., 1] = w - bboxes[..., 3] - 1\n    flipped[..., 3] = w - bboxes[..., 1] - 1\n    return flipped\n\ndef impad_to_square(img, pad_size):\n    '''Pad an image to ensure each edge to equal to pad_size.\n    \n    Args\n    ---\n        img: [height, width, channels]. Image to be padded\n        pad_size: Int.\n    \n    Returns\n    ---\n        ndarray: The padded image with shape of \n            [pad_size, pad_size, channels].\n    '''\n    shape = (pad_size, pad_size, img.shape[-1])\n    \n    pad = np.zeros(shape, dtype=img.dtype)\n    pad[:img.shape[0], :img.shape[1], ...] = img\n    return pad\n\ndef impad_to_multiple(img, divisor):\n    '''Pad an image to ensure each edge to be multiple to some number.\n    \n    Args\n    ---\n        img: [height, width, channels]. Image to be padded.\n        divisor: Int. Padded image edges will be multiple to divisor.\n    \n    Returns\n    ---\n        ndarray: The padded image.\n    '''\n    pad_h = int(np.ceil(img.shape[0] / divisor)) * divisor\n    pad_w = int(np.ceil(img.shape[1] / divisor)) * divisor\n    shape = (pad_h, pad_w, img.shape[-1])\n    \n    pad = np.zeros(shape, dtype=img.dtype)\n    pad[:img.shape[0], :img.shape[1], ...] = img\n    return pad\n\ndef imrescale(img, scale):\n    '''Resize image while keeping the aspect ratio.\n    \n    Args\n    ---\n        img: [height, width, channels]. The input image.\n        scale: Tuple of 2 integers. the image will be rescaled \n            as large as possible within the scale\n    \n    Returns\n    ---\n        np.ndarray: the scaled image.\n    ''' \n    h, w = img.shape[:2]\n    \n    max_long_edge = max(scale)\n    max_short_edge = min(scale)\n    scale_factor = min(max_long_edge / max(h, w),\n                       max_short_edge / min(h, w))\n    \n    new_size = (int(w * float(scale_factor) + 0.5),\n                int(h * float(scale_factor) + 0.5))\n\n    rescaled_img = cv2.resize(\n        img, new_size, interpolation=cv2.INTER_LINEAR)\n    \n    return rescaled_img, scale_factor\n\ndef imnormalize(img, mean, std):\n    '''Normalize the image.\n    \n    Args\n    ---\n        img: [height, width, channel]\n        mean: Tuple or np.ndarray. [3]\n        std: Tuple or np.ndarray. [3]\n    \n    Returns\n    ---\n        np.ndarray: the normalized image.\n    '''\n    img = (img - mean) / std    \n    return img.astype(np.float32)\n\ndef imdenormalize(norm_img, mean, std):\n    '''Denormalize the image.\n    \n    Args\n    ---\n        norm_img: [height, width, channel]\n        mean: Tuple or np.ndarray. [3]\n        std: Tuple or np.ndarray. [3]\n    \n    Returns\n    ---\n        np.ndarray: the denormalized image.\n    '''\n    img = norm_img * std + mean\n    return img.astype(np.float32)\n\n#######################################\n#\n# Utility Functions for Data Formatting\n#\n#######################################\n\ndef get_original_image(img, img_meta, \n                       mean=(0, 0, 0), std=(1, 1, 1)):\n    '''Recover the origanal image.\n    \n    Args\n    ---\n        img: np.ndarray. [height, width, channel]. \n            The transformed image.\n        img_meta: np.ndarray. [11]\n        mean: Tuple or np.ndarray. [3]\n        std: Tuple or np.ndarray. [3]\n    \n    Returns\n    ---\n        np.ndarray: the original image.\n    '''\n    img_meta_dict = parse_image_meta(img_meta)\n    ori_shape = img_meta_dict['ori_shape']\n    img_shape = img_meta_dict['img_shape']\n    flip = img_meta_dict['flip']\n    \n    img = img[:img_shape[0], :img_shape[1]]\n    if flip:\n        img = img_flip(img)\n    img = cv2.resize(img, (ori_shape[1], ori_shape[0]), \n                     interpolation=cv2.INTER_LINEAR)\n    img = imdenormalize(img, mean, std)\n    return img\n\ndef compose_image_meta(img_meta_dict):\n    '''Takes attributes of an image and puts them in one 1D array.\n\n    Args\n    ---\n        img_meta_dict: dict\n\n    Returns\n    ---\n        img_meta: np.ndarray\n    '''\n    ori_shape = img_meta_dict['ori_shape']\n    img_shape = img_meta_dict['img_shape']\n    pad_shape = img_meta_dict['pad_shape']\n    scale_factor = img_meta_dict['scale_factor']\n    flip = 1 if img_meta_dict['flip'] else 0\n    img_meta = np.array(\n        ori_shape +               # size=3\n        img_shape +               # size=3\n        pad_shape +               # size=3\n        tuple([scale_factor]) +   # size=1\n        tuple([flip])             # size=1\n    ).astype(np.float32)\n\n    return img_meta\n\ndef parse_image_meta(img_meta):\n    '''Parses an array that contains image attributes to its components.\n\n    Args\n    ---\n        meta: [11]\n\n    Returns\n    ---\n        a dict of the parsed values.\n    '''\n    ori_shape = img_meta[0:3]\n    img_shape = img_meta[3:6]\n    pad_shape = img_meta[6:9]\n    scale_factor = img_meta[9]\n    flip = img_meta[10]\n    return {\n        'ori_shape': ori_shape.astype(np.int32),\n        'img_shape': img_shape.astype(np.int32),\n        'pad_shape': pad_shape.astype(np.int32),\n        'scale_factor': scale_factor.astype(np.float32),\n        'flip': flip.astype(np.bool),\n    }\n"""
16-fasterRCNN/detection/utils/misc.py,5,"b""import tensorflow as tf\n\ndef trim_zeros(boxes, name=None):\n    '''\n    Often boxes are represented with matrices of shape [N, 4] and\n    are padded with zeros. This removes zero boxes.\n    \n    Args\n    ---\n        boxes: [N, 4] matrix of boxes.\n        non_zeros: [N] a 1D boolean mask identifying the rows to keep\n    '''\n    non_zeros = tf.cast(tf.reduce_sum(tf.abs(boxes), axis=1), tf.bool)\n    boxes = tf.boolean_mask(boxes, non_zeros, name=name)\n    return boxes, non_zeros\n\ndef parse_image_meta(meta):\n    '''\n    Parses a tensor that contains image attributes to its components.\n    \n    Args\n    ---\n        meta: [..., 11]\n\n    Returns\n    ---\n        a dict of the parsed tensors.\n    '''\n    meta = meta.numpy()\n    ori_shape = meta[..., 0:3]\n    img_shape = meta[..., 3:6]\n    pad_shape = meta[..., 6:9]\n    scale = meta[..., 9]  \n    flip = meta[..., 10]\n    return {\n        'ori_shape': ori_shape,\n        'img_shape': img_shape,\n        'pad_shape': pad_shape,\n        'scale': scale,\n        'flip': flip\n    }\n\ndef calc_batch_padded_shape(meta):\n    '''\n    Args\n    ---\n        meta: [batch_size, 11]\n    \n    Returns\n    ---\n        nd.ndarray. Tuple of (height, width)\n    '''\n    return tf.cast(tf.reduce_max(meta[:, 6:8], axis=0), tf.int32).numpy()\n\ndef calc_img_shapes(meta):\n    '''\n    Args\n    ---\n        meta: [..., 11]\n    \n    Returns\n    ---\n        nd.ndarray. [..., (height, width)]\n    '''\n    return tf.cast(meta[..., 3:5], tf.int32).numpy()\n\n\ndef calc_pad_shapes(meta):\n    '''\n    Args\n    ---\n        meta: [..., 11]\n    \n    Returns\n    ---\n        nd.ndarray. [..., (height, width)]\n    '''\n    return tf.cast(meta[..., 6:8], tf.int32).numpy()"""
16-fasterRCNN/detection/core/anchor/anchor_generator.py,32,"b'import tensorflow as tf\nfrom detection.utils.misc import calc_img_shapes, calc_batch_padded_shape\n\nclass AnchorGenerator:\n    """"""\n    This class operate on padded iamge, eg. [1216, 1216]\n    and generate scales*ratios number of anchor boxes for each point in\n    padded image, with stride = feature_strides\n    number of anchor = (1216 // feature_stride)^2\n    number of anchor boxes = number of anchor * (scales_len*ratio_len)\n    """"""\n    def __init__(self, \n                 scales=(32, 64, 128, 256, 512), \n                 ratios=(0.5, 1, 2), \n                 feature_strides=(4, 8, 16, 32, 64)):\n        \'\'\'\n        Anchor Generator\n        \n        Attributes\n        ---\n            scales: 1D array of anchor sizes in pixels.\n            ratios: 1D array of anchor ratios of width/height.\n            feature_strides: Stride of the feature map relative to the image in pixels.\n        \'\'\'\n        self.scales = scales\n        self.ratios = ratios\n        self.feature_strides = feature_strides\n     \n    def generate_pyramid_anchors(self, img_metas):\n        \'\'\'\n        Generate the multi-level anchors for Region Proposal Network\n        \n        Args\n        ---\n            img_metas: [batch_size, 11]\n        \n        Returns\n        ---\n            anchors: [num_anchors, (y1, x1, y2, x2)] in image coordinates.\n            valid_flags: [batch_size, num_anchors]\n        \'\'\'\n        # generate anchors\n        pad_shape = calc_batch_padded_shape(img_metas) # [1216, 1216]\n        # <class \'list\'>: [(304, 304), (152, 152), (76, 76), (38, 38), (19, 19)]\n        feature_shapes = [(pad_shape[0] // stride, pad_shape[1] // stride)\n                          for stride in self.feature_strides]\n        anchors = [\n            self._generate_level_anchors(level, feature_shape)\n            for level, feature_shape in enumerate(feature_shapes)\n        ] # [277248, 4], [69312, 4], [17328, 4], [4332, 4], [1083, 4]\n        anchors = tf.concat(anchors, axis=0) # [369303, 4]\n        # print(\'total anchors:\', anchors.shape)\n        # print(\'---------\')\n\n        # generate valid flags\n        img_shapes = calc_img_shapes(img_metas) # (800, 1067)\n        valid_flags = [\n            self._generate_valid_flags(anchors, img_shapes[i])\n            for i in range(img_shapes.shape[0])\n        ]\n        valid_flags = tf.stack(valid_flags, axis=0)\n        \n        anchors = tf.stop_gradient(anchors)\n        valid_flags = tf.stop_gradient(valid_flags)\n        \n        return anchors, valid_flags\n    \n    def _generate_valid_flags(self, anchors, img_shape):\n        \'\'\'\n        remove these anchor boxed on padded area\n        ---\n            anchors: [num_anchors, (y1, x1, y2, x2)] in image coordinates.\n            img_shape: Tuple. (height, width, channels)\n            \n        Returns\n        ---\n            valid_flags: [num_anchors]\n        \'\'\'\n        y_center = (anchors[:, 2] + anchors[:, 0]) / 2 # [369300]\n        x_center = (anchors[:, 3] + anchors[:, 1]) / 2\n        \n        valid_flags = tf.ones(anchors.shape[0], dtype=tf.int32) # [369300]\n        zeros = tf.zeros(anchors.shape[0], dtype=tf.int32)\n        # set boxes whose center is out of image area as invalid.\n        valid_flags = tf.where(y_center <= img_shape[0], valid_flags, zeros)\n        valid_flags = tf.where(x_center <= img_shape[1], valid_flags, zeros)\n        \n        return valid_flags\n    \n    def _generate_level_anchors(self, level, feature_shape):\n        \'\'\'Generate the anchors given the spatial shape of feature map.\n        \n        scale: 32\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (256, 256) (256, 256)\n        scale: 64\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (128, 128) (128, 128)\n        scale: 128\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (64, 64) (64, 64)\n        scale: 256\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (32, 32) (32, 32)\n        scale: 512\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (16, 16) (16, 16)\n\n\n        scale: 32\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (304, 304) (304, 304)\n        boxes: (277248, 4)\n        scale: 64\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (152, 152) (152, 152)\n        boxes: (69312, 4)\n        scale: 128\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (76, 76) (76, 76)\n        boxes: (17328, 4)\n        scale: 256\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (38, 38) (38, 38)\n        boxes: (4332, 4)\n        scale: 512\n        ratios: tf.Tensor([0.5 1.  2. ], shape=(3,), dtype=float32)\n        pos: (19, 19) (19, 19)\n        boxes: (1083, 4)\n        total anchors: (369303, 4)\n\n        ---\n            feature_shape: (height, width)\n\n        Returns\n        ---\n            numpy.ndarray [anchors_num, (y1, x1, y2, x2)]\n        \'\'\'\n        scale = self.scales[level]\n        ratios = self.ratios\n        feature_stride = self.feature_strides[level]\n        \n        # Get all combinations of scales and ratios\n        scales, ratios = tf.meshgrid([float(scale)], ratios)\n        scales = tf.reshape(scales, [-1]) # [32, 32, 32]\n        ratios = tf.reshape(ratios, [-1]) # [0.5, 1, 2]\n        \n        # Enumerate heights and widths from scales and ratios\n        heights = scales / tf.sqrt(ratios) # [45, 32, 22], square root\n        widths = scales * tf.sqrt(ratios)  # [22, 32, 45]\n\n        # Enumerate shifts in feature space, [0, 4, ..., 1216-4]\n        shifts_y = tf.multiply(tf.range(feature_shape[0]), feature_stride)\n        shifts_x = tf.multiply(tf.range(feature_shape[1]), feature_stride)\n        \n        shifts_x, shifts_y = tf.cast(shifts_x, tf.float32), tf.cast(shifts_y, tf.float32)\n        shifts_x, shifts_y = tf.meshgrid(shifts_x, shifts_y) # [304, 304, 2] coordinates\n\n        # Enumerate combinations of shifts, widths, and heights # mesh A: [3] B:[304,304]=>[92416] =>[92416,3,2]\n        box_widths, box_centers_x = tf.meshgrid(widths, shifts_x)\n        box_heights, box_centers_y = tf.meshgrid(heights, shifts_y)\n\n        # Reshape to get a list of (y, x) and a list of (h, w)\n        box_centers = tf.reshape(tf.stack([box_centers_y, box_centers_x], axis=2), (-1, 2))\n        box_sizes = tf.reshape(tf.stack([box_heights, box_widths], axis=2), (-1, 2))\n\n        # Convert to corner coordinates (y1, x1, y2, x2) [304x304, 3, 4] => [277448, 4]\n        boxes = tf.concat([box_centers - 0.5 * box_sizes,\n                           box_centers + 0.5 * box_sizes], axis=1)\n        # print(\'scale:\', scale)\n        # print(\'ratios:\', ratios)\n        # print(\'pos:\', shifts_x.shape, shifts_y.shape)\n        # print(\'boxes:\', boxes.shape)\n        return boxes\n'"
16-fasterRCNN/detection/core/anchor/anchor_target.py,32,"b'import tensorflow as tf\nfrom detection.core.bbox import geometry, transforms\nfrom detection.utils.misc import trim_zeros\n\n\n\nclass AnchorTarget:\n    """"""\n    for every generated anchors boxes: [326393, 4],\n    create its rpn_target_matchs and rpn_target_matchs\n    which is used to train RPN network.\n    """"""\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.), \n                 target_stds=(0.1, 0.1, 0.2, 0.2),\n                 num_rpn_deltas=256,\n                 positive_fraction=0.5,\n                 pos_iou_thr=0.7,\n                 neg_iou_thr=0.3):\n        \'\'\'\n        Compute regression and classification targets for anchors.\n        \n        Attributes\n        ---\n            target_means: [4]. Bounding box refinement mean for RPN.\n            target_stds: [4]. Bounding box refinement standard deviation for RPN.\n            num_rpn_deltas: int. Maximal number of Anchors per image to feed to rpn heads.\n            positive_fraction: float.\n            pos_iou_thr: float.\n            neg_iou_thr: float.\n        \'\'\'\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.num_rpn_deltas = num_rpn_deltas\n        self.positive_fraction = positive_fraction\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n\n    def build_targets(self, anchors, valid_flags, gt_boxes, gt_class_ids):\n        \'\'\'\n        Given the anchors and GT boxes, compute overlaps and identify positive\n        anchors and deltas to refine them to match their corresponding GT boxes.\n\n        Args\n        ---\n            anchors: [num_anchors, (y1, x1, y2, x2)] in image coordinates.\n            valid_flags: [batch_size, num_anchors]\n            gt_boxes: [batch_size, num_gt_boxes, (y1, x1, y2, x2)] in image \n                coordinates. batch_size = 1 usually\n            gt_class_ids: [batch_size, num_gt_boxes] Integer class IDs.\n\n        Returns\n        ---\n            rpn_target_matchs: [batch_size, num_anchors] matches between anchors and GT boxes.\n                1 = positive anchor, -1 = negative anchor, 0 = neutral anchor\n            rpn_target_deltas: [batch_size, num_rpn_deltas, (dy, dx, log(dh), log(dw))] \n                Anchor bbox deltas.\n        \'\'\'\n        rpn_target_matchs = []\n        rpn_target_deltas = []\n        \n        num_imgs = gt_class_ids.shape[0] # namely, batchsz , 1\n        for i in range(num_imgs):\n            target_match, target_delta = self._build_single_target(\n                anchors, valid_flags[i], gt_boxes[i], gt_class_ids[i])\n            rpn_target_matchs.append(target_match)\n            rpn_target_deltas.append(target_delta)\n        \n        rpn_target_matchs = tf.stack(rpn_target_matchs)\n        rpn_target_deltas = tf.stack(rpn_target_deltas)\n        \n        rpn_target_matchs = tf.stop_gradient(rpn_target_matchs)\n        rpn_target_deltas = tf.stop_gradient(rpn_target_deltas)\n        \n        return rpn_target_matchs, rpn_target_deltas\n\n    def _build_single_target(self, anchors, valid_flags, gt_boxes, gt_class_ids):\n        \'\'\'Compute targets per instance.\n        \n        Args\n        ---\n            anchors: [num_anchors, (y1, x1, y2, x2)]\n            valid_flags: [num_anchors]\n            gt_class_ids: [num_gt_boxes]\n            gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\n        \n        Returns\n        ---\n            target_matchs: [num_anchors]\n            target_deltas: [num_rpn_deltas, (dy, dx, log(dh), log(dw))] \n        \'\'\'\n        gt_boxes, _ = trim_zeros(gt_boxes) # remove padded zero boxes, [new_N, 4]\n        \n        target_matchs = tf.zeros(anchors.shape[0], dtype=tf.int32) # [326393]\n        \n        # Compute overlaps [num_anchors, num_gt_boxes] 326393 vs 10 => [326393, 10]\n        overlaps = geometry.compute_overlaps(anchors, gt_boxes)\n\n        # Match anchors to GT Boxes\n        # If an anchor overlaps ANY GT box with IoU >= 0.7 then it\'s positive.\n        # If an anchor overlaps ALL GT box with IoU < 0.3 then it\'s negative.\n        # Neutral anchors are those that don\'t match the conditions above,\n        # and they don\'t influence the loss function.\n        # However, don\'t keep any GT box unmatched (rare, but happens). Instead,\n        # match it to the closest anchor (even if its max IoU is < 0.3).\n        \n        neg_values = tf.constant([0, -1])\n        pos_values = tf.constant([0, 1])\n        \n        # 1. Set negative anchors first. They get overwritten below if a GT box is\n        # matched to them. [N_anchors, N_gt_boxes]\n        anchor_iou_argmax = tf.argmax(overlaps, axis=1) # [326396] get clost gt boxes for each anchors\n        anchor_iou_max = tf.reduce_max(overlaps, axis=[1]) # [326396] get closet gt boxes\'s overlap scores\n        # if an anchor box overlap all GT box with IoU < 0.3, marked as -1 background\n        target_matchs = tf.where(anchor_iou_max < self.neg_iou_thr, \n                                 -tf.ones(anchors.shape[0], dtype=tf.int32), target_matchs)\n\n        # filter invalid anchors\n        target_matchs = tf.where(tf.equal(valid_flags, 1),\n                                 target_matchs, tf.zeros(anchors.shape[0], dtype=tf.int32))\n        # if an anchor overlap with any GT box with IoU > 0.7, marked as foreground\n        # 2. Set anchors with high overlap as positive.\n        target_matchs = tf.where(anchor_iou_max >= self.pos_iou_thr, \n                                 tf.ones(anchors.shape[0], dtype=tf.int32), target_matchs)\n\n        # 3. Set an anchor for each GT box (regardless of IoU value).        \n        gt_iou_argmax = tf.argmax(overlaps, axis=0) # [N_gt_boxes]\n        target_matchs = tf.compat.v1.scatter_update(tf.Variable(target_matchs), gt_iou_argmax, 1)\n        # update corresponding value=>1 for GT boxes\' closest boxes\n        \n        # Subsample to balance positive and negative anchors\n        # Don\'t let positives be more than half the anchors\n        ids = tf.where(tf.equal(target_matchs, 1))  # [N_pos_anchors, 1], [15, 1]\n        ids = tf.squeeze(ids, 1) # [15]\n        extra = ids.shape.as_list()[0] - int(self.num_rpn_deltas * self.positive_fraction) # 256*0.5\n        if extra > 0: # extra means the redundant pos_anchors\n            # Reset the extra random ones to neutral\n            ids = tf.random.shuffle(ids)[:extra]\n            target_matchs = tf.compat.v1.scatter_update(target_matchs, ids, 0)\n        # Same for negative proposals\n        ids = tf.where(tf.equal(target_matchs, -1)) # [213748, 1]\n        ids = tf.squeeze(ids, 1)\n        extra = ids.shape.as_list()[0] - (self.num_rpn_deltas - # 213748 - (256 - num_of_pos_anchors:15)\n            tf.reduce_sum(tf.cast(tf.equal(target_matchs, 1), tf.int32)))\n        if extra > 0: # 213507, so many negative anchors!\n            # Rest the extra ones to neutral\n            ids = tf.random.shuffle(ids)[:extra]\n            target_matchs = tf.compat.v1.scatter_update(target_matchs, ids, 0)\n        # since we only need 256 anchors, and it had better contains half positive anchors, and harlf neg .\n        \n        # For positive anchors, compute shift and scale needed to transform them\n        # to match the corresponding GT boxes.\n        ids = tf.where(tf.equal(target_matchs, 1)) # [15]\n        \n        a = tf.gather_nd(anchors, ids) # [369303, 4], [15] => [15, 4]\n        anchor_idx = tf.gather_nd(anchor_iou_argmax, ids) # closed gt boxes index for 369303 anchors\n        gt = tf.gather(gt_boxes, anchor_idx) # get closed gt boxes coordinates for ids=15\n        # a: [15, 4], postive anchors, gt: [15, 4] closed gt boxes for each anchors=15\n        target_deltas = transforms.bbox2delta(\n            a, gt, self.target_means, self.target_stds)\n        # target_deltas: [15, (dy,dx,logw,logh)]?\n        padding = tf.maximum(self.num_rpn_deltas - tf.shape(target_deltas)[0], 0) # 256-15\n        target_deltas = tf.pad(target_deltas, [(0, padding), (0, 0)]) #padding to [256,4], last padding 0\n\n        return target_matchs, target_deltas'"
16-fasterRCNN/detection/core/bbox/bbox_target.py,21,"b""import numpy as np\nimport tensorflow as tf\n\nfrom detection.core.bbox import geometry, transforms\nfrom detection.utils.misc import *\n\nclass ProposalTarget:\n\n    def __init__(self,\n                 target_means=(0., 0., 0., 0.),\n                 target_stds=(0.1, 0.1, 0.2, 0.2), \n                 num_rcnn_deltas=256,\n                 positive_fraction=0.25,\n                 pos_iou_thr=0.5,\n                 neg_iou_thr=0.5):\n        '''\n        Compute regression and classification targets for proposals.\n        \n        Attributes\n        ---\n            target_means: [4]. Bounding box refinement mean for RCNN.\n            target_stds: [4]. Bounding box refinement standard deviation for RCNN.\n            num_rcnn_deltas: int. Maximal number of RoIs per image to feed to bbox heads.\n\n        '''\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.num_rcnn_deltas = num_rcnn_deltas\n        self.positive_fraction = positive_fraction\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n            \n    def build_targets(self, proposals_list, gt_boxes, gt_class_ids, img_metas):\n        '''\n        Generates detection targets for images. Subsamples proposals and\n        generates target class IDs, bounding box deltas for each.\n        \n        Args\n        ---\n            proposals_list: list of [num_proposals, (y1, x1, y2, x2)] in normalized coordinates.\n            gt_boxes: [batch_size, num_gt_boxes, (y1, x1, y2, x2)] in image coordinates.\n            gt_class_ids: [batch_size, num_gt_boxes] Integer class IDs.\n            img_metas: [batch_size, 11]\n            \n        Returns\n        ---\n            rois_list: list of [num_rois, (y1, x1, y2, x2)] in normalized coordinates\n            rcnn_target_matchs_list: list of [num_rois]. Integer class IDs.\n            rcnn_target_deltas_list: list of [num_positive_rois, (dy, dx, log(dh), log(dw))].\n            \n        Note that self.num_rcnn_deltas >= num_rois > num_positive_rois. And different \n           images in one batch may have different num_rois and num_positive_rois.\n        '''\n        \n        pad_shapes = calc_pad_shapes(img_metas) # [[1216, 1216]]\n        \n        rois_list = []\n        rcnn_target_matchs_list = []\n        rcnn_target_deltas_list = []\n        \n        for i in range(img_metas.shape[0]):\n            rois, target_matchs, target_deltas = self._build_single_target(\n                proposals_list[i], gt_boxes[i], gt_class_ids[i], pad_shapes[i])\n            rois_list.append(rois) # [192, 4], including pos/neg anchors\n            rcnn_target_matchs_list.append(target_matchs) # positive target label, and padding with zero for neg\n            rcnn_target_deltas_list.append(target_deltas) # positive target deltas, and padding with zero for neg\n        \n        return rois_list, rcnn_target_matchs_list, rcnn_target_deltas_list\n    \n    def _build_single_target(self, proposals, gt_boxes, gt_class_ids, img_shape):\n        '''\n        Args\n        ---\n            proposals: [num_proposals, (y1, x1, y2, x2)] in normalized coordinates.\n            gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\n            gt_class_ids: [num_gt_boxes]\n            img_shape: np.ndarray. [2]. (img_height, img_width)\n            \n        Returns\n        ---\n            rois: [num_rois, (y1, x1, y2, x2)]\n            target_matchs: [num_positive_rois]\n            target_deltas: [num_positive_rois, (dy, dx, log(dh), log(dw))]\n        '''\n        H, W = img_shape # 1216, 1216\n        \n        \n        gt_boxes, non_zeros = trim_zeros(gt_boxes) # [7, 4], remove padded zero boxes\n        gt_class_ids = tf.boolean_mask(gt_class_ids, non_zeros) # [7]\n        # normalize (y1, x1, y2, x2) => 0~1\n        gt_boxes = gt_boxes / tf.constant([H, W, H, W], dtype=tf.float32)\n        # [2k, 4] with [7, 4] => [2k, 7] overlop scores\n        overlaps = geometry.compute_overlaps(proposals, gt_boxes)\n        anchor_iou_argmax = tf.argmax(overlaps, axis=1) # [2000]get cloest gt boxed id for each anchor boxes\n        roi_iou_max = tf.reduce_max(overlaps, axis=1) # [2000]get clost gt boxes overlop score for each anchor boxes\n        # roi_iou_max: [2000],\n        positive_roi_bool = (roi_iou_max >= self.pos_iou_thr) #[2000]\n        positive_indices = tf.where(positive_roi_bool)[:, 0] #[48, 1] =>[48]\n        # get all positive indices, namely get all pos_anchor indices\n        negative_indices = tf.where(roi_iou_max < self.neg_iou_thr)[:, 0]\n        # get all negative anchor indices\n        # Subsample ROIs. Aim for 33% positive\n        # Positive ROIs\n        positive_count = int(self.num_rcnn_deltas * self.positive_fraction) # 0.25?\n        positive_indices = tf.random.shuffle(positive_indices)[:positive_count] # [256*0.25]=64, at most get 64\n        positive_count = tf.shape(positive_indices)[0] # 34\n        \n        # Negative ROIs. Add enough to maintain positive:negative ratio.\n        r = 1.0 / self.positive_fraction\n        negative_count = tf.cast(r * tf.cast(positive_count, tf.float32), tf.int32) - positive_count #102\n        negative_indices = tf.random.shuffle(negative_indices)[:negative_count] #[102]\n        \n        # Gather selected ROIs, based on remove redundant pos/neg indices\n        positive_rois = tf.gather(proposals, positive_indices) # [34, 4]\n        negative_rois = tf.gather(proposals, negative_indices) # [102, 4]\n        \n        # Assign positive ROIs to GT boxes.\n        positive_overlaps = tf.gather(overlaps, positive_indices) # [34, 7]\n        roi_gt_box_assignment = tf.argmax(positive_overlaps, axis=1) # [34]for each anchor, get its clost gt boxes\n        roi_gt_boxes = tf.gather(gt_boxes, roi_gt_box_assignment) # [34, 4]\n        target_matchs = tf.gather(gt_class_ids, roi_gt_box_assignment) # [34]\n        # target_matchs, target_deltas all get!!\n        # proposal: [34, 4], target: [34, 4]\n        target_deltas = transforms.bbox2delta(positive_rois, roi_gt_boxes, self.target_means, self.target_stds)\n        # [34, 4] [102, 4]\n        rois = tf.concat([positive_rois, negative_rois], axis=0)\n        \n        N = tf.shape(negative_rois)[0] # 102\n        target_matchs = tf.pad(target_matchs, [(0, N)]) # [34] padding after with [N]\n        \n        target_matchs = tf.stop_gradient(target_matchs) # [34+102]\n        target_deltas = tf.stop_gradient(target_deltas) # [34, 4]\n        # rois: [34+102, 4]\n        return rois, target_matchs, target_deltas"""
16-fasterRCNN/detection/core/bbox/geometry.py,12,"b""import tensorflow as tf\n\ndef compute_overlaps(boxes1, boxes2):\n    '''Computes IoU overlaps between two sets of boxes.\n    boxes1, boxes2: [N, (y1, x1, y2, x2)].\n    '''\n    # 1. Tile boxes2 and repeate boxes1. This allows us to compare\n    # every boxes1 against every boxes2 without loops.\n    # TF doesn't have an equivalent to np.repeate() so simulate it\n    # using tf.tile() and tf.reshape.\n    b1 = tf.reshape(tf.tile(tf.expand_dims(boxes1, 1),\n                            [1, 1, tf.shape(boxes2)[0]]), [-1, 4])\n    b2 = tf.tile(boxes2, [tf.shape(boxes1)[0], 1])\n    # 2. Compute intersections\n    b1_y1, b1_x1, b1_y2, b1_x2 = tf.split(b1, 4, axis=1)\n    b2_y1, b2_x1, b2_y2, b2_x2 = tf.split(b2, 4, axis=1)\n    y1 = tf.maximum(b1_y1, b2_y1)\n    x1 = tf.maximum(b1_x1, b2_x1)\n    y2 = tf.minimum(b1_y2, b2_y2)\n    x2 = tf.minimum(b1_x2, b2_x2)\n    intersection = tf.maximum(x2 - x1, 0) * tf.maximum(y2 - y1, 0)\n    # 3. Compute unions\n    b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n    b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n    union = b1_area + b2_area - intersection\n    # 4. Compute IoU and reshape to [boxes1, boxes2]\n    iou = intersection / union\n    overlaps = tf.reshape(iou, [tf.shape(boxes1)[0], tf.shape(boxes2)[0]])\n    return overlaps\n"""
16-fasterRCNN/detection/core/bbox/transforms.py,27,"b""import tensorflow as tf\n\nfrom detection.utils.misc import *\n\ndef bbox2delta(box, gt_box, target_means, target_stds):\n    '''Compute refinement needed to transform box to gt_box.\n    \n    Args\n    ---\n        box: [..., (y1, x1, y2, x2)]\n        gt_box: [..., (y1, x1, y2, x2)]\n        target_means: [4]\n        target_stds: [4]\n    '''\n    target_means = tf.constant(\n        target_means, dtype=tf.float32)\n    target_stds = tf.constant(\n        target_stds, dtype=tf.float32)\n    \n    box = tf.cast(box, tf.float32)\n    gt_box = tf.cast(gt_box, tf.float32)\n\n    height = box[..., 2] - box[..., 0]\n    width = box[..., 3] - box[..., 1]\n    center_y = box[..., 0] + 0.5 * height\n    center_x = box[..., 1] + 0.5 * width\n\n    gt_height = gt_box[..., 2] - gt_box[..., 0]\n    gt_width = gt_box[..., 3] - gt_box[..., 1]\n    gt_center_y = gt_box[..., 0] + 0.5 * gt_height\n    gt_center_x = gt_box[..., 1] + 0.5 * gt_width\n\n    dy = (gt_center_y - center_y) / height\n    dx = (gt_center_x - center_x) / width\n    dh = tf.math.log(gt_height / height)\n    dw = tf.math.log(gt_width / width)\n\n    delta = tf.stack([dy, dx, dh, dw], axis=-1)\n    delta = (delta - target_means) / target_stds\n    \n    return delta\n\ndef delta2bbox(box, delta, target_means, target_stds):\n    '''Compute bounding box based on roi and delta.\n    \n    Args\n    ---\n        box: [N, (y1, x1, y2, x2)] box to update\n        delta: [N, (dy, dx, log(dh), log(dw))] refinements to apply\n        target_means: [4]\n        target_stds: [4]\n    '''\n    target_means = tf.constant(\n        target_means, dtype=tf.float32)\n    target_stds = tf.constant(\n        target_stds, dtype=tf.float32)\n    delta = delta * target_stds + target_means    \n    # Convert to y, x, h, w\n    height = box[:, 2] - box[:, 0]\n    width = box[:, 3] - box[:, 1]\n    center_y = box[:, 0] + 0.5 * height\n    center_x = box[:, 1] + 0.5 * width\n    \n    # Apply delta\n    center_y += delta[:, 0] * height\n    center_x += delta[:, 1] * width\n    height *= tf.exp(delta[:, 2])\n    width *= tf.exp(delta[:, 3])\n    \n    # Convert back to y1, x1, y2, x2\n    y1 = center_y - 0.5 * height\n    x1 = center_x - 0.5 * width\n    y2 = y1 + height\n    x2 = x1 + width\n    result = tf.stack([y1, x1, y2, x2], axis=1)\n    return result\n\ndef bbox_clip(box, window):\n    '''\n    Args\n    ---\n        box: [N, (y1, x1, y2, x2)]\n        window: [4] in the form y1, x1, y2, x2\n    '''\n    # Split\n    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n    y1, x1, y2, x2 = tf.split(box, 4, axis=1)\n    # Clip\n    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n    clipped = tf.concat([y1, x1, y2, x2], axis=1)\n    clipped.set_shape((clipped.shape[0], 4))\n    return clipped\n\ndef bbox_flip(bboxes, width):\n    '''\n    Flip bboxes horizontally.\n    \n    Args\n    ---\n        bboxes: [..., 4]\n        width: Int or Float\n    '''\n    y1, x1, y2, x2 = tf.split(bboxes, 4, axis=-1)\n    \n    new_x1 = width - x2\n    new_x2 = width - x1\n    \n    flipped = tf.concat([y1, new_x1, y2, new_x2], axis=-1)\n    \n    return flipped\n\n\n\ndef bbox_mapping(box, img_meta):\n    '''\n    Args\n    ---\n        box: [N, 4]\n        img_meta: [11]\n    '''\n    img_meta = parse_image_meta(img_meta)\n    scale = img_meta['scale']\n    flip = img_meta['flip']\n    \n    box = box * scale\n    if tf.equal(flip, 1):\n        box = bbox_flip(box, img_meta['img_shape'][1])\n    \n    return box\n\ndef bbox_mapping_back(box, img_meta):\n    '''\n    Args\n    ---\n        box: [N, 4]\n        img_meta: [11]\n    '''\n    img_meta = parse_image_meta(img_meta)\n    scale = img_meta['scale']\n    flip = img_meta['flip']\n    if tf.equal(flip, 1):\n        box = bbox_flip(box, img_meta['img_shape'][1])\n    box = box / scale\n    \n    return box"""
16-fasterRCNN/detection/core/loss/losses.py,29,"b""import tensorflow as tf\nfrom    tensorflow import keras\n\n\ndef smooth_l1_loss(y_true, y_pred):\n    '''Implements Smooth-L1 loss.\n    \n    Args\n    ---\n        y_true and y_pred are typically: [N, 4], but could be any shape.\n    '''\n    diff = tf.abs(y_true - y_pred)\n    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n    return loss\n\n\n\ndef rpn_class_loss(target_matchs, rpn_class_logits):\n    '''RPN anchor classifier loss.\n    \n    Args\n    ---\n        target_matchs: [batch_size, num_anchors]. Anchor match type. 1=positive,\n            -1=negative, 0=neutral anchor.\n        rpn_class_logits: [batch_size, num_anchors, 2]. RPN classifier logits for FG/BG.\n    '''\n\n    # Get anchor classes. Convert the -1/+1 match to 0/1 values.\n    anchor_class = tf.cast(tf.equal(target_matchs, 1), tf.int32)\n    # Positive and Negative anchors contribute to the loss,\n    # but neutral anchors (match value = 0) don't.\n    indices = tf.where(tf.not_equal(target_matchs, 0))\n    # Pick rows that contribute to the loss and filter out the rest.\n    rpn_class_logits = tf.gather_nd(rpn_class_logits, indices)\n    anchor_class = tf.gather_nd(anchor_class, indices)\n    # Cross entropy loss\n    # loss = tf.losses.sparse_softmax_cross_entropy(labels=anchor_class,\n    #                                               logits=rpn_class_logits)\n\n    num_classes = rpn_class_logits.shape[-1]\n    # print(rpn_class_logits.shape)\n    loss = keras.losses.categorical_crossentropy(tf.one_hot(anchor_class, depth=num_classes),\n                                                 rpn_class_logits, from_logits=True)\n\n    \n    loss = tf.reduce_mean(loss) if tf.size(loss) > 0 else tf.constant(0.0)\n    return loss\n\n\ndef rpn_bbox_loss(target_deltas, target_matchs, rpn_deltas):\n    '''Return the RPN bounding box loss graph.\n    \n    Args\n    ---\n        target_deltas: [batch, num_rpn_deltas, (dy, dx, log(dh), log(dw))].\n            Uses 0 padding to fill in unsed bbox deltas.\n        target_matchs: [batch, anchors]. Anchor match type. 1=positive,\n            -1=negative, 0=neutral anchor.\n        rpn_deltas: [batch, anchors, (dy, dx, log(dh), log(dw))]\n    '''\n    def batch_pack(x, counts, num_rows):\n        '''Picks different number of values from each row\n        in x depending on the values in counts.\n        '''\n        outputs = []\n        for i in range(num_rows):\n            outputs.append(x[i, :counts[i]])\n        return tf.concat(outputs, axis=0)\n    \n    # Positive anchors contribute to the loss, but negative and\n    # neutral anchors (match value of 0 or -1) don't.\n    indices = tf.where(tf.equal(target_matchs, 1))\n\n    # Pick bbox deltas that contribute to the loss\n    rpn_deltas = tf.gather_nd(rpn_deltas, indices)\n\n    # Trim target bounding box deltas to the same length as rpn_deltas.\n    batch_counts = tf.reduce_sum(tf.cast(tf.equal(target_matchs, 1), tf.int32), axis=1)\n    target_deltas = batch_pack(target_deltas, batch_counts,\n                              target_deltas.shape.as_list()[0])\n\n    loss = smooth_l1_loss(target_deltas, rpn_deltas)\n    \n    loss = tf.reduce_mean(loss) if tf.size(loss) > 0 else tf.constant(0.0)\n    \n    return loss\n\n\n\n\n\ndef rcnn_class_loss(target_matchs_list, rcnn_class_logits_list):\n    '''Loss for the classifier head of Faster RCNN.\n    \n    Args\n    ---\n        target_matchs_list: list of [num_rois]. Integer class IDs. Uses zero\n            padding to fill in the array.\n        rcnn_class_logits_list: list of [num_rois, num_classes]\n    '''\n    \n    class_ids = tf.concat(target_matchs_list, 0)\n    class_logits = tf.concat(rcnn_class_logits_list, 0)\n    class_ids = tf.cast(class_ids, 'int64')\n    \n    # loss = tf.losses.sparse_softmax_cross_entropy(labels=class_ids,\n    #                                               logits=class_logits)\n\n    num_classes = class_logits.shape[-1]\n    # print(class_logits.shape)\n    loss = keras.losses.categorical_crossentropy(tf.one_hot(class_ids, depth=num_classes),\n                                                 class_logits, from_logits=True)\n\n\n    loss = tf.reduce_mean(loss) if tf.size(loss) > 0 else tf.constant(0.0)\n    return loss\n\n\ndef rcnn_bbox_loss(target_deltas_list, target_matchs_list, rcnn_deltas_list):\n    '''Loss for Faster R-CNN bounding box refinement.\n    \n    Args\n    ---\n        target_deltas_list: list of [num_positive_rois, (dy, dx, log(dh), log(dw))]\n        target_matchs_list: list of [num_rois]. Integer class IDs.\n        rcnn_deltas_list: list of [num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n    '''\n    \n    target_deltas = tf.concat(target_deltas_list, 0)\n    target_class_ids = tf.concat(target_matchs_list, 0)\n    rcnn_deltas = tf.concat(rcnn_deltas_list, 0)\n\n    # Only positive ROIs contribute to the loss. And only\n    # the right class_id of each ROI. Get their indicies.\n    positive_roi_ix = tf.where(target_class_ids > 0)[:, 0]\n    positive_roi_class_ids = tf.cast(\n        tf.gather(target_class_ids, positive_roi_ix), tf.int64)\n    indices = tf.stack([positive_roi_ix, positive_roi_class_ids], axis=1)\n    \n    # Gather the deltas (predicted and true) that contribute to loss\n    rcnn_deltas = tf.gather_nd(rcnn_deltas, indices)\n\n    # Smooth-L1 Loss\n    loss = smooth_l1_loss(target_deltas, rcnn_deltas)\n    loss = tf.reduce_mean(loss) if tf.size(loss) > 0 else tf.constant(0.0)\n\n    return loss\n"""
16-fasterRCNN/detection/models/backbones/resnet.py,13,"b""'''ResNet model for Keras.\n\n# Reference:\n- [Deep Residual Learning for Image Recognition](\n    https://arxiv.org/abs/1512.03385)\n\n'''\nimport  tensorflow as tf\nfrom    tensorflow.keras import layers\n\nclass _Bottleneck(tf.keras.Model):\n\n    def __init__(self, filters, block, \n                 downsampling=False, stride=1, **kwargs):\n        super(_Bottleneck, self).__init__(**kwargs)\n\n        filters1, filters2, filters3 = filters\n        conv_name_base = 'res' + block + '_branch'\n        bn_name_base   = 'bn'  + block + '_branch'\n\n        self.downsampling = downsampling\n        self.stride = stride\n        self.out_channel = filters3\n        \n        self.conv2a = layers.Conv2D(filters1, (1, 1), strides=(stride, stride),\n                                    kernel_initializer='he_normal',\n                                    name=conv_name_base + '2a')\n        self.bn2a = layers.BatchNormalization(name=bn_name_base + '2a')\n\n        self.conv2b = layers.Conv2D(filters2, (3, 3), padding='same',\n                                    kernel_initializer='he_normal',\n                                    name=conv_name_base + '2b')\n        self.bn2b = layers.BatchNormalization(name=bn_name_base + '2b')\n\n        self.conv2c = layers.Conv2D(filters3, (1, 1),\n                                    kernel_initializer='he_normal',\n                                    name=conv_name_base + '2c')\n        self.bn2c = layers.BatchNormalization(name=bn_name_base + '2c')\n         \n        if self.downsampling:\n            self.conv_shortcut = layers.Conv2D(filters3, (1, 1), strides=(stride, stride),\n                                               kernel_initializer='he_normal',\n                                               name=conv_name_base + '1')\n            self.bn_shortcut = layers.BatchNormalization(name=bn_name_base + '1')     \n    \n    def call(self, inputs, training=False):\n        x = self.conv2a(inputs)\n        x = self.bn2a(x, training=training)\n        x = tf.nn.relu(x)\n        \n        x = self.conv2b(x)\n        x = self.bn2b(x, training=training)\n        x = tf.nn.relu(x)\n        \n        x = self.conv2c(x)\n        x = self.bn2c(x, training=training)\n        \n        if self.downsampling:\n            shortcut = self.conv_shortcut(inputs)\n            shortcut = self.bn_shortcut(shortcut, training=training)\n        else:\n            shortcut = inputs\n            \n        x += shortcut\n        x = tf.nn.relu(x)\n        \n        return x\n    \n    def compute_output_shape(self, input_shape):\n        shape = tf.TensorShape(input_shape).as_list()\n\n        shape[1] = shape[1] // self.stride\n        shape[2] = shape[2] // self.stride\n        shape[-1] = self.out_channel\n        return tf.TensorShape(shape)        \n        \n\nclass ResNet(tf.keras.Model):\n\n    def __init__(self, depth, **kwargs):\n        super(ResNet, self).__init__(**kwargs)\n              \n        if depth not in [50, 101]:\n            raise AssertionError('depth must be 50 or 101.')\n        self.depth = depth\n    \n        self.padding = layers.ZeroPadding2D((3, 3))\n        self.conv1 = layers.Conv2D(64, (7, 7),\n                                   strides=(2, 2),\n                                   kernel_initializer='he_normal',\n                                   name='conv1')\n        self.bn_conv1 = layers.BatchNormalization(name='bn_conv1')\n        self.max_pool = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')\n        \n        self.res2a = _Bottleneck([64, 64, 256], block='2a',\n                                 downsampling=True, stride=1)\n        self.res2b = _Bottleneck([64, 64, 256], block='2b')\n        self.res2c = _Bottleneck([64, 64, 256], block='2c')\n        \n        self.res3a = _Bottleneck([128, 128, 512], block='3a', \n                                 downsampling=True, stride=2)\n        self.res3b = _Bottleneck([128, 128, 512], block='3b')\n        self.res3c = _Bottleneck([128, 128, 512], block='3c')\n        self.res3d = _Bottleneck([128, 128, 512], block='3d')\n        \n        self.res4a = _Bottleneck([256, 256, 1024], block='4a', \n                                 downsampling=True, stride=2)\n        self.res4b = _Bottleneck([256, 256, 1024], block='4b')\n        self.res4c = _Bottleneck([256, 256, 1024], block='4c')\n        self.res4d = _Bottleneck([256, 256, 1024], block='4d')\n        self.res4e = _Bottleneck([256, 256, 1024], block='4e')\n        self.res4f = _Bottleneck([256, 256, 1024], block='4f')\n        if self.depth == 101:\n            self.res4g = _Bottleneck([256, 256, 1024], block='4g')\n            self.res4h = _Bottleneck([256, 256, 1024], block='4h')\n            self.res4i = _Bottleneck([256, 256, 1024], block='4i')\n            self.res4j = _Bottleneck([256, 256, 1024], block='4j')\n            self.res4k = _Bottleneck([256, 256, 1024], block='4k')\n            self.res4l = _Bottleneck([256, 256, 1024], block='4l')\n            self.res4m = _Bottleneck([256, 256, 1024], block='4m')\n            self.res4n = _Bottleneck([256, 256, 1024], block='4n')\n            self.res4o = _Bottleneck([256, 256, 1024], block='4o')\n            self.res4p = _Bottleneck([256, 256, 1024], block='4p')\n            self.res4q = _Bottleneck([256, 256, 1024], block='4q')\n            self.res4r = _Bottleneck([256, 256, 1024], block='4r')\n            self.res4s = _Bottleneck([256, 256, 1024], block='4s')\n            self.res4t = _Bottleneck([256, 256, 1024], block='4t')\n            self.res4u = _Bottleneck([256, 256, 1024], block='4u')\n            self.res4v = _Bottleneck([256, 256, 1024], block='4v')\n            self.res4w = _Bottleneck([256, 256, 1024], block='4w') \n        \n        self.res5a = _Bottleneck([512, 512, 2048], block='5a', \n                                 downsampling=True, stride=2)\n        self.res5b = _Bottleneck([512, 512, 2048], block='5b')\n        self.res5c = _Bottleneck([512, 512, 2048], block='5c')\n        \n        \n        self.out_channel = (256, 512, 1024, 2048)\n    \n    def call(self, inputs, training=True):\n        x = self.padding(inputs)\n        x = self.conv1(x)\n        x = self.bn_conv1(x, training=training)\n        x = tf.nn.relu(x)\n        x = self.max_pool(x)\n        \n        x = self.res2a(x, training=training)\n        x = self.res2b(x, training=training)\n        C2 = x = self.res2c(x, training=training)\n        \n        x = self.res3a(x, training=training)\n        x = self.res3b(x, training=training)\n        x = self.res3c(x, training=training)\n        C3 = x = self.res3d(x, training=training)\n        \n        x = self.res4a(x, training=training)\n        x = self.res4b(x, training=training)\n        x = self.res4c(x, training=training)\n        x = self.res4d(x, training=training)\n        x = self.res4e(x, training=training)\n        x = self.res4f(x, training=training)\n        if self.depth == 101:\n            x = self.res4g(x, training=training)\n            x = self.res4h(x, training=training)\n            x = self.res4i(x, training=training)\n            x = self.res4j(x, training=training)\n            x = self.res4k(x, training=training)\n            x = self.res4l(x, training=training)\n            x = self.res4m(x, training=training)\n            x = self.res4n(x, training=training)\n            x = self.res4o(x, training=training)\n            x = self.res4p(x, training=training)\n            x = self.res4q(x, training=training)\n            x = self.res4r(x, training=training)\n            x = self.res4s(x, training=training)\n            x = self.res4t(x, training=training)\n            x = self.res4u(x, training=training)\n            x = self.res4v(x, training=training)\n            x = self.res4w(x, training=training) \n        C4 = x\n        \n        x = self.res5a(x, training=training)\n        x = self.res5b(x, training=training)\n        C5 = x = self.res5c(x, training=training)\n        \n        return (C2, C3, C4, C5)\n    \n    def compute_output_shape(self, input_shape):\n        shape = tf.TensorShape(input_shape).as_list()\n        batch, H, W, C = shape\n        \n        C2_shape = tf.TensorShape([batch, H //  4, W //  4, self.out_channel[0]])\n        C3_shape = tf.TensorShape([batch, H //  8, W //  8, self.out_channel[1]])\n        C4_shape = tf.TensorShape([batch, H // 16, W // 16, self.out_channel[2]])\n        C5_shape = tf.TensorShape([batch, H // 32, W // 32, self.out_channel[3]])\n        \n        return (C2_shape, C3_shape, C4_shape, C5_shape)"""
16-fasterRCNN/detection/models/bbox_heads/bbox_head.py,42,"b'import tensorflow as tf\nfrom    tensorflow.keras import layers\n\nfrom detection.core.bbox import transforms\nfrom detection.core.loss import losses\nfrom detection.utils.misc import *\n\nclass BBoxHead(tf.keras.Model):\n    def __init__(self, num_classes, \n                 pool_size=(7, 7),\n                 target_means=(0., 0., 0., 0.), \n                 target_stds=(0.1, 0.1, 0.2, 0.2),\n                 min_confidence=0.7,\n                 nms_threshold=0.3,\n                 max_instances=100,\n                 **kwags):\n        super(BBoxHead, self).__init__(**kwags)\n        \n        self.num_classes = num_classes\n        self.pool_size = tuple(pool_size)\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.min_confidence = min_confidence\n        self.nms_threshold = nms_threshold\n        self.max_instances = max_instances\n        \n        self.rcnn_class_loss = losses.rcnn_class_loss\n        self.rcnn_bbox_loss = losses.rcnn_bbox_loss\n        \n        self.rcnn_class_conv1 = layers.Conv2D(1024, self.pool_size, \n                                              padding=\'valid\', name=\'rcnn_class_conv1\')\n        \n        self.rcnn_class_bn1 = layers.BatchNormalization(name=\'rcnn_class_bn1\')\n        \n        self.rcnn_class_conv2 = layers.Conv2D(1024, (1, 1), \n                                              name=\'rcnn_class_conv2\')\n        \n        self.rcnn_class_bn2 = layers.BatchNormalization(name=\'rcnn_class_bn2\')\n        \n        self.rcnn_class_logits = layers.Dense(num_classes, name=\'rcnn_class_logits\')\n        \n        self.rcnn_delta_fc = layers.Dense(num_classes * 4, name=\'rcnn_bbox_fc\')\n        \n    def call(self, inputs, training=True):\n        \'\'\'\n        Args\n        ---\n            pooled_rois_list: List of [num_rois, pool_size, pool_size, channels]\n        \n        Returns\n        ---\n            rcnn_class_logits_list: List of [num_rois, num_classes]\n            rcnn_probs_list: List of [num_rois, num_classes]\n            rcnn_deltas_list: List of [num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n        \'\'\'\n        pooled_rois_list = inputs\n        num_pooled_rois_list = [pooled_rois.shape[0] for pooled_rois in pooled_rois_list]\n        pooled_rois = tf.concat(pooled_rois_list, axis=0)\n        \n        x = self.rcnn_class_conv1(pooled_rois)\n        x = self.rcnn_class_bn1(x, training=training)\n        x = tf.nn.relu(x)\n        \n        x = self.rcnn_class_conv2(x)\n        x = self.rcnn_class_bn2(x, training=training)\n        x = tf.nn.relu(x)\n        \n        x = tf.squeeze(tf.squeeze(x, 2), 1)\n        \n        logits = self.rcnn_class_logits(x)\n        probs = tf.nn.softmax(logits)\n        \n        deltas = self.rcnn_delta_fc(x)\n        deltas = tf.reshape(deltas, (-1, self.num_classes, 4))\n        \n\n        rcnn_class_logits_list = tf.split(logits, num_pooled_rois_list, 0)\n        rcnn_probs_list = tf.split(probs, num_pooled_rois_list, 0)\n        rcnn_deltas_list = tf.split(deltas, num_pooled_rois_list, 0)\n\n            \n        return rcnn_class_logits_list, rcnn_probs_list, rcnn_deltas_list\n\n    def loss(self, \n             rcnn_class_logits_list, rcnn_deltas_list, \n             rcnn_target_matchs_list, rcnn_target_deltas_list):\n        """"""\n\n        :param rcnn_class_logits_list:\n        :param rcnn_deltas_list:\n        :param rcnn_target_matchs_list:\n        :param rcnn_target_deltas_list:\n        :return:\n        """"""\n        rcnn_class_loss = self.rcnn_class_loss(\n            rcnn_target_matchs_list, rcnn_class_logits_list)\n        rcnn_bbox_loss = self.rcnn_bbox_loss(\n            rcnn_target_deltas_list, rcnn_target_matchs_list, rcnn_deltas_list)\n        \n        return rcnn_class_loss, rcnn_bbox_loss\n        \n    def get_bboxes(self, rcnn_probs_list, rcnn_deltas_list, rois_list, img_metas):\n        \'\'\'\n        Args\n        ---\n            rcnn_probs_list: List of [num_rois, num_classes]\n            rcnn_deltas_list: List of [num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n            rois_list: List of [num_rois, (y1, x1, y2, x2)]\n            img_meta_list: [batch_size, 11]\n        \n        Returns\n        ---\n            detections_list: List of [num_detections, (y1, x1, y2, x2, class_id, score)]\n                coordinates are in pixel coordinates.\n        \'\'\'\n        \n        pad_shapes = calc_pad_shapes(img_metas)\n        detections_list = [\n            self._get_bboxes_single(\n                rcnn_probs_list[i], rcnn_deltas_list[i], rois_list[i], pad_shapes[i])\n            for i in range(img_metas.shape[0])\n        ]\n        return detections_list  \n    \n    def _get_bboxes_single(self, rcnn_probs, rcnn_deltas, rois, img_shape):\n        \'\'\'\n        Args\n        ---\n            rcnn_probs: [num_rois, num_classes]\n            rcnn_deltas: [num_rois, num_classes, (dy, dx, log(dh), log(dw))]\n            rois: [num_rois, (y1, x1, y2, x2)]\n            img_shape: np.ndarray. [2]. (img_height, img_width)       \n        \'\'\'\n        H, W = img_shape   \n        # Class IDs per ROI\n        class_ids = tf.argmax(rcnn_probs, axis=1, output_type=tf.int32)\n        \n        # Class probability of the top class of each ROI\n        indices = tf.stack([tf.range(rcnn_probs.shape[0]), class_ids], axis=1)\n        class_scores = tf.gather_nd(rcnn_probs, indices)\n        # Class-specific bounding box deltas\n        deltas_specific = tf.gather_nd(rcnn_deltas, indices)\n        # Apply bounding box deltas\n        # Shape: [num_rois, (y1, x1, y2, x2)] in normalized coordinates        \n        refined_rois = transforms.delta2bbox(rois, deltas_specific, self.target_means, self.target_stds)\n        \n        # Clip boxes to image window\n        refined_rois *= tf.constant([H, W, H, W], dtype=tf.float32)\n        window = tf.constant([0., 0., H * 1., W * 1.], dtype=tf.float32)\n        refined_rois = transforms.bbox_clip(refined_rois, window)\n        \n        \n        # Filter out background boxes\n        keep = tf.where(class_ids > 0)[:, 0]\n        \n        # Filter out low confidence boxes\n        if self.min_confidence:\n            conf_keep = tf.where(class_scores >= self.min_confidence)[:, 0]\n            keep = tf.compat.v2.sets.intersection(tf.expand_dims(keep, 0),\n                                            tf.expand_dims(conf_keep, 0))\n            keep = tf.sparse.to_dense(keep)[0]\n            \n        # Apply per-class NMS\n        # 1. Prepare variables\n        pre_nms_class_ids = tf.gather(class_ids, keep)\n        pre_nms_scores = tf.gather(class_scores, keep)\n        pre_nms_rois = tf.gather(refined_rois,   keep)\n        unique_pre_nms_class_ids = tf.unique(pre_nms_class_ids)[0]\n\n        def nms_keep_map(class_id):\n            \'\'\'Apply Non-Maximum Suppression on ROIs of the given class.\'\'\'\n            # Indices of ROIs of the given class\n            ixs = tf.where(tf.equal(pre_nms_class_ids, class_id))[:, 0]\n            # Apply NMS\n            class_keep = tf.image.non_max_suppression(\n                    tf.gather(pre_nms_rois, ixs),\n                    tf.gather(pre_nms_scores, ixs),\n                    max_output_size=self.max_instances,\n                    iou_threshold=self.nms_threshold)\n            # Map indices\n            class_keep = tf.gather(keep, tf.gather(ixs, class_keep))\n            return class_keep\n\n        # 2. Map over class IDs\n        nms_keep = []\n        for i in range(unique_pre_nms_class_ids.shape[0]):\n            nms_keep.append(nms_keep_map(unique_pre_nms_class_ids[i]))\n        nms_keep = tf.concat(nms_keep, axis=0)\n        \n        # 3. Compute intersection between keep and nms_keep\n        keep = tf.compat.v2.sets.intersection(tf.expand_dims(keep, 0),\n                                        tf.expand_dims(nms_keep, 0))\n        keep = tf.sparse.to_dense(keep)[0]\n        # Keep top detections\n        roi_count = self.max_instances\n        class_scores_keep = tf.gather(class_scores, keep)\n        num_keep = tf.minimum(tf.shape(class_scores_keep)[0], roi_count)\n        top_ids = tf.nn.top_k(class_scores_keep, k=num_keep, sorted=True)[1]\n        keep = tf.gather(keep, top_ids)  \n        \n        detections = tf.concat([\n            tf.gather(refined_rois, keep),\n            tf.cast(tf.gather(class_ids, keep), tf.float32)[..., tf.newaxis],\n            tf.gather(class_scores, keep)[..., tf.newaxis]\n            ], axis=1)\n        \n        return detections\n        '"
16-fasterRCNN/detection/models/detectors/faster_rcnn.py,1,"b'import tensorflow as tf\n\nfrom detection.models.backbones import resnet\nfrom detection.models.necks import fpn\nfrom detection.models.rpn_heads import rpn_head\nfrom detection.models.bbox_heads import bbox_head\nfrom detection.models.roi_extractors import roi_align\nfrom detection.models.detectors.test_mixins import RPNTestMixin, BBoxTestMixin\n\nfrom detection.core.bbox import bbox_target\n\n\n\nclass FasterRCNN(tf.keras.Model, RPNTestMixin, BBoxTestMixin):\n\n    def __init__(self, num_classes, **kwags):\n        super(FasterRCNN, self).__init__(**kwags)\n       \n        self.NUM_CLASSES = num_classes\n        \n        # RPN configuration\n        # Anchor attributes\n        self.ANCHOR_SCALES = (32, 64, 128, 256, 512)\n        self.ANCHOR_RATIOS = (0.5, 1, 2)\n        self.ANCHOR_FEATURE_STRIDES = (4, 8, 16, 32, 64)\n        \n        # Bounding box refinement mean and standard deviation\n        self.RPN_TARGET_MEANS = (0., 0., 0., 0.)\n        self.RPN_TARGET_STDS = (0.1, 0.1, 0.2, 0.2)\n        \n        # RPN training configuration\n        self.PRN_BATCH_SIZE = 256\n        self.RPN_POS_FRAC = 0.5\n        self.RPN_POS_IOU_THR = 0.7\n        self.RPN_NEG_IOU_THR = 0.3\n\n        # ROIs kept configuration\n        self.PRN_PROPOSAL_COUNT = 2000\n        self.PRN_NMS_THRESHOLD = 0.7\n        \n        # RCNN configuration\n        # Bounding box refinement mean and standard deviation\n        self.RCNN_TARGET_MEANS = (0., 0., 0., 0.)\n        self.RCNN_TARGET_STDS = (0.1, 0.1, 0.2, 0.2)\n        \n        # ROI Feat Size\n        self.POOL_SIZE = (7, 7)\n        \n        # RCNN training configuration\n        self.RCNN_BATCH_SIZE = 256\n        self.RCNN_POS_FRAC = 0.25\n        self.RCNN_POS_IOU_THR = 0.5\n        self.RCNN_NEG_IOU_THR = 0.5\n        \n        # Boxes kept configuration\n        self.RCNN_MIN_CONFIDENCE = 0.7\n        self.RCNN_NME_THRESHOLD = 0.3\n        self.RCNN_MAX_INSTANCES = 100\n        \n        # Target Generator for the second stage.\n        self.bbox_target = bbox_target.ProposalTarget(\n            target_means=self.RCNN_TARGET_MEANS,\n            target_stds=self.RPN_TARGET_STDS, \n            num_rcnn_deltas=self.RCNN_BATCH_SIZE,\n            positive_fraction=self.RCNN_POS_FRAC,\n            pos_iou_thr=self.RCNN_POS_IOU_THR,\n            neg_iou_thr=self.RCNN_NEG_IOU_THR)\n                \n        # Modules\n        self.backbone = resnet.ResNet(\n            depth=101, \n            name=\'res_net\')\n        \n        self.neck = fpn.FPN(\n            name=\'fpn\')\n        \n        self.rpn_head = rpn_head.RPNHead(\n            anchor_scales=self.ANCHOR_SCALES,\n            anchor_ratios=self.ANCHOR_RATIOS,\n            anchor_feature_strides=self.ANCHOR_FEATURE_STRIDES,\n            proposal_count=self.PRN_PROPOSAL_COUNT,\n            nms_threshold=self.PRN_NMS_THRESHOLD,\n            target_means=self.RPN_TARGET_MEANS,\n            target_stds=self.RPN_TARGET_STDS,\n            num_rpn_deltas=self.PRN_BATCH_SIZE,\n            positive_fraction=self.RPN_POS_FRAC,\n            pos_iou_thr=self.RPN_POS_IOU_THR,\n            neg_iou_thr=self.RPN_NEG_IOU_THR,\n            name=\'rpn_head\')\n        \n        self.roi_align = roi_align.PyramidROIAlign(\n            pool_shape=self.POOL_SIZE,\n            name=\'pyramid_roi_align\')\n        \n        self.bbox_head = bbox_head.BBoxHead(\n            num_classes=self.NUM_CLASSES,\n            pool_size=self.POOL_SIZE,\n            target_means=self.RCNN_TARGET_MEANS,\n            target_stds=self.RCNN_TARGET_STDS,\n            min_confidence=self.RCNN_MIN_CONFIDENCE,\n            nms_threshold=self.RCNN_NME_THRESHOLD,\n            max_instances=self.RCNN_MAX_INSTANCES,\n            name=\'b_box_head\')\n\n    def call(self, inputs, training=True):\n        """"""\n\n        :param inputs: [1, 1216, 1216, 3], [1, 11], [1, 14, 4], [1, 14]\n        :param training:\n        :return:\n        """"""\n        if training: # training\n            imgs, img_metas, gt_boxes, gt_class_ids = inputs\n        else: # inference\n            imgs, img_metas = inputs\n        # [1, 304, 304, 256] => [1, 152, 152, 512]=>[1,76,76,1024]=>[1,38,38,2048]\n        C2, C3, C4, C5 = self.backbone(imgs, \n                                       training=training)\n        # [1, 304, 304, 256] <= [1, 152, 152, 256]<=[1,76,76,256]<=[1,38,38,256]=>[1,19,19,256]\n        P2, P3, P4, P5, P6 = self.neck([C2, C3, C4, C5], \n                                       training=training)\n        \n        rpn_feature_maps = [P2, P3, P4, P5, P6]\n        rcnn_feature_maps = [P2, P3, P4, P5]\n        # [1, 369303, 2] [1, 369303, 2], [1, 369303, 4], includes all anchors on pyramid level of features\n        rpn_class_logits, rpn_probs, rpn_deltas = self.rpn_head(\n            rpn_feature_maps, training=training)\n        # [369303, 4] => [215169, 4], valid => [6000, 4], performance =>[2000, 4],  NMS\n        proposals_list = self.rpn_head.get_proposals(\n            rpn_probs, rpn_deltas, img_metas)\n        \n        if training: # get target value for these proposal target label and target delta\n            rois_list, rcnn_target_matchs_list, rcnn_target_deltas_list = \\\n                self.bbox_target.build_targets(\n                    proposals_list, gt_boxes, gt_class_ids, img_metas)\n        else:\n            rois_list = proposals_list\n        # rois_list only contains coordinates, rcnn_feature_maps save the 5 features data=>[192,7,7,256]\n        pooled_regions_list = self.roi_align(#\n            (rois_list, rcnn_feature_maps, img_metas), training=training)\n        # [192, 81], [192, 81], [192, 81, 4]\n        rcnn_class_logits_list, rcnn_probs_list, rcnn_deltas_list = \\\n            self.bbox_head(pooled_regions_list, training=training)\n\n        if training:         \n            rpn_class_loss, rpn_bbox_loss = self.rpn_head.loss(\n                rpn_class_logits, rpn_deltas, gt_boxes, gt_class_ids, img_metas)\n            \n            rcnn_class_loss, rcnn_bbox_loss = self.bbox_head.loss(\n                rcnn_class_logits_list, rcnn_deltas_list, \n                rcnn_target_matchs_list, rcnn_target_deltas_list)\n            \n            return [rpn_class_loss, rpn_bbox_loss, \n                    rcnn_class_loss, rcnn_bbox_loss]\n        else:\n            detections_list = self.bbox_head.get_bboxes(\n                rcnn_probs_list, rcnn_deltas_list, rois_list, img_metas)\n        \n            return detections_list\n'"
16-fasterRCNN/detection/models/detectors/test_mixins.py,8,"b""import numpy as np\nimport tensorflow as tf\n\nfrom detection.core.bbox import transforms\nfrom detection.utils.misc import *\n\nclass RPNTestMixin:\n    \n    def simple_test_rpn(self, img, img_meta):\n        '''\n        Args\n        ---\n            imgs: np.ndarray. [height, width, channel]\n            img_metas: np.ndarray. [11]\n        \n        '''\n        imgs = tf.Variable(np.expand_dims(img, 0))\n        img_metas = tf.Variable(np.expand_dims(img_meta, 0))\n\n        x = self.backbone(imgs, training=False)\n        x = self.neck(x, training=False)\n        \n        rpn_class_logits, rpn_probs, rpn_deltas = self.rpn_head(x, training=False)\n        \n        proposals_list = self.rpn_head.get_proposals(\n            rpn_probs, rpn_deltas, img_metas, with_probs=False)\n\n        return proposals_list[0]\n    \nclass BBoxTestMixin(object):\n    \n    def _unmold_detections(self, detections_list, img_metas):\n        return [\n            self._unmold_single_detection(detections_list[i], img_metas[i])\n            for i in range(img_metas.shape[0])\n        ]\n\n    def _unmold_single_detection(self, detections, img_meta):\n        zero_ix = tf.where(tf.not_equal(detections[:, 4], 0))\n        detections = tf.gather_nd(detections, zero_ix)\n\n        # Extract boxes, class_ids, scores, and class-specific masks\n        boxes = detections[:, :4]\n        class_ids = tf.cast(detections[:, 4], tf.int32)\n        scores = detections[:, 5]\n\n        boxes = transforms.bbox_mapping_back(boxes, img_meta)\n\n        return {'rois': boxes.numpy(),\n                'class_ids': class_ids.numpy(),\n                'scores': scores.numpy()}\n\n    def simple_test_bboxes(self, img, img_meta, proposals):\n        '''\n        Args\n        ---\n            imgs: np.ndarray. [height, width, channel]\n            img_meta: np.ndarray. [11]\n        \n        '''\n        imgs = tf.Variable(np.expand_dims(img, 0))\n        img_metas = tf.Variable(np.expand_dims(img_meta, 0))\n        rois_list = [tf.Variable(proposals)]\n        \n        x = self.backbone(imgs, training=False)\n        P2, P3, P4, P5, _ = self.neck(x, training=False)\n        \n        rcnn_feature_maps = [P2, P3, P4, P5]\n        \n        \n        pooled_regions_list = self.roi_align(\n            (rois_list, rcnn_feature_maps, img_metas), training=False)\n\n        rcnn_class_logits_list, rcnn_probs_list, rcnn_deltas_list = \\\n            self.bbox_head(pooled_regions_list, training=False)\n        \n        detections_list = self.bbox_head.get_bboxes(\n            rcnn_probs_list, rcnn_deltas_list, rois_list, img_metas)\n        \n        return self._unmold_detections(detections_list, img_metas)[0]"""
16-fasterRCNN/detection/models/necks/fpn.py,10,"b""'''\nFRN model for Keras.\n\n# Reference:\n- [Feature Pyramid Networks for Object Detection](\n    https://arxiv.org/abs/1612.03144)\n\n'''\nimport  tensorflow as tf\nfrom    tensorflow.keras import layers\n\nclass FPN(tf.keras.Model):\n\n    def __init__(self, out_channels=256, **kwargs):\n        '''\n        Feature Pyramid Networks\n        \n        Attributes\n        ---\n            out_channels: int. the channels of pyramid feature maps.\n        '''\n        super(FPN, self).__init__(**kwargs)\n        \n        self.out_channels = out_channels\n        \n        self.fpn_c2p2 = layers.Conv2D(out_channels, (1, 1), \n                                      kernel_initializer='he_normal', name='fpn_c2p2')\n        self.fpn_c3p3 = layers.Conv2D(out_channels, (1, 1), \n                                      kernel_initializer='he_normal', name='fpn_c3p3')\n        self.fpn_c4p4 = layers.Conv2D(out_channels, (1, 1), \n                                      kernel_initializer='he_normal', name='fpn_c4p4')\n        self.fpn_c5p5 = layers.Conv2D(out_channels, (1, 1), \n                                      kernel_initializer='he_normal', name='fpn_c5p5')\n        \n        self.fpn_p3upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p3upsampled')\n        self.fpn_p4upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p4upsampled')\n        self.fpn_p5upsampled = layers.UpSampling2D(size=(2, 2), name='fpn_p5upsampled')\n        \n        \n        self.fpn_p2 = layers.Conv2D(out_channels, (3, 3), padding='SAME', \n                                    kernel_initializer='he_normal', name='fpn_p2')\n        self.fpn_p3 = layers.Conv2D(out_channels, (3, 3), padding='SAME', \n                                    kernel_initializer='he_normal', name='fpn_p3')\n        self.fpn_p4 = layers.Conv2D(out_channels, (3, 3), padding='SAME', \n                                    kernel_initializer='he_normal', name='fpn_p4')\n        self.fpn_p5 = layers.Conv2D(out_channels, (3, 3), padding='SAME', \n                                    kernel_initializer='he_normal', name='fpn_p5')\n        \n        self.fpn_p6 = layers.MaxPooling2D(pool_size=(1, 1), strides=2, name='fpn_p6')\n        \n            \n    def call(self, inputs, training=True):\n        C2, C3, C4, C5 = inputs\n        \n        P5 = self.fpn_c5p5(C5)\n        P4 = self.fpn_c4p4(C4) + self.fpn_p5upsampled(P5)\n        P3 = self.fpn_c3p3(C3) + self.fpn_p4upsampled(P4)\n        P2 = self.fpn_c2p2(C2) + self.fpn_p3upsampled(P3)\n        \n        # Attach 3x3 conv to all P layers to get the final feature maps.\n        P2 = self.fpn_p2(P2)\n        P3 = self.fpn_p3(P3)\n        P4 = self.fpn_p4(P4)\n        P5 = self.fpn_p5(P5)\n        \n        # subsampling from P5 with stride of 2.\n        P6 = self.fpn_p6(P5)\n        \n        return [P2, P3, P4, P5, P6]\n        \n    def compute_output_shape(self, input_shape):\n        C2_shape, C3_shape, C4_shape, C5_shape = input_shape\n        \n        C2_shape, C3_shape, C4_shape, C5_shape = \\\n            C2_shape.as_list(), C3_shape.as_list(), C4_shape.as_list(), C5_shape.as_list()\n        \n        C6_shape = [C5_shape[0], (C5_shape[1] + 1) // 2, (C5_shape[2] + 1) // 2, self.out_channels]\n        \n        C2_shape[-1] = self.out_channels\n        C3_shape[-1] = self.out_channels\n        C4_shape[-1] = self.out_channels\n        C5_shape[-1] = self.out_channels\n        \n        return [tf.TensorShape(C2_shape),\n                tf.TensorShape(C3_shape),\n                tf.TensorShape(C4_shape),\n                tf.TensorShape(C5_shape),\n                tf.TensorShape(C6_shape)]\n\nif __name__ == '__main__':\n    \n    C2 = tf.random.normal((2, 256, 256,  256))\n    C3 = tf.random.normal((2, 128, 128,  512))\n    C4 = tf.random.normal((2,  64,  64, 1024))\n    C5 = tf.random.normal((2,  32,  32, 2048))\n    \n    fpn = FPN()\n    \n    P2, P3, P4, P5, P6 = fpn([C2, C3, C4, C5])\n    \n    print('P2 shape:', P2.shape.as_list())\n    print('P3 shape:', P3.shape.as_list())\n    print('P4 shape:', P4.shape.as_list())\n    print('P5 shape:', P5.shape.as_list())\n    print('P6 shape:', P6.shape.as_list())"""
16-fasterRCNN/detection/models/roi_extractors/roi_align.py,29,"b'import tensorflow as tf\n\nfrom detection.utils.misc import *\n\nclass PyramidROIAlign(tf.keras.layers.Layer):\n\n    def __init__(self, pool_shape, **kwargs):\n        \'\'\'\n        Implements ROI Pooling on multiple levels of the feature pyramid.\n\n        Attributes\n        ---\n            pool_shape: (height, width) of the output pooled regions.\n                Example: (7, 7)\n        \'\'\'\n        super(PyramidROIAlign, self).__init__(**kwargs)\n\n        self.pool_shape = tuple(pool_shape)\n\n    def call(self, inputs, training=True):\n        \'\'\'\n        Args\n        ---\n            rois_list: list of [num_rois, (y1, x1, y2, x2)] in normalized coordinates.\n            feature_map_list: List of [batch, height, width, channels].\n                feature maps from different levels of the pyramid.\n            img_metas: [batch_size, 11]\n\n        Returns\n        ---\n            pooled_rois_list: list of [num_rois, pooled_height, pooled_width, channels].\n                The width and height are those specific in the pool_shape in the layer\n                constructor.\n        \'\'\'\n        rois_list, feature_map_list, img_metas = inputs # [2000 ,4], list:[P2, P3, P4, P5]\n\n        pad_shapes = calc_pad_shapes(img_metas)\n        \n        pad_areas = pad_shapes[:, 0] * pad_shapes[:, 1] # 1216*1216\n        \n        num_rois_list = [rois.shape.as_list()[0] for rois in rois_list] # data:[2000]\n        roi_indices = tf.constant(\n            [i for i in range(len(rois_list)) for _ in range(rois_list[i].shape.as_list()[0])],\n            dtype=tf.int32\n        ) #[0.....], shape:[2000]\n        \n        areas = tf.constant(#              range(1)                               range(2000)\n            [pad_areas[i] for i in range(pad_areas.shape[0]) for _ in range(num_rois_list[i])],\n            dtype=tf.float32\n        )#[1216*1216, 1216*1216,...], shape:[2000]\n\n\n        rois = tf.concat(rois_list, axis=0) # [2000, 4]\n        \n        # Assign each ROI to a level in the pyramid based on the ROI area.\n        y1, x1, y2, x2 = tf.split(rois, 4, axis=1) # 4 of [2000, 1]\n        h = y2 - y1 # [2000, 1]\n        w = x2 - x1 # [2000, 1]\n        \n        # Equation 1 in the Feature Pyramid Networks paper. Account for\n        # the fact that our coordinates are normalized here.\n        # e.g. a 224x224 ROI (in pixels) maps to P4\n\n        roi_level = tf.math.log( # [2000]\n                    tf.sqrt(tf.squeeze(h * w, 1))\n                    / tf.cast((224.0 / tf.sqrt(areas * 1.0)), tf.float32)\n                    ) / tf.math.log(2.0)\n        roi_level = tf.minimum(5, tf.maximum( # [2000], clamp to [2-5]\n            2, 4 + tf.cast(tf.round(roi_level), tf.int32)))\n        # roi_level will indicates which level of feature to use\n\n        \n        # Loop through levels and apply ROI pooling to each. P2 to P5.\n        pooled_rois = []\n        roi_to_level = []\n        for i, level in enumerate(range(2, 6)): # 2,3,4,5\n            ix = tf.where(tf.equal(roi_level, level)) # [1999, 1], means 1999 of 2000 select P2\n            level_rois = tf.gather_nd(rois, ix) # boxes to crop, [1999, 4]\n\n            # ROI indices for crop_and_resize.\n            level_roi_indices = tf.gather_nd(roi_indices, ix) # [19999], data:[0....0]\n\n            # Keep track of which roi is mapped to which level\n            roi_to_level.append(ix)\n\n            # Stop gradient propogation to ROI proposals\n            level_rois = tf.stop_gradient(level_rois)\n            level_roi_indices = tf.stop_gradient(level_roi_indices)\n\n            # Crop and Resize\n            # From Mask R-CNN paper: ""We sample four regular locations, so\n            # that we can evaluate either max or average pooling. In fact,\n            # interpolating only a single value at each bin center (without\n            # pooling) is nearly as effective.""\n            #\n            # Here we use the simplified approach of a single value per bin,\n            # which is how it\'s done in tf.crop_and_resize()\n            # Result: [batch * num_rois, pool_height, pool_width, channels]\n            pooled_rois.append(tf.image.crop_and_resize(\n                feature_map_list[i], level_rois, level_roi_indices, self.pool_shape,\n                method=""bilinear"")) # [1, 304, 304, 256], [1999, 4], [1999], [2]=[7,7]=>[1999,7,7,256]\n        # [1999, 7, 7, 256], [], [], [1,7,7,256] => [2000, 7, 7, 256]\n        # Pack pooled features into one tensor\n        pooled_rois = tf.concat(pooled_rois, axis=0)\n\n        # Pack roi_to_level mapping into one array and add another\n        # column representing the order of pooled rois\n        roi_to_level = tf.concat(roi_to_level, axis=0) # [2000, 1], 1999 of P2, and 1 other P\n        roi_range = tf.expand_dims(tf.range(tf.shape(roi_to_level)[0]), 1) # [2000, 1], 0~1999\n        roi_to_level = tf.concat([tf.cast(roi_to_level, tf.int32), roi_range],\n                                 axis=1) # [2000, 2], (P, range)\n\n        # Rearrange pooled features to match the order of the original rois\n        # Sort roi_to_level by batch then roi indextf.Tensor([        0    100001    200002 ... 199801997 199901998  20101999], shape=(2000,), dtype=int32)\n        # TF doesn\'t have a way to sort by two columns, so merge them and sort.\n        sorting_tensor = roi_to_level[:, 0] * 100000 + roi_to_level[:, 1]\n        ix = tf.nn.top_k(sorting_tensor, k=tf.shape( # k=2000\n            roi_to_level)[0]).indices[::-1]# reverse the order\n        ix = tf.gather(roi_to_level[:, 1], ix) # [2000]\n        pooled_rois = tf.gather(pooled_rois, ix) # [2000, 7, 7, 256]\n        # 2000 of [7, 7, 256]\n        pooled_rois_list = tf.split(pooled_rois, num_rois_list, axis=0)\n        return pooled_rois_list\n'"
16-fasterRCNN/detection/models/rpn_heads/rpn_head.py,20,"b'import  tensorflow as tf\nfrom    tensorflow.keras import layers\n\nfrom detection.core.bbox import transforms\nfrom detection.utils.misc import *\n\nfrom detection.core.anchor import anchor_generator, anchor_target\nfrom detection.core.loss import losses\n\nclass RPNHead(tf.keras.Model):\n\n    def __init__(self, \n                 anchor_scales=(32, 64, 128, 256, 512), \n                 anchor_ratios=(0.5, 1, 2), \n                 anchor_feature_strides=(4, 8, 16, 32, 64),\n                 proposal_count=2000, \n                 nms_threshold=0.7, \n                 target_means=(0., 0., 0., 0.), \n                 target_stds=(0.1, 0.1, 0.2, 0.2), \n                 num_rpn_deltas=256,\n                 positive_fraction=0.5,\n                 pos_iou_thr=0.7,\n                 neg_iou_thr=0.3,\n                 **kwags):\n        \'\'\'\n        Network head of Region Proposal Network.\n\n                                      / - rpn_cls (1x1 conv)\n        input - rpn_conv (3x3 conv) -\n                                      \\ - rpn_reg (1x1 conv)\n\n        Attributes\n        ---\n            anchor_scales: 1D array of anchor sizes in pixels.\n            anchor_ratios: 1D array of anchor ratios of width/height.\n            anchor_feature_strides: Stride of the feature map relative \n                to the image in pixels.\n            proposal_count: int. RPN proposals kept after non-maximum \n                suppression.\n            nms_threshold: float. Non-maximum suppression threshold to \n                filter RPN proposals.\n            target_means: [4] Bounding box refinement mean.\n            target_stds: [4] Bounding box refinement standard deviation.\n            num_rpn_deltas: int.\n            positive_fraction: float.\n            pos_iou_thr: float.\n            neg_iou_thr: float.\n        \'\'\'\n        super(RPNHead, self).__init__(**kwags)\n        \n        self.proposal_count = proposal_count\n        self.nms_threshold = nms_threshold\n        self.target_means = target_means\n        self.target_stds = target_stds\n\n        self.generator = anchor_generator.AnchorGenerator(\n            scales=anchor_scales, \n            ratios=anchor_ratios, \n            feature_strides=anchor_feature_strides)\n        \n        self.anchor_target = anchor_target.AnchorTarget(\n            target_means=target_means, \n            target_stds=target_stds,\n            num_rpn_deltas=num_rpn_deltas,\n            positive_fraction=positive_fraction,\n            pos_iou_thr=pos_iou_thr,\n            neg_iou_thr=neg_iou_thr)\n        \n        self.rpn_class_loss = losses.rpn_class_loss\n        self.rpn_bbox_loss = losses.rpn_bbox_loss\n        \n        \n        # Shared convolutional base of the RPN\n        self.rpn_conv_shared = layers.Conv2D(512, (3, 3), padding=\'same\',\n                                             kernel_initializer=\'he_normal\', \n                                             name=\'rpn_conv_shared\')\n        \n        self.rpn_class_raw = layers.Conv2D(len(anchor_ratios) * 2, (1, 1),\n                                           kernel_initializer=\'he_normal\', \n                                           name=\'rpn_class_raw\')\n\n        self.rpn_delta_pred = layers.Conv2D(len(anchor_ratios) * 4, (1, 1),\n                                           kernel_initializer=\'he_normal\', \n                                           name=\'rpn_bbox_pred\')\n        \n    def call(self, inputs, training=True):\n        \'\'\'\n        Args\n        ---\n            inputs: [batch_size, feat_map_height, feat_map_width, channels] \n                one level of pyramid feat-maps.\n        \n        Returns\n        ---\n            rpn_class_logits: [batch_size, num_anchors, 2]\n            rpn_probs: [batch_size, num_anchors, 2]\n            rpn_deltas: [batch_size, num_anchors, 4]\n        \'\'\'\n        \n        layer_outputs = []\n        \n        for feat in inputs: # for every anchors feature maps\n            """"""\n            (1, 304, 304, 256)\n            (1, 152, 152, 256)\n            (1, 76, 76, 256)\n            (1, 38, 38, 256)\n            (1, 19, 19, 256)\n            rpn_class_raw: (1, 304, 304, 6)\n            rpn_class_logits: (1, 277248, 2)\n            rpn_delta_pred: (1, 304, 304, 12)\n            rpn_deltas: (1, 277248, 4)\n            rpn_class_raw: (1, 152, 152, 6)\n            rpn_class_logits: (1, 69312, 2)\n            rpn_delta_pred: (1, 152, 152, 12)\n            rpn_deltas: (1, 69312, 4)\n            rpn_class_raw: (1, 76, 76, 6)\n            rpn_class_logits: (1, 17328, 2)\n            rpn_delta_pred: (1, 76, 76, 12)\n            rpn_deltas: (1, 17328, 4)\n            rpn_class_raw: (1, 38, 38, 6)\n            rpn_class_logits: (1, 4332, 2)\n            rpn_delta_pred: (1, 38, 38, 12)\n            rpn_deltas: (1, 4332, 4)\n            rpn_class_raw: (1, 19, 19, 6)\n            rpn_class_logits: (1, 1083, 2)\n            rpn_delta_pred: (1, 19, 19, 12)\n            rpn_deltas: (1, 1083, 4)\n\n            """"""\n            # print(feat.shape)\n            shared = self.rpn_conv_shared(feat)\n            shared = tf.nn.relu(shared)\n\n            x = self.rpn_class_raw(shared)\n            # print(\'rpn_class_raw:\', x.shape)\n            rpn_class_logits = tf.reshape(x, [tf.shape(x)[0], -1, 2])\n            rpn_probs = tf.nn.softmax(rpn_class_logits)\n            # print(\'rpn_class_logits:\', rpn_class_logits.shape)\n\n            x = self.rpn_delta_pred(shared)\n            # print(\'rpn_delta_pred:\', x.shape)\n            rpn_deltas = tf.reshape(x, [tf.shape(x)[0], -1, 4])\n            # print(\'rpn_deltas:\', rpn_deltas.shape)\n            \n            layer_outputs.append([rpn_class_logits, rpn_probs, rpn_deltas])\n            # print(rpn_class_logits.shape, rpn_probs.shape, rpn_deltas.shape)\n            """"""\n            (1, 277248, 2) (1, 277248, 2) (1, 277248, 4)\n            (1, 69312, 2) (1, 69312, 2) (1, 69312, 4)\n            (1, 17328, 2) (1, 17328, 2) (1, 17328, 4)\n            (1, 4332, 2) (1, 4332, 2) (1, 4332, 4)\n            (1, 1083, 2) (1, 1083, 2) (1, 1083, 4)\n\n            """"""\n\n\n        outputs = list(zip(*layer_outputs))\n        outputs = [tf.concat(list(o), axis=1) for o in outputs]\n        rpn_class_logits, rpn_probs, rpn_deltas = outputs\n        # (1, 369303, 2) (1, 369303, 2) (1, 369303, 4)\n        # print(rpn_class_logits.shape, rpn_probs.shape, rpn_deltas.shape)\n        \n        return rpn_class_logits, rpn_probs, rpn_deltas\n\n    def loss(self, rpn_class_logits, rpn_deltas, gt_boxes, gt_class_ids, img_metas):\n        """"""\n\n        :param rpn_class_logits: [N, 2]\n        :param rpn_deltas: [N, 4]\n        :param gt_boxes:  [GT_N]\n        :param gt_class_ids:  [GT_N]\n        :param img_metas: [11]\n        :return:\n        """"""\n        # valid_flags indicates anchors located in padded area or not.\n        anchors, valid_flags = self.generator.generate_pyramid_anchors(img_metas)\n\n        #\n        rpn_target_matchs, rpn_target_deltas = self.anchor_target.build_targets(\n            anchors, valid_flags, gt_boxes, gt_class_ids)\n        \n        rpn_class_loss = self.rpn_class_loss(\n            rpn_target_matchs, rpn_class_logits)\n        rpn_bbox_loss = self.rpn_bbox_loss(\n            rpn_target_deltas, rpn_target_matchs, rpn_deltas)\n        \n        return rpn_class_loss, rpn_bbox_loss\n    \n    def get_proposals(self, \n                      rpn_probs, \n                      rpn_deltas, \n                      img_metas, \n                      with_probs=False):\n        \'\'\'\n        Calculate proposals.\n        \n        Args\n        ---\n            rpn_probs: [batch_size, num_anchors, (bg prob, fg prob)]\n            rpn_deltas: [batch_size, num_anchors, (dy, dx, log(dh), log(dw))]\n            img_metas: [batch_size, 11]\n            with_probs: bool.\n        \n        Returns\n        ---\n            proposals_list: list of [num_proposals, (y1, x1, y2, x2)] in \n                normalized coordinates if with_probs is False. \n                Otherwise, the shape of proposals in proposals_list is \n                [num_proposals, (y1, x1, y2, x2, score)]\n        \n        Note that num_proposals is no more than proposal_count. And different \n           images in one batch may have different num_proposals.\n        \'\'\'\n        anchors, valid_flags = self.generator.generate_pyramid_anchors(img_metas)\n        # [369303, 4], [b, 11]\n        # [b, N, (background prob, foreground prob)], get anchor\'s foreground prob, [1, 369303]\n        rpn_probs = rpn_probs[:, :, 1]\n        # [[1216, 1216]]\n        pad_shapes = calc_pad_shapes(img_metas)\n        \n        proposals_list = [\n            self._get_proposals_single(\n                rpn_probs[i], rpn_deltas[i], anchors, valid_flags[i], pad_shapes[i], with_probs)\n            for i in range(img_metas.shape[0])\n        ]\n        \n        return proposals_list\n    \n    def _get_proposals_single(self, \n                              rpn_probs, \n                              rpn_deltas, \n                              anchors, \n                              valid_flags, \n                              img_shape, \n                              with_probs):\n        \'\'\'\n        Calculate proposals.\n        \n        Args\n        ---\n            rpn_probs: [num_anchors]\n            rpn_deltas: [num_anchors, (dy, dx, log(dh), log(dw))]\n            anchors: [num_anchors, (y1, x1, y2, x2)] anchors defined in \n                pixel coordinates.\n            valid_flags: [num_anchors]\n            img_shape: np.ndarray. [2]. (img_height, img_width)\n            with_probs: bool.\n        \n        Returns\n        ---\n            proposals: [num_proposals, (y1, x1, y2, x2)] in normalized \n                coordinates.\n        \'\'\'\n        \n        H, W = img_shape\n        \n        # filter invalid anchors, int => bool\n        valid_flags = tf.cast(valid_flags, tf.bool)\n        # [369303] => [215169], respectively\n        rpn_probs = tf.boolean_mask(rpn_probs, valid_flags)\n        rpn_deltas = tf.boolean_mask(rpn_deltas, valid_flags)\n        anchors = tf.boolean_mask(anchors, valid_flags)\n\n        # Improve performance\n        pre_nms_limit = min(6000, anchors.shape[0]) # min(6000, 215169) => 6000\n        ix = tf.nn.top_k(rpn_probs, pre_nms_limit, sorted=True).indices\n        # [215169] => [6000], respectively\n        rpn_probs = tf.gather(rpn_probs, ix)\n        rpn_deltas = tf.gather(rpn_deltas, ix)\n        anchors = tf.gather(anchors, ix)\n        \n        # Get refined anchors, => [6000, 4]\n        proposals = transforms.delta2bbox(anchors, rpn_deltas, \n                                          self.target_means, self.target_stds)\n        # clipping to valid area, [6000, 4]\n        window = tf.constant([0., 0., H, W], dtype=tf.float32)\n        proposals = transforms.bbox_clip(proposals, window)\n        \n        # Normalize, (y1, x1, y2, x2)\n        proposals = proposals / tf.constant([H, W, H, W], dtype=tf.float32)\n        \n        # NMS, indices: [2000]\n        indices = tf.image.non_max_suppression(\n            proposals, rpn_probs, self.proposal_count, self.nms_threshold)\n        proposals = tf.gather(proposals, indices) # [2000, 4]\n        \n        if with_probs:\n            proposal_probs = tf.expand_dims(tf.gather(rpn_probs, indices), axis=1)\n            proposals = tf.concat([proposals, proposal_probs], axis=1)\n   \n        return proposals\n        \n        '"
