file_path,api_count,code
a2c.py,9,"b'import gym\nimport logging\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.layers as kl\nimport tensorflow.keras.losses as kls\nimport tensorflow.keras.optimizers as ko\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'-b\', \'--batch_size\', type=int, default=64)\nparser.add_argument(\'-n\', \'--num_updates\', type=int, default=250)\nparser.add_argument(\'-lr\', \'--learning_rate\', type=float, default=7e-3)\nparser.add_argument(\'-r\', \'--render_test\', action=\'store_true\', default=False)\nparser.add_argument(\'-p\', \'--plot_results\', action=\'store_true\', default=False)\n\n\nclass ProbabilityDistribution(tf.keras.Model):\n  def call(self, logits, **kwargs):\n    # Sample a random categorical action from the given logits.\n    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n\n\nclass Model(tf.keras.Model):\n  def __init__(self, num_actions):\n    super().__init__(\'mlp_policy\')\n    # Note: no tf.get_variable(), just simple Keras API!\n    self.hidden1 = kl.Dense(128, activation=\'relu\')\n    self.hidden2 = kl.Dense(128, activation=\'relu\')\n    self.value = kl.Dense(1, name=\'value\')\n    # Logits are unnormalized log probabilities.\n    self.logits = kl.Dense(num_actions, name=\'policy_logits\')\n    self.dist = ProbabilityDistribution()\n\n  def call(self, inputs, **kwargs):\n    # Inputs is a numpy array, convert to a tensor.\n    x = tf.convert_to_tensor(inputs)\n    # Separate hidden layers from the same input tensor.\n    hidden_logs = self.hidden1(x)\n    hidden_vals = self.hidden2(x)\n    return self.logits(hidden_logs), self.value(hidden_vals)\n\n  def action_value(self, obs):\n    # Executes `call()` under the hood.\n    logits, value = self.predict_on_batch(obs)\n    action = self.dist.predict_on_batch(logits)\n    # Another way to sample actions:\n    #   action = tf.random.categorical(logits, 1)\n    # Will become clearer later why we don\'t use it.\n    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)\n\n\nclass A2CAgent:\n  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n    # `gamma` is the discount factor; coefficients are used for the loss terms.\n    self.gamma = gamma\n    self.value_c = value_c\n    self.entropy_c = entropy_c\n\n    self.model = model\n    self.model.compile(\n      optimizer=ko.RMSprop(lr=lr),\n      # Define separate losses for policy logits and value estimate.\n      loss=[self._logits_loss, self._value_loss])\n\n  def train(self, env, batch_sz=64, updates=250):\n    # Storage helpers for a single batch of data.\n    actions = np.empty((batch_sz,), dtype=np.int32)\n    rewards, dones, values = np.empty((3, batch_sz))\n    observations = np.empty((batch_sz,) + env.observation_space.shape)\n    # Training loop: collect samples, send to optimizer, repeat updates times.\n    ep_rewards = [0.0]\n    next_obs = env.reset()\n    for update in range(updates):\n      for step in range(batch_sz):\n        observations[step] = next_obs.copy()\n        actions[step], values[step] = self.model.action_value(next_obs[None, :])\n        next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n\n        ep_rewards[-1] += rewards[step]\n        if dones[step]:\n          ep_rewards.append(0.0)\n          next_obs = env.reset()\n          logging.info(""Episode: %03d, Reward: %03d"" % (len(ep_rewards) - 1, ep_rewards[-2]))\n\n      _, next_value = self.model.action_value(next_obs[None, :])\n      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n      # A trick to input actions and advantages through same API.\n      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n      # Performs a full training step on the collected batch.\n      # Note: no need to mess around with gradients, Keras API handles it.\n      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n      logging.debug(""[%d/%d] Losses: %s"" % (update + 1, updates, losses))\n\n    return ep_rewards\n\n  def test(self, env, render=False):\n    obs, done, ep_reward = env.reset(), False, 0\n    while not done:\n      action, _ = self.model.action_value(obs[None, :])\n      obs, reward, done, _ = env.step(action)\n      ep_reward += reward\n      if render:\n        env.render()\n    return ep_reward\n\n  def _returns_advantages(self, rewards, dones, values, next_value):\n    # `next_value` is the bootstrap value estimate of the future state (critic).\n    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n    # Returns are calculated as discounted sum of future rewards.\n    for t in reversed(range(rewards.shape[0])):\n      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n    returns = returns[:-1]\n    # Advantages are equal to returns - baseline (value estimates in our case).\n    advantages = returns - values\n    return returns, advantages\n\n  def _value_loss(self, returns, value):\n    # Value loss is typically MSE between value estimates and returns.\n    return self.value_c * kls.mean_squared_error(returns, value)\n\n  def _logits_loss(self, actions_and_advantages, logits):\n    # A trick to input actions and advantages through the same API.\n    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n    # `from_logits` argument ensures transformation into normalized probabilities.\n    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n    # Policy loss is defined by policy gradients, weighted by advantages.\n    # Note: we only calculate the loss on the actions we\'ve actually taken.\n    actions = tf.cast(actions, tf.int32)\n    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n    # Entropy loss can be calculated as cross-entropy over itself.\n    probs = tf.nn.softmax(logits)\n    entropy_loss = kls.categorical_crossentropy(probs, probs)\n    # We want to minimize policy and maximize entropy losses.\n    # Here signs are flipped because the optimizer minimizes.\n    return policy_loss - self.entropy_c * entropy_loss\n\n\nif __name__ == \'__main__\':\n  args = parser.parse_args()\n  logging.getLogger().setLevel(logging.INFO)\n\n  env = gym.make(\'CartPole-v0\')\n  model = Model(num_actions=env.action_space.n)\n  agent = A2CAgent(model, args.learning_rate)\n\n  rewards_history = agent.train(env, args.batch_size, args.num_updates)\n  print(""Finished training. Testing..."")\n  print(""Total Episode Reward: %d out of 200"" % agent.test(env, args.render_test))\n\n  if args.plot_results:\n    plt.style.use(\'seaborn\')\n    plt.plot(np.arange(0, len(rewards_history), 10), rewards_history[::10])\n    plt.xlabel(\'Episode\')\n    plt.ylabel(\'Total Reward\')\n    plt.show()\n\n'"
