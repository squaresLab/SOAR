file_path,api_count,code
config.py,2,"b'import os\nimport tensorflow as tf\n\nfrom prepro import prepro\nfrom main import train, test\n\nflags = tf.flags\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\nhome = os.path.expanduser(""~"")\ntrain_file = os.path.join(home, ""data"", ""squad"", ""train-v1.1.json"")\ndev_file = os.path.join(home, ""data"", ""squad"", ""dev-v1.1.json"")\ntest_file = os.path.join(home, ""data"", ""squad"", ""dev-v1.1.json"")\nglove_word_file = os.path.join(home, ""data"", ""glove"", ""glove.840B.300d.txt"")\n\ntarget_dir = ""data""\nlog_dir = ""log/event""\nsave_dir = ""log/model""\nanswer_dir = ""log/answer""\ntrain_record_file = os.path.join(target_dir, ""train.tfrecords"")\ndev_record_file = os.path.join(target_dir, ""dev.tfrecords"")\ntest_record_file = os.path.join(target_dir, ""test.tfrecords"")\nword_emb_file = os.path.join(target_dir, ""word_emb.json"")\nchar_emb_file = os.path.join(target_dir, ""char_emb.json"")\ntrain_eval = os.path.join(target_dir, ""train_eval.json"")\ndev_eval = os.path.join(target_dir, ""dev_eval.json"")\ntest_eval = os.path.join(target_dir, ""test_eval.json"")\ndev_meta = os.path.join(target_dir, ""dev_meta.json"")\ntest_meta = os.path.join(target_dir, ""test_meta.json"")\nword2idx_file = os.path.join(target_dir, ""word2idx.json"")\nchar2idx_file = os.path.join(target_dir, ""char2idx.json"")\nanswer_file = os.path.join(answer_dir, ""answer.json"")\n\nif not os.path.exists(target_dir):\n    os.makedirs(target_dir)\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\nif not os.path.exists(answer_dir):\n    os.makedirs(answer_dir)\n\nflags.DEFINE_string(""mode"", ""train"", ""train/debug/test"")\n\nflags.DEFINE_string(""target_dir"", target_dir, """")\nflags.DEFINE_string(""log_dir"", log_dir, """")\nflags.DEFINE_string(""save_dir"", save_dir, """")\nflags.DEFINE_string(""train_file"", train_file, """")\nflags.DEFINE_string(""dev_file"", dev_file, """")\nflags.DEFINE_string(""test_file"", test_file, """")\nflags.DEFINE_string(""glove_word_file"", glove_word_file, """")\n\nflags.DEFINE_string(""train_record_file"", train_record_file, """")\nflags.DEFINE_string(""dev_record_file"", dev_record_file, """")\nflags.DEFINE_string(""test_record_file"", test_record_file, """")\nflags.DEFINE_string(""word_emb_file"", word_emb_file, """")\nflags.DEFINE_string(""char_emb_file"", char_emb_file, """")\nflags.DEFINE_string(""train_eval_file"", train_eval, """")\nflags.DEFINE_string(""dev_eval_file"", dev_eval, """")\nflags.DEFINE_string(""test_eval_file"", test_eval, """")\nflags.DEFINE_string(""dev_meta"", dev_meta, """")\nflags.DEFINE_string(""test_meta"", test_meta, """")\nflags.DEFINE_string(""word2idx_file"", word2idx_file, """")\nflags.DEFINE_string(""char2idx_file"", char2idx_file, """")\nflags.DEFINE_string(""answer_file"", answer_file, """")\n\n\nflags.DEFINE_integer(""glove_char_size"", 94, ""Corpus size for Glove"")\nflags.DEFINE_integer(""glove_word_size"", int(2.2e6), ""Corpus size for Glove"")\nflags.DEFINE_integer(""glove_dim"", 300, ""Embedding dimension for Glove"")\nflags.DEFINE_integer(""char_dim"", 8, ""Embedding dimension for char"")\n\nflags.DEFINE_integer(""para_limit"", 400, ""Limit length for paragraph"")\nflags.DEFINE_integer(""ques_limit"", 50, ""Limit length for question"")\nflags.DEFINE_integer(""test_para_limit"", 1000,\n                     ""Max length for paragraph in test"")\nflags.DEFINE_integer(""test_ques_limit"", 100, ""Max length of questions in test"")\nflags.DEFINE_integer(""char_limit"", 16, ""Limit length for character"")\nflags.DEFINE_integer(""word_count_limit"", -1, ""Min count for word"")\nflags.DEFINE_integer(""char_count_limit"", -1, ""Min count for char"")\n\nflags.DEFINE_integer(""capacity"", 15000, ""Batch size of dataset shuffle"")\nflags.DEFINE_integer(""num_threads"", 4, ""Number of threads in input pipeline"")\nflags.DEFINE_boolean(""use_cudnn"", True, ""Whether to use cudnn (only for GPU)"")\nflags.DEFINE_boolean(""is_bucket"", False, ""Whether to use bucketing"")\nflags.DEFINE_list(""bucket_range"", [40, 361, 40], ""range of bucket"")\n\nflags.DEFINE_integer(""batch_size"", 64, ""Batch size"")\nflags.DEFINE_integer(""num_steps"", 60000, ""Number of steps"")\nflags.DEFINE_integer(""checkpoint"", 1000, ""checkpoint for evaluation"")\nflags.DEFINE_integer(""period"", 100, ""period to save batch loss"")\nflags.DEFINE_integer(""val_num_batches"", 150, ""Num of batches for evaluation"")\nflags.DEFINE_float(""init_lr"", 0.5, ""Initial lr for Adadelta"")\nflags.DEFINE_float(""keep_prob"", 0.7, ""Keep prob in rnn"")\nflags.DEFINE_float(""ptr_keep_prob"", 0.7, ""Keep prob for pointer network"")\nflags.DEFINE_float(""grad_clip"", 5.0, ""Global Norm gradient clipping rate"")\nflags.DEFINE_integer(""hidden"", 75, ""Hidden size"")\nflags.DEFINE_integer(""char_hidden"", 100, ""GRU dim for char"")\nflags.DEFINE_integer(""patience"", 3, ""Patience for lr decay"")\n\n# Extensions (Uncomment corresponding line in download.sh to download the required data)\nglove_char_file = os.path.join(\n    home, ""data"", ""glove"", ""glove.840B.300d-char.txt"")\nflags.DEFINE_string(""glove_char_file"", glove_char_file,\n                    ""Glove character embedding"")\nflags.DEFINE_boolean(""pretrained_char"", False,\n                     ""Whether to use pretrained char embedding"")\n\nfasttext_file = os.path.join(home, ""data"", ""fasttext"", ""wiki-news-300d-1M.vec"")\nflags.DEFINE_string(""fasttext_file"", fasttext_file, ""Fasttext word embedding"")\nflags.DEFINE_boolean(""fasttext"", False, ""Whether to use fasttext"")\n\n\ndef main(_):\n    config = flags.FLAGS\n    if config.mode == ""train"":\n        train(config)\n    elif config.mode == ""prepro"":\n        prepro(config)\n    elif config.mode == ""debug"":\n        config.num_steps = 2\n        config.val_num_batches = 1\n        config.checkpoint = 1\n        config.period = 1\n        train(config)\n    elif config.mode == ""test"":\n        test(config)\n    else:\n        print(""Unknown mode"")\n        exit(0)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
evaluate-v1.1.py,0,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa in paragraph[\'qas\']:\n                total += 1\n                if qa[\'id\'] not in predictions:\n                    message = \'Unanswered question \' + qa[\'id\'] + \\\n                              \' will receive score 0.\'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n                prediction = predictions[qa[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\nif __name__ == \'__main__\':\n    expected_version = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + expected_version)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if (dataset_json[\'version\'] != expected_version):\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(args.prediction_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    print(json.dumps(evaluate(dataset, predictions)))\n'"
func.py,76,"b'import tensorflow as tf\n\nINF = 1e30\n\n\nclass cudnn_gru:\n\n    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=None):\n        self.num_layers = num_layers\n        self.grus = []\n        self.inits = []\n        self.dropout_mask = []\n        for layer in range(num_layers):\n            input_size_ = input_size if layer == 0 else 2 * num_units\n            gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units)\n            gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units)\n            init_fw = tf.tile(tf.Variable(\n                tf.zeros([1, 1, num_units])), [1, batch_size, 1])\n            init_bw = tf.tile(tf.Variable(\n                tf.zeros([1, 1, num_units])), [1, batch_size, 1])\n            mask_fw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                              keep_prob=keep_prob, is_train=is_train, mode=None)\n            mask_bw = dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                              keep_prob=keep_prob, is_train=is_train, mode=None)\n            self.grus.append((gru_fw, gru_bw, ))\n            self.inits.append((init_fw, init_bw, ))\n            self.dropout_mask.append((mask_fw, mask_bw, ))\n\n    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n        outputs = [tf.transpose(inputs, [1, 0, 2])]\n        for layer in range(self.num_layers):\n            gru_fw, gru_bw = self.grus[layer]\n            init_fw, init_bw = self.inits[layer]\n            mask_fw, mask_bw = self.dropout_mask[layer]\n            with tf.variable_scope(""fw_{}"".format(layer)):\n                out_fw, _ = gru_fw(\n                    outputs[-1] * mask_fw, initial_state=(init_fw, ))\n            with tf.variable_scope(""bw_{}"".format(layer)):\n                inputs_bw = tf.reverse_sequence(\n                    outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n                out_bw, _ = gru_bw(inputs_bw, initial_state=(init_bw, ))\n                out_bw = tf.reverse_sequence(\n                    out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n            outputs.append(tf.concat([out_fw, out_bw], axis=2))\n        if concat_layers:\n            res = tf.concat(outputs[1:], axis=2)\n        else:\n            res = outputs[-1]\n        res = tf.transpose(res, [1, 0, 2])\n        return res\n\n\nclass native_gru:\n\n    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0, is_train=None, scope=""native_gru""):\n        self.num_layers = num_layers\n        self.grus = []\n        self.inits = []\n        self.dropout_mask = []\n        self.scope = scope\n        for layer in range(num_layers):\n            input_size_ = input_size if layer == 0 else 2 * num_units\n            gru_fw = tf.contrib.rnn.GRUCell(num_units)\n            gru_bw = tf.contrib.rnn.GRUCell(num_units)\n            init_fw = tf.tile(tf.Variable(\n                tf.zeros([1, num_units])), [batch_size, 1])\n            init_bw = tf.tile(tf.Variable(\n                tf.zeros([1, num_units])), [batch_size, 1])\n            mask_fw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),\n                              keep_prob=keep_prob, is_train=is_train, mode=None)\n            mask_bw = dropout(tf.ones([batch_size, 1, input_size_], dtype=tf.float32),\n                              keep_prob=keep_prob, is_train=is_train, mode=None)\n            self.grus.append((gru_fw, gru_bw, ))\n            self.inits.append((init_fw, init_bw, ))\n            self.dropout_mask.append((mask_fw, mask_bw, ))\n\n    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n        outputs = [inputs]\n        with tf.variable_scope(self.scope):\n            for layer in range(self.num_layers):\n                gru_fw, gru_bw = self.grus[layer]\n                init_fw, init_bw = self.inits[layer]\n                mask_fw, mask_bw = self.dropout_mask[layer]\n                with tf.variable_scope(""fw_{}"".format(layer)):\n                    out_fw, _ = tf.nn.dynamic_rnn(\n                        gru_fw, outputs[-1] * mask_fw, seq_len, initial_state=init_fw, dtype=tf.float32)\n                with tf.variable_scope(""bw_{}"".format(layer)):\n                    inputs_bw = tf.reverse_sequence(\n                        outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n                    out_bw, _ = tf.nn.dynamic_rnn(\n                        gru_bw, inputs_bw, seq_len, initial_state=init_bw, dtype=tf.float32)\n                    out_bw = tf.reverse_sequence(\n                        out_bw, seq_lengths=seq_len, seq_dim=1, batch_dim=0)\n                outputs.append(tf.concat([out_fw, out_bw], axis=2))\n        if concat_layers:\n            res = tf.concat(outputs[1:], axis=2)\n        else:\n            res = outputs[-1]\n        return res\n\n\nclass ptr_net:\n    def __init__(self, batch, hidden, keep_prob=1.0, is_train=None, scope=""ptr_net""):\n        self.gru = tf.contrib.rnn.GRUCell(hidden)\n        self.batch = batch\n        self.scope = scope\n        self.keep_prob = keep_prob\n        self.is_train = is_train\n        self.dropout_mask = dropout(tf.ones(\n            [batch, hidden], dtype=tf.float32), keep_prob=keep_prob, is_train=is_train)\n\n    def __call__(self, init, match, d, mask):\n        with tf.variable_scope(self.scope):\n            d_match = dropout(match, keep_prob=self.keep_prob,\n                              is_train=self.is_train)\n            inp, logits1 = pointer(d_match, init * self.dropout_mask, d, mask)\n            d_inp = dropout(inp, keep_prob=self.keep_prob,\n                            is_train=self.is_train)\n            _, state = self.gru(d_inp, init)\n            tf.get_variable_scope().reuse_variables()\n            _, logits2 = pointer(d_match, state * self.dropout_mask, d, mask)\n            return logits1, logits2\n\n\ndef dropout(args, keep_prob, is_train, mode=""recurrent""):\n    if keep_prob < 1.0:\n        noise_shape = None\n        scale = 1.0\n        shape = tf.shape(args)\n        if mode == ""embedding"":\n            noise_shape = [shape[0], 1]\n            scale = keep_prob\n        if mode == ""recurrent"" and len(args.get_shape().as_list()) == 3:\n            noise_shape = [shape[0], 1, shape[-1]]\n        args = tf.cond(is_train, lambda: tf.nn.dropout(\n            args, keep_prob, noise_shape=noise_shape) * scale, lambda: args)\n    return args\n\n\ndef softmax_mask(val, mask):\n    return -INF * (1 - tf.cast(mask, tf.float32)) + val\n\n\ndef pointer(inputs, state, hidden, mask, scope=""pointer""):\n    with tf.variable_scope(scope):\n        u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [\n            1, tf.shape(inputs)[1], 1]), inputs], axis=2)\n        s0 = tf.nn.tanh(dense(u, hidden, use_bias=False, scope=""s0""))\n        s = dense(s0, 1, use_bias=False, scope=""s"")\n        s1 = softmax_mask(tf.squeeze(s, [2]), mask)\n        a = tf.expand_dims(tf.nn.softmax(s1), axis=2)\n        res = tf.reduce_sum(a * inputs, axis=1)\n        return res, s1\n\n\ndef summ(memory, hidden, mask, keep_prob=1.0, is_train=None, scope=""summ""):\n    with tf.variable_scope(scope):\n        d_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)\n        s0 = tf.nn.tanh(dense(d_memory, hidden, scope=""s0""))\n        s = dense(s0, 1, use_bias=False, scope=""s"")\n        s1 = softmax_mask(tf.squeeze(s, [2]), mask)\n        a = tf.expand_dims(tf.nn.softmax(s1), axis=2)\n        res = tf.reduce_sum(a * memory, axis=1)\n        return res\n\n\ndef dot_attention(inputs, memory, mask, hidden, keep_prob=1.0, is_train=None, scope=""dot_attention""):\n    with tf.variable_scope(scope):\n\n        d_inputs = dropout(inputs, keep_prob=keep_prob, is_train=is_train)\n        d_memory = dropout(memory, keep_prob=keep_prob, is_train=is_train)\n        JX = tf.shape(inputs)[1]\n\n        with tf.variable_scope(""attention""):\n            inputs_ = tf.nn.relu(\n                dense(d_inputs, hidden, use_bias=False, scope=""inputs""))\n            memory_ = tf.nn.relu(\n                dense(d_memory, hidden, use_bias=False, scope=""memory""))\n            outputs = tf.matmul(inputs_, tf.transpose(\n                memory_, [0, 2, 1])) / (hidden ** 0.5)\n            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, JX, 1])\n            logits = tf.nn.softmax(softmax_mask(outputs, mask))\n            outputs = tf.matmul(logits, memory)\n            res = tf.concat([inputs, outputs], axis=2)\n\n        with tf.variable_scope(""gate""):\n            dim = res.get_shape().as_list()[-1]\n            d_res = dropout(res, keep_prob=keep_prob, is_train=is_train)\n            gate = tf.nn.sigmoid(dense(d_res, dim, use_bias=False))\n            return res * gate\n\n\ndef dense(inputs, hidden, use_bias=True, scope=""dense""):\n    with tf.variable_scope(scope):\n        shape = tf.shape(inputs)\n        dim = inputs.get_shape().as_list()[-1]\n        out_shape = [shape[idx] for idx in range(\n            len(inputs.get_shape().as_list()) - 1)] + [hidden]\n        flat_inputs = tf.reshape(inputs, [-1, dim])\n        W = tf.get_variable(""W"", [dim, hidden])\n        res = tf.matmul(flat_inputs, W)\n        if use_bias:\n            b = tf.get_variable(\n                ""b"", [hidden], initializer=tf.constant_initializer(0.))\n            res = tf.nn.bias_add(res, b)\n        res = tf.reshape(res, out_shape)\n        return res\n'"
inference.py,54,"b'import tensorflow as tf\nimport spacy\nimport os\nimport numpy as np\nimport ujson as json\n\n\nfrom func import cudnn_gru, native_gru, dot_attention, summ, ptr_net\nfrom prepro import word_tokenize, convert_idx\n\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\n# Must be consistant with training\nchar_limit = 16\nhidden = 75\nchar_dim = 8\nchar_hidden = 100\nuse_cudnn = True\n\n# File path\ntarget_dir = ""data""\nsave_dir = ""log/model""\nword_emb_file = os.path.join(target_dir, ""word_emb.json"")\nchar_emb_file = os.path.join(target_dir, ""char_emb.json"")\nword2idx_file = os.path.join(target_dir, ""word2idx.json"")\nchar2idx_file = os.path.join(target_dir, ""char2idx.json"")\n\n\nclass InfModel(object):\n    # Used to zero elements in the probability matrix that correspond to answer\n    # spans that are longer than the number of tokens specified here.\n    max_answer_tokens = 15\n\n    def __init__(self, word_mat, char_mat):\n        self.c = tf.placeholder(tf.int32, [1, None])\n        self.q = tf.placeholder(tf.int32, [1, None])\n        self.ch = tf.placeholder(tf.int32, [1, None, char_limit])\n        self.qh = tf.placeholder(tf.int32, [1, None, char_limit])\n        self.tokens_in_context = tf.placeholder(tf.int64)\n\n        self.word_mat = tf.get_variable(""word_mat"", initializer=tf.constant(\n            word_mat, dtype=tf.float32), trainable=False)\n        self.char_mat = tf.get_variable(\n            ""char_mat"", initializer=tf.constant(char_mat, dtype=tf.float32))\n\n        self.c_mask = tf.cast(self.c, tf.bool)\n        self.q_mask = tf.cast(self.q, tf.bool)\n        self.c_len = tf.reduce_sum(tf.cast(self.c_mask, tf.int32), axis=1)\n        self.q_len = tf.reduce_sum(tf.cast(self.q_mask, tf.int32), axis=1)\n\n        self.c_maxlen = tf.reduce_max(self.c_len)\n        self.q_maxlen = tf.reduce_max(self.q_len)\n\n        self.ch_len = tf.reshape(tf.reduce_sum(\n            tf.cast(tf.cast(self.ch, tf.bool), tf.int32), axis=2), [-1])\n        self.qh_len = tf.reshape(tf.reduce_sum(\n            tf.cast(tf.cast(self.qh, tf.bool), tf.int32), axis=2), [-1])\n\n        self.ready()\n\n    def ready(self):\n        N, PL, QL, CL, d, dc, dg = \\\n            1, self.c_maxlen, self.q_maxlen, char_limit, hidden, char_dim, \\\n            char_hidden\n        gru = cudnn_gru if use_cudnn else native_gru\n\n        with tf.variable_scope(""emb""):\n            with tf.variable_scope(""char""):\n                ch_emb = tf.reshape(tf.nn.embedding_lookup(\n                    self.char_mat, self.ch), [N * PL, CL, dc])\n                qh_emb = tf.reshape(tf.nn.embedding_lookup(\n                    self.char_mat, self.qh), [N * QL, CL, dc])\n                cell_fw = tf.contrib.rnn.GRUCell(dg)\n                cell_bw = tf.contrib.rnn.GRUCell(dg)\n                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, ch_emb, self.ch_len, dtype=tf.float32)\n                ch_emb = tf.concat([state_fw, state_bw], axis=1)\n                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, qh_emb, self.qh_len, dtype=tf.float32)\n                qh_emb = tf.concat([state_fw, state_bw], axis=1)\n                qh_emb = tf.reshape(qh_emb, [N, QL, 2 * dg])\n                ch_emb = tf.reshape(ch_emb, [N, PL, 2 * dg])\n\n            with tf.name_scope(""word""):\n                c_emb = tf.nn.embedding_lookup(self.word_mat, self.c)\n                q_emb = tf.nn.embedding_lookup(self.word_mat, self.q)\n\n            c_emb = tf.concat([c_emb, ch_emb], axis=2)\n            q_emb = tf.concat([q_emb, qh_emb], axis=2)\n\n        with tf.variable_scope(""encoding""):\n            rnn = gru(num_layers=3, num_units=d, batch_size=N,\n                      input_size=c_emb.get_shape().as_list()[-1])\n            c = rnn(c_emb, seq_len=self.c_len)\n            q = rnn(q_emb, seq_len=self.q_len)\n\n        with tf.variable_scope(""attention""):\n            qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d)\n            rnn = gru(num_layers=1, num_units=d, batch_size=N,\n                      input_size=qc_att.get_shape().as_list()[-1])\n            att = rnn(qc_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""match""):\n            self_att = dot_attention(att, att, mask=self.c_mask, hidden=d)\n            rnn = gru(num_layers=1, num_units=d, batch_size=N,\n                      input_size=self_att.get_shape().as_list()[-1])\n            match = rnn(self_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""pointer""):\n            init = summ(q[:, :, -2 * d:], d, mask=self.q_mask)\n            pointer = ptr_net(batch=N, hidden=init.get_shape().as_list()[-1])\n            logits1, logits2 = pointer(init, match, d, self.c_mask)\n\n        with tf.variable_scope(""predict""):\n            outer = tf.matmul(tf.expand_dims(tf.nn.softmax(logits1), axis=2),\n                              tf.expand_dims(tf.nn.softmax(logits2), axis=1))\n            outer = tf.cond(\n                self.tokens_in_context < self.max_answer_tokens,\n                lambda: tf.matrix_band_part(outer, 0, -1),\n                lambda: tf.matrix_band_part(outer, 0, self.max_answer_tokens))\n            self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n            self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n\n\nclass Inference(object):\n\n    def __init__(self):\n        with open(word_emb_file, ""r"") as fh:\n            self.word_mat = np.array(json.load(fh), dtype=np.float32)\n        with open(char_emb_file, ""r"") as fh:\n            self.char_mat = np.array(json.load(fh), dtype=np.float32)\n        with open(word2idx_file, ""r"") as fh:\n            self.word2idx_dict = json.load(fh)\n        with open(char2idx_file, ""r"") as fh:\n            self.char2idx_dict = json.load(fh)\n        self.model = InfModel(self.word_mat, self.char_mat)\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n        saver = tf.train.Saver()\n        saver.restore(self.sess, tf.train.latest_checkpoint(save_dir))\n\n    def response(self, context, question):\n        sess = self.sess\n        model = self.model\n        span, context_idxs, ques_idxs, context_char_idxs, ques_char_idxs = \\\n            self.prepro(context, question)\n        yp1, yp2 = \\\n            sess.run(\n                [model.yp1, model.yp2],\n                feed_dict={\n                    model.c: context_idxs, model.q: ques_idxs,\n                    model.ch: context_char_idxs, model.qh: ques_char_idxs,\n                    model.tokens_in_context: len(span)})\n        start_idx = span[yp1[0]][0]\n        end_idx = span[yp2[0]][1]\n        return context[start_idx: end_idx]\n\n    def prepro(self, context, question):\n        context = context.replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n        context_tokens = word_tokenize(context)\n        context_chars = [list(token) for token in context_tokens]\n        spans = convert_idx(context, context_tokens)\n        ques = question.replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n        ques_tokens = word_tokenize(ques)\n        ques_chars = [list(token) for token in ques_tokens]\n\n        context_idxs = np.zeros([1, len(context_tokens)], dtype=np.int32)\n        context_char_idxs = np.zeros(\n            [1, len(context_tokens), char_limit], dtype=np.int32)\n        ques_idxs = np.zeros([1, len(ques_tokens)], dtype=np.int32)\n        ques_char_idxs = np.zeros(\n            [1, len(ques_tokens), char_limit], dtype=np.int32)\n\n        def _get_word(word):\n            for each in (word, word.lower(), word.capitalize(), word.upper()):\n                if each in self.word2idx_dict:\n                    return self.word2idx_dict[each]\n            return 1\n\n        def _get_char(char):\n            if char in self.char2idx_dict:\n                return self.char2idx_dict[char]\n            return 1\n\n        for i, token in enumerate(context_tokens):\n            context_idxs[0, i] = _get_word(token)\n\n        for i, token in enumerate(ques_tokens):\n            ques_idxs[0, i] = _get_word(token)\n\n        for i, token in enumerate(context_chars):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                context_char_idxs[0, i, j] = _get_char(char)\n\n        for i, token in enumerate(ques_chars):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                ques_char_idxs[0, i, j] = _get_char(char)\n        return spans, context_idxs, ques_idxs, context_char_idxs, ques_char_idxs\n\n\n# Demo, example from paper ""SQuAD: 100,000+ Questions for Machine Comprehension of Text""\nif __name__ == ""__main__"":\n    infer = Inference()\n    context = ""In meteorology, precipitation is any product of the condensation "" \\\n              ""of atmospheric water vapor that falls under gravity. The main forms "" \\\n              ""of precipitation include drizzle, rain, sleet, snow, graupel and hail."" \\\n              ""Precipitation forms as smaller droplets coalesce via collision with other "" \\\n              ""rain drops or ice crystals within a cloud. Short, intense periods of rain "" \\\n              ""in scattered locations are called \xe2\x80\x9cshowers\xe2\x80\x9d.""\n    ques1 = ""What causes precipitation to fall?""\n    ques2 = ""What is another main form of precipitation besides drizzle, rain, snow, sleet and hail?""\n    ques3 = ""Where do water droplets collide with ice crystals to form precipitation?""\n\n    # Correct: gravity, Output: drizzle, rain, sleet, snow, graupel and hail\n    ans1 = infer.response(context, ques1)\n    print(""Answer 1: {}"".format(ans1))\n\n    # Correct: graupel, Output: graupel\n    ans2 = infer.response(context, ques2)\n    print(""Answer 2: {}"".format(ans2))\n\n    # Correct: within a cloud, Output: within a cloud\n    ans3 = infer.response(context, ques3)\n    print(""Answer 3: {}"".format(ans3))\n'"
main.py,24,"b'import tensorflow as tf\nimport ujson as json\nimport numpy as np\nfrom tqdm import tqdm\nimport os\n\nfrom model import Model\nfrom util import get_record_parser, convert_tokens, evaluate, get_batch_dataset, get_dataset\n\n\ndef train(config):\n    with open(config.word_emb_file, ""r"") as fh:\n        word_mat = np.array(json.load(fh), dtype=np.float32)\n    with open(config.char_emb_file, ""r"") as fh:\n        char_mat = np.array(json.load(fh), dtype=np.float32)\n    with open(config.train_eval_file, ""r"") as fh:\n        train_eval_file = json.load(fh)\n    with open(config.dev_eval_file, ""r"") as fh:\n        dev_eval_file = json.load(fh)\n    with open(config.dev_meta, ""r"") as fh:\n        meta = json.load(fh)\n\n    dev_total = meta[""total""]\n\n    print(""Building model..."")\n    parser = get_record_parser(config)\n    train_dataset = get_batch_dataset(config.train_record_file, parser, config)\n    dev_dataset = get_dataset(config.dev_record_file, parser, config)\n    handle = tf.placeholder(tf.string, shape=[])\n    iterator = tf.data.Iterator.from_string_handle(\n        handle, train_dataset.output_types, train_dataset.output_shapes)\n    train_iterator = train_dataset.make_one_shot_iterator()\n    dev_iterator = dev_dataset.make_one_shot_iterator()\n\n    model = Model(config, iterator, word_mat, char_mat)\n\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.allow_growth = True\n\n    loss_save = 100.0\n    patience = 0\n    lr = config.init_lr\n\n    with tf.Session(config=sess_config) as sess:\n        writer = tf.summary.FileWriter(config.log_dir)\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        train_handle = sess.run(train_iterator.string_handle())\n        dev_handle = sess.run(dev_iterator.string_handle())\n        sess.run(tf.assign(model.is_train, tf.constant(True, dtype=tf.bool)))\n        sess.run(tf.assign(model.lr, tf.constant(lr, dtype=tf.float32)))\n\n        for _ in tqdm(range(1, config.num_steps + 1)):\n            global_step = sess.run(model.global_step) + 1\n            loss, train_op = sess.run([model.loss, model.train_op], feed_dict={\n                                      handle: train_handle})\n            if global_step % config.period == 0:\n                loss_sum = tf.Summary(value=[tf.Summary.Value(\n                    tag=""model/loss"", simple_value=loss), ])\n                writer.add_summary(loss_sum, global_step)\n            if global_step % config.checkpoint == 0:\n                sess.run(tf.assign(model.is_train,\n                                   tf.constant(False, dtype=tf.bool)))\n                _, summ = evaluate_batch(\n                    model, config.val_num_batches, train_eval_file, sess, ""train"", handle, train_handle)\n                for s in summ:\n                    writer.add_summary(s, global_step)\n\n                metrics, summ = evaluate_batch(\n                    model, dev_total // config.batch_size + 1, dev_eval_file, sess, ""dev"", handle, dev_handle)\n                sess.run(tf.assign(model.is_train,\n                                   tf.constant(True, dtype=tf.bool)))\n\n                dev_loss = metrics[""loss""]\n                if dev_loss < loss_save:\n                    loss_save = dev_loss\n                    patience = 0\n                else:\n                    patience += 1\n                if patience >= config.patience:\n                    lr /= 2.0\n                    loss_save = dev_loss\n                    patience = 0\n                sess.run(tf.assign(model.lr, tf.constant(lr, dtype=tf.float32)))\n                for s in summ:\n                    writer.add_summary(s, global_step)\n                writer.flush()\n                filename = os.path.join(\n                    config.save_dir, ""model_{}.ckpt"".format(global_step))\n                saver.save(sess, filename)\n\n\ndef evaluate_batch(model, num_batches, eval_file, sess, data_type, handle, str_handle):\n    answer_dict = {}\n    losses = []\n    for _ in tqdm(range(1, num_batches + 1)):\n        qa_id, loss, yp1, yp2, = sess.run(\n            [model.qa_id, model.loss, model.yp1, model.yp2], feed_dict={handle: str_handle})\n        answer_dict_, _ = convert_tokens(\n            eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist())\n        answer_dict.update(answer_dict_)\n        losses.append(loss)\n    loss = np.mean(losses)\n    metrics = evaluate(eval_file, answer_dict)\n    metrics[""loss""] = loss\n    loss_sum = tf.Summary(value=[tf.Summary.Value(\n        tag=""{}/loss"".format(data_type), simple_value=metrics[""loss""]), ])\n    f1_sum = tf.Summary(value=[tf.Summary.Value(\n        tag=""{}/f1"".format(data_type), simple_value=metrics[""f1""]), ])\n    em_sum = tf.Summary(value=[tf.Summary.Value(\n        tag=""{}/em"".format(data_type), simple_value=metrics[""exact_match""]), ])\n    return metrics, [loss_sum, f1_sum, em_sum]\n\n\ndef test(config):\n    with open(config.word_emb_file, ""r"") as fh:\n        word_mat = np.array(json.load(fh), dtype=np.float32)\n    with open(config.char_emb_file, ""r"") as fh:\n        char_mat = np.array(json.load(fh), dtype=np.float32)\n    with open(config.test_eval_file, ""r"") as fh:\n        eval_file = json.load(fh)\n    with open(config.test_meta, ""r"") as fh:\n        meta = json.load(fh)\n\n    total = meta[""total""]\n\n    print(""Loading model..."")\n    test_batch = get_dataset(config.test_record_file, get_record_parser(\n        config, is_test=True), config).make_one_shot_iterator()\n\n    model = Model(config, test_batch, word_mat, char_mat, trainable=False)\n\n    sess_config = tf.ConfigProto(allow_soft_placement=True)\n    sess_config.gpu_options.allow_growth = True\n\n    with tf.Session(config=sess_config) as sess:\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        saver.restore(sess, tf.train.latest_checkpoint(config.save_dir))\n        sess.run(tf.assign(model.is_train, tf.constant(False, dtype=tf.bool)))\n        losses = []\n        answer_dict = {}\n        remapped_dict = {}\n        for step in tqdm(range(total // config.batch_size + 1)):\n            qa_id, loss, yp1, yp2 = sess.run(\n                [model.qa_id, model.loss, model.yp1, model.yp2])\n            answer_dict_, remapped_dict_ = convert_tokens(\n                eval_file, qa_id.tolist(), yp1.tolist(), yp2.tolist())\n            answer_dict.update(answer_dict_)\n            remapped_dict.update(remapped_dict_)\n            losses.append(loss)\n        loss = np.mean(losses)\n        metrics = evaluate(eval_file, answer_dict)\n        with open(config.answer_file, ""w"") as fh:\n            json.dump(remapped_dict, fh)\n        print(""Exact Match: {}, F1: {}"".format(\n            metrics[\'exact_match\'], metrics[\'f1\']))\n'"
model.py,64,"b'import tensorflow as tf\nfrom func import cudnn_gru, native_gru, dot_attention, summ, dropout, ptr_net\n\n\nclass Model(object):\n    def __init__(self, config, batch, word_mat=None, char_mat=None, trainable=True, opt=True):\n        self.config = config\n        self.global_step = tf.get_variable(\'global_step\', shape=[], dtype=tf.int32,\n                                           initializer=tf.constant_initializer(0), trainable=False)\n        self.c, self.q, self.ch, self.qh, self.y1, self.y2, self.qa_id = batch.get_next()\n        self.is_train = tf.get_variable(\n            ""is_train"", shape=[], dtype=tf.bool, trainable=False)\n        self.word_mat = tf.get_variable(""word_mat"", initializer=tf.constant(\n            word_mat, dtype=tf.float32), trainable=False)\n        self.char_mat = tf.get_variable(\n            ""char_mat"", initializer=tf.constant(char_mat, dtype=tf.float32))\n\n        self.c_mask = tf.cast(self.c, tf.bool)\n        self.q_mask = tf.cast(self.q, tf.bool)\n        self.c_len = tf.reduce_sum(tf.cast(self.c_mask, tf.int32), axis=1)\n        self.q_len = tf.reduce_sum(tf.cast(self.q_mask, tf.int32), axis=1)\n\n        if opt:\n            N, CL = config.batch_size, config.char_limit\n            self.c_maxlen = tf.reduce_max(self.c_len)\n            self.q_maxlen = tf.reduce_max(self.q_len)\n            self.c = tf.slice(self.c, [0, 0], [N, self.c_maxlen])\n            self.q = tf.slice(self.q, [0, 0], [N, self.q_maxlen])\n            self.c_mask = tf.slice(self.c_mask, [0, 0], [N, self.c_maxlen])\n            self.q_mask = tf.slice(self.q_mask, [0, 0], [N, self.q_maxlen])\n            self.ch = tf.slice(self.ch, [0, 0, 0], [N, self.c_maxlen, CL])\n            self.qh = tf.slice(self.qh, [0, 0, 0], [N, self.q_maxlen, CL])\n            self.y1 = tf.slice(self.y1, [0, 0], [N, self.c_maxlen])\n            self.y2 = tf.slice(self.y2, [0, 0], [N, self.c_maxlen])\n        else:\n            self.c_maxlen, self.q_maxlen = config.para_limit, config.ques_limit\n\n        self.ch_len = tf.reshape(tf.reduce_sum(\n            tf.cast(tf.cast(self.ch, tf.bool), tf.int32), axis=2), [-1])\n        self.qh_len = tf.reshape(tf.reduce_sum(\n            tf.cast(tf.cast(self.qh, tf.bool), tf.int32), axis=2), [-1])\n\n        self.ready()\n\n        if trainable:\n            self.lr = tf.get_variable(\n                ""lr"", shape=[], dtype=tf.float32, trainable=False)\n            self.opt = tf.train.AdadeltaOptimizer(\n                learning_rate=self.lr, epsilon=1e-6)\n            grads = self.opt.compute_gradients(self.loss)\n            gradients, variables = zip(*grads)\n            capped_grads, _ = tf.clip_by_global_norm(\n                gradients, config.grad_clip)\n            self.train_op = self.opt.apply_gradients(\n                zip(capped_grads, variables), global_step=self.global_step)\n\n    def ready(self):\n        config = self.config\n        N, PL, QL, CL, d, dc, dg = config.batch_size, self.c_maxlen, self.q_maxlen, config.char_limit, config.hidden, config.char_dim, config.char_hidden\n        gru = cudnn_gru if config.use_cudnn else native_gru\n\n        with tf.variable_scope(""emb""):\n            with tf.variable_scope(""char""):\n                ch_emb = tf.reshape(tf.nn.embedding_lookup(\n                    self.char_mat, self.ch), [N * PL, CL, dc])\n                qh_emb = tf.reshape(tf.nn.embedding_lookup(\n                    self.char_mat, self.qh), [N * QL, CL, dc])\n                ch_emb = dropout(\n                    ch_emb, keep_prob=config.keep_prob, is_train=self.is_train)\n                qh_emb = dropout(\n                    qh_emb, keep_prob=config.keep_prob, is_train=self.is_train)\n                cell_fw = tf.contrib.rnn.GRUCell(dg)\n                cell_bw = tf.contrib.rnn.GRUCell(dg)\n                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, ch_emb, self.ch_len, dtype=tf.float32)\n                ch_emb = tf.concat([state_fw, state_bw], axis=1)\n                _, (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw, cell_bw, qh_emb, self.qh_len, dtype=tf.float32)\n                qh_emb = tf.concat([state_fw, state_bw], axis=1)\n                qh_emb = tf.reshape(qh_emb, [N, QL, 2 * dg])\n                ch_emb = tf.reshape(ch_emb, [N, PL, 2 * dg])\n\n            with tf.name_scope(""word""):\n                c_emb = tf.nn.embedding_lookup(self.word_mat, self.c)\n                q_emb = tf.nn.embedding_lookup(self.word_mat, self.q)\n\n            c_emb = tf.concat([c_emb, ch_emb], axis=2)\n            q_emb = tf.concat([q_emb, qh_emb], axis=2)\n\n        with tf.variable_scope(""encoding""):\n            rnn = gru(num_layers=3, num_units=d, batch_size=N, input_size=c_emb.get_shape(\n            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n            c = rnn(c_emb, seq_len=self.c_len)\n            q = rnn(q_emb, seq_len=self.q_len)\n\n        with tf.variable_scope(""attention""):\n            qc_att = dot_attention(c, q, mask=self.q_mask, hidden=d,\n                                   keep_prob=config.keep_prob, is_train=self.is_train)\n            rnn = gru(num_layers=1, num_units=d, batch_size=N, input_size=qc_att.get_shape(\n            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n            att = rnn(qc_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""match""):\n            self_att = dot_attention(\n                att, att, mask=self.c_mask, hidden=d, keep_prob=config.keep_prob, is_train=self.is_train)\n            rnn = gru(num_layers=1, num_units=d, batch_size=N, input_size=self_att.get_shape(\n            ).as_list()[-1], keep_prob=config.keep_prob, is_train=self.is_train)\n            match = rnn(self_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""pointer""):\n            init = summ(q[:, :, -2 * d:], d, mask=self.q_mask,\n                        keep_prob=config.ptr_keep_prob, is_train=self.is_train)\n            pointer = ptr_net(batch=N, hidden=init.get_shape().as_list(\n            )[-1], keep_prob=config.ptr_keep_prob, is_train=self.is_train)\n            logits1, logits2 = pointer(init, match, d, self.c_mask)\n\n        with tf.variable_scope(""predict""):\n            outer = tf.matmul(tf.expand_dims(tf.nn.softmax(logits1), axis=2),\n                              tf.expand_dims(tf.nn.softmax(logits2), axis=1))\n            outer = tf.matrix_band_part(outer, 0, 15)\n            self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n            self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n            losses = tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits1, labels=tf.stop_gradient(self.y1))\n            losses2 = tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits2, labels=tf.stop_gradient(self.y2))\n            self.loss = tf.reduce_mean(losses + losses2)\n\n    def get_loss(self):\n        return self.loss\n\n    def get_global_step(self):\n        return self.global_step\n'"
prepro.py,9,"b'import tensorflow as tf\nimport random\nfrom tqdm import tqdm\nimport spacy\nimport ujson as json\nfrom collections import Counter\nimport numpy as np\nimport os.path\n\nnlp = spacy.blank(""en"")\n\n\ndef word_tokenize(sent):\n    doc = nlp(sent)\n    return [token.text for token in doc]\n\n\ndef convert_idx(text, tokens):\n    current = 0\n    spans = []\n    for token in tokens:\n        current = text.find(token, current)\n        if current < 0:\n            print(""Token {} cannot be found"".format(token))\n            raise Exception()\n        spans.append((current, current + len(token)))\n        current += len(token)\n    return spans\n\n\ndef process_file(filename, data_type, word_counter, char_counter):\n    print(""Generating {} examples..."".format(data_type))\n    examples = []\n    eval_examples = {}\n    total = 0\n    with open(filename, ""r"") as fh:\n        source = json.load(fh)\n        for article in tqdm(source[""data""]):\n            for para in article[""paragraphs""]:\n                context = para[""context""].replace(\n                    ""\'\'"", \'"" \').replace(""``"", \'"" \')\n                context_tokens = word_tokenize(context)\n                context_chars = [list(token) for token in context_tokens]\n                spans = convert_idx(context, context_tokens)\n                for token in context_tokens:\n                    word_counter[token] += len(para[""qas""])\n                    for char in token:\n                        char_counter[char] += len(para[""qas""])\n                for qa in para[""qas""]:\n                    total += 1\n                    ques = qa[""question""].replace(\n                        ""\'\'"", \'"" \').replace(""``"", \'"" \')\n                    ques_tokens = word_tokenize(ques)\n                    ques_chars = [list(token) for token in ques_tokens]\n                    for token in ques_tokens:\n                        word_counter[token] += 1\n                        for char in token:\n                            char_counter[char] += 1\n                    y1s, y2s = [], []\n                    answer_texts = []\n                    for answer in qa[""answers""]:\n                        answer_text = answer[""text""]\n                        answer_start = answer[\'answer_start\']\n                        answer_end = answer_start + len(answer_text)\n                        answer_texts.append(answer_text)\n                        answer_span = []\n                        for idx, span in enumerate(spans):\n                            if not (answer_end <= span[0] or answer_start >= span[1]):\n                                answer_span.append(idx)\n                        y1, y2 = answer_span[0], answer_span[-1]\n                        y1s.append(y1)\n                        y2s.append(y2)\n                    example = {""context_tokens"": context_tokens, ""context_chars"": context_chars, ""ques_tokens"": ques_tokens,\n                               ""ques_chars"": ques_chars, ""y1s"": y1s, ""y2s"": y2s, ""id"": total}\n                    examples.append(example)\n                    eval_examples[str(total)] = {\n                        ""context"": context, ""spans"": spans, ""answers"": answer_texts, ""uuid"": qa[""id""]}\n        random.shuffle(examples)\n        print(""{} questions in total"".format(len(examples)))\n    return examples, eval_examples\n\n\ndef get_embedding(counter, data_type, limit=-1, emb_file=None, size=None, vec_size=None, token2idx_dict=None):\n    print(""Generating {} embedding..."".format(data_type))\n    embedding_dict = {}\n    filtered_elements = [k for k, v in counter.items() if v > limit]\n    if emb_file is not None:\n        assert size is not None\n        assert vec_size is not None\n        with open(emb_file, ""r"", encoding=""utf-8"") as fh:\n            for line in tqdm(fh, total=size):\n                array = line.split()\n                word = """".join(array[0:-vec_size])\n                vector = list(map(float, array[-vec_size:]))\n                if word in counter and counter[word] > limit:\n                    embedding_dict[word] = vector\n        print(""{} / {} tokens have corresponding {} embedding vector"".format(\n            len(embedding_dict), len(filtered_elements), data_type))\n    else:\n        assert vec_size is not None\n        for token in filtered_elements:\n            embedding_dict[token] = [np.random.normal(\n                scale=0.01) for _ in range(vec_size)]\n        print(""{} tokens have corresponding embedding vector"".format(\n            len(filtered_elements)))\n\n    NULL = ""--NULL--""\n    OOV = ""--OOV--""\n    token2idx_dict = {token: idx for idx, token in enumerate(\n        embedding_dict.keys(), 2)} if token2idx_dict is None else token2idx_dict\n    token2idx_dict[NULL] = 0\n    token2idx_dict[OOV] = 1\n    embedding_dict[NULL] = [0. for _ in range(vec_size)]\n    embedding_dict[OOV] = [0. for _ in range(vec_size)]\n    idx2emb_dict = {idx: embedding_dict[token]\n                    for token, idx in token2idx_dict.items()}\n    emb_mat = [idx2emb_dict[idx] for idx in range(len(idx2emb_dict))]\n    return emb_mat, token2idx_dict\n\n\ndef build_features(config, examples, data_type, out_file, word2idx_dict, char2idx_dict, is_test=False):\n\n    para_limit = config.test_para_limit if is_test else config.para_limit\n    ques_limit = config.test_ques_limit if is_test else config.ques_limit\n    char_limit = config.char_limit\n\n    def filter_func(example, is_test=False):\n        return len(example[""context_tokens""]) > para_limit or len(example[""ques_tokens""]) > ques_limit\n\n    print(""Processing {} examples..."".format(data_type))\n    writer = tf.python_io.TFRecordWriter(out_file)\n    total = 0\n    total_ = 0\n    meta = {}\n    for example in tqdm(examples):\n        total_ += 1\n\n        if filter_func(example, is_test):\n            continue\n\n        total += 1\n        context_idxs = np.zeros([para_limit], dtype=np.int32)\n        context_char_idxs = np.zeros([para_limit, char_limit], dtype=np.int32)\n        ques_idxs = np.zeros([ques_limit], dtype=np.int32)\n        ques_char_idxs = np.zeros([ques_limit, char_limit], dtype=np.int32)\n        y1 = np.zeros([para_limit], dtype=np.float32)\n        y2 = np.zeros([para_limit], dtype=np.float32)\n\n        def _get_word(word):\n            for each in (word, word.lower(), word.capitalize(), word.upper()):\n                if each in word2idx_dict:\n                    return word2idx_dict[each]\n            return 1\n\n        def _get_char(char):\n            if char in char2idx_dict:\n                return char2idx_dict[char]\n            return 1\n\n        for i, token in enumerate(example[""context_tokens""]):\n            context_idxs[i] = _get_word(token)\n\n        for i, token in enumerate(example[""ques_tokens""]):\n            ques_idxs[i] = _get_word(token)\n\n        for i, token in enumerate(example[""context_chars""]):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                context_char_idxs[i, j] = _get_char(char)\n\n        for i, token in enumerate(example[""ques_chars""]):\n            for j, char in enumerate(token):\n                if j == char_limit:\n                    break\n                ques_char_idxs[i, j] = _get_char(char)\n\n        start, end = example[""y1s""][-1], example[""y2s""][-1]\n        y1[start], y2[end] = 1.0, 1.0\n\n        record = tf.train.Example(features=tf.train.Features(feature={\n                                  ""context_idxs"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_idxs.tostring()])),\n                                  ""ques_idxs"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_idxs.tostring()])),\n                                  ""context_char_idxs"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[context_char_idxs.tostring()])),\n                                  ""ques_char_idxs"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[ques_char_idxs.tostring()])),\n                                  ""y1"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y1.tostring()])),\n                                  ""y2"": tf.train.Feature(bytes_list=tf.train.BytesList(value=[y2.tostring()])),\n                                  ""id"": tf.train.Feature(int64_list=tf.train.Int64List(value=[example[""id""]]))\n                                  }))\n        writer.write(record.SerializeToString())\n    print(""Build {} / {} instances of features in total"".format(total, total_))\n    meta[""total""] = total\n    writer.close()\n    return meta\n\n\ndef save(filename, obj, message=None):\n    if message is not None:\n        print(""Saving {}..."".format(message))\n        with open(filename, ""w"") as fh:\n            json.dump(obj, fh)\n\n\ndef prepro(config):\n    word_counter, char_counter = Counter(), Counter()\n    train_examples, train_eval = process_file(\n        config.train_file, ""train"", word_counter, char_counter)\n    dev_examples, dev_eval = process_file(\n        config.dev_file, ""dev"", word_counter, char_counter)\n    test_examples, test_eval = process_file(\n        config.test_file, ""test"", word_counter, char_counter)\n\n    word_emb_file = config.fasttext_file if config.fasttext else config.glove_word_file\n    char_emb_file = config.glove_char_file if config.pretrained_char else None\n    char_emb_size = config.glove_char_size if config.pretrained_char else None\n    char_emb_dim = config.glove_dim if config.pretrained_char else config.char_dim\n\n    word2idx_dict = None\n    if os.path.isfile(config.word2idx_file):\n        with open(config.word2idx_file, ""r"") as fh:\n            word2idx_dict = json.load(fh)\n    word_emb_mat, word2idx_dict = get_embedding(word_counter, ""word"", emb_file=word_emb_file,\n                                                size=config.glove_word_size, vec_size=config.glove_dim, token2idx_dict=word2idx_dict)\n\n    char2idx_dict = None\n    if os.path.isfile(config.char2idx_file):\n        with open(config.char2idx_file, ""r"") as fh:\n            char2idx_dict = json.load(fh)\n    char_emb_mat, char2idx_dict = get_embedding(\n        char_counter, ""char"", emb_file=char_emb_file, size=char_emb_size, vec_size=char_emb_dim, token2idx_dict=char2idx_dict)\n\n    build_features(config, train_examples, ""train"",\n                   config.train_record_file, word2idx_dict, char2idx_dict)\n    dev_meta = build_features(config, dev_examples, ""dev"",\n                              config.dev_record_file, word2idx_dict, char2idx_dict)\n    test_meta = build_features(config, test_examples, ""test"",\n                               config.test_record_file, word2idx_dict, char2idx_dict, is_test=True)\n\n    save(config.word_emb_file, word_emb_mat, message=""word embedding"")\n    save(config.char_emb_file, char_emb_mat, message=""char embedding"")\n    save(config.train_eval_file, train_eval, message=""train eval"")\n    save(config.dev_eval_file, dev_eval, message=""dev eval"")\n    save(config.test_eval_file, test_eval, message=""test eval"")\n    save(config.dev_meta, dev_meta, message=""dev meta"")\n    save(config.word2idx_file, word2idx_dict, message=""word2idx"")\n    save(config.char2idx_file, char2idx_dict, message=""char2idx"")\n    save(config.test_meta, test_meta, message=""test meta"")\n'"
util.py,31,"b'import tensorflow as tf\nimport numpy as np\nimport re\nfrom collections import Counter\nimport string\n\n\ndef get_record_parser(config, is_test=False):\n    def parse(example):\n        para_limit = config.test_para_limit if is_test else config.para_limit\n        ques_limit = config.test_ques_limit if is_test else config.ques_limit\n        char_limit = config.char_limit\n        features = tf.parse_single_example(example,\n                                           features={\n                                               ""context_idxs"": tf.FixedLenFeature([], tf.string),\n                                               ""ques_idxs"": tf.FixedLenFeature([], tf.string),\n                                               ""context_char_idxs"": tf.FixedLenFeature([], tf.string),\n                                               ""ques_char_idxs"": tf.FixedLenFeature([], tf.string),\n                                               ""y1"": tf.FixedLenFeature([], tf.string),\n                                               ""y2"": tf.FixedLenFeature([], tf.string),\n                                               ""id"": tf.FixedLenFeature([], tf.int64)\n                                           })\n        context_idxs = tf.reshape(tf.decode_raw(\n            features[""context_idxs""], tf.int32), [para_limit])\n        ques_idxs = tf.reshape(tf.decode_raw(\n            features[""ques_idxs""], tf.int32), [ques_limit])\n        context_char_idxs = tf.reshape(tf.decode_raw(\n            features[""context_char_idxs""], tf.int32), [para_limit, char_limit])\n        ques_char_idxs = tf.reshape(tf.decode_raw(\n            features[""ques_char_idxs""], tf.int32), [ques_limit, char_limit])\n        y1 = tf.reshape(tf.decode_raw(\n            features[""y1""], tf.float32), [para_limit])\n        y2 = tf.reshape(tf.decode_raw(\n            features[""y2""], tf.float32), [para_limit])\n        qa_id = features[""id""]\n        return context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, y1, y2, qa_id\n    return parse\n\n\ndef get_batch_dataset(record_file, parser, config):\n    num_threads = tf.constant(config.num_threads, dtype=tf.int32)\n    dataset = tf.data.TFRecordDataset(record_file).map(\n        parser, num_parallel_calls=num_threads).shuffle(config.capacity).repeat()\n    if config.is_bucket:\n        buckets = [tf.constant(num) for num in range(*config.bucket_range)]\n\n        def key_func(context_idxs, ques_idxs, context_char_idxs, ques_char_idxs, y1, y2, qa_id):\n            c_len = tf.reduce_sum(\n                tf.cast(tf.cast(context_idxs, tf.bool), tf.int32))\n            buckets_min = [np.iinfo(np.int32).min] + buckets\n            buckets_max = buckets + [np.iinfo(np.int32).max]\n            conditions_c = tf.logical_and(\n                tf.less(buckets_min, c_len), tf.less_equal(c_len, buckets_max))\n            bucket_id = tf.reduce_min(tf.where(conditions_c))\n            return bucket_id\n\n        def reduce_func(key, elements):\n            return elements.batch(config.batch_size)\n\n        dataset = dataset.apply(tf.contrib.data.group_by_window(\n            key_func, reduce_func, window_size=5 * config.batch_size)).shuffle(len(buckets) * 25)\n    else:\n        dataset = dataset.batch(config.batch_size)\n    return dataset\n\n\ndef get_dataset(record_file, parser, config):\n    num_threads = tf.constant(config.num_threads, dtype=tf.int32)\n    dataset = tf.data.TFRecordDataset(record_file).map(\n        parser, num_parallel_calls=num_threads).repeat().batch(config.batch_size)\n    return dataset\n\n\ndef convert_tokens(eval_file, qa_id, pp1, pp2):\n    answer_dict = {}\n    remapped_dict = {}\n    for qid, p1, p2 in zip(qa_id, pp1, pp2):\n        context = eval_file[str(qid)][""context""]\n        spans = eval_file[str(qid)][""spans""]\n        uuid = eval_file[str(qid)][""uuid""]\n        start_idx = spans[p1][0]\n        end_idx = spans[p2][1]\n        answer_dict[str(qid)] = context[start_idx: end_idx]\n        remapped_dict[uuid] = context[start_idx: end_idx]\n    return answer_dict, remapped_dict\n\n\ndef evaluate(eval_file, answer_dict):\n    f1 = exact_match = total = 0\n    for key, value in answer_dict.items():\n        total += 1\n        ground_truths = eval_file[key][""answers""]\n        prediction = value\n        exact_match += metric_max_over_ground_truths(\n            exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(f1_score,\n                                            prediction, ground_truths)\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\ndef normalize_answer(s):\n\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n'"
