file_path,api_count,code
visuals.py,10,"b'###########################################\n# Suppress matplotlib user warnings\n# Necessary for newer version of matplotlib\nimport warnings\nwarnings.filterwarnings(""ignore"", category = UserWarning, module = ""matplotlib"")\n#\n# Display inline matplotlib plots with IPython\nfrom IPython import get_ipython\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n###########################################\n\nimport matplotlib.pyplot as pl\nimport numpy as np\nimport sklearn.learning_curve as curves\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.cross_validation import ShuffleSplit, train_test_split\n\ndef ModelLearning(X, y):\n    """""" Calculates the performance of several models with varying sizes of training data.\n        The learning and testing scores for each model are then plotted. """"""\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\n\n    # Generate the training set sizes increasing by 50\n    train_sizes = np.rint(np.linspace(1, X.shape[0]*0.8 - 1, 9)).astype(int)\n\n    # Create the figure window\n    fig = pl.figure(figsize=(10,7))\n\n    # Create three different models based on max_depth\n    for k, depth in enumerate([1,3,6,10]):\n        \n        # Create a Decision tree regressor at max_depth = depth\n        regressor = DecisionTreeRegressor(max_depth = depth)\n\n        # Calculate the training and testing scores\n        sizes, train_scores, test_scores = curves.learning_curve(regressor, X, y, \\\n            cv = cv, train_sizes = train_sizes, scoring = \'r2\')\n        \n        # Find the mean and standard deviation for smoothing\n        train_std = np.std(train_scores, axis = 1)\n        train_mean = np.mean(train_scores, axis = 1)\n        test_std = np.std(test_scores, axis = 1)\n        test_mean = np.mean(test_scores, axis = 1)\n\n        # Subplot the learning curve \n        ax = fig.add_subplot(2, 2, k+1)\n        ax.plot(sizes, train_mean, \'o-\', color = \'r\', label = \'Training Score\')\n        ax.plot(sizes, test_mean, \'o-\', color = \'g\', label = \'Testing Score\')\n        ax.fill_between(sizes, train_mean - train_std, \\\n            train_mean + train_std, alpha = 0.15, color = \'r\')\n        ax.fill_between(sizes, test_mean - test_std, \\\n            test_mean + test_std, alpha = 0.15, color = \'g\')\n        \n        # Labels\n        ax.set_title(\'max_depth = %s\'%(depth))\n        ax.set_xlabel(\'Number of Training Points\')\n        ax.set_ylabel(\'Score\')\n        ax.set_xlim([0, X.shape[0]*0.8])\n        ax.set_ylim([-0.05, 1.05])\n    \n    # Visual aesthetics\n    ax.legend(bbox_to_anchor=(1.05, 2.05), loc=\'lower left\', borderaxespad = 0.)\n    fig.suptitle(\'Decision Tree Regressor Learning Performances\', fontsize = 16, y = 1.03)\n    fig.tight_layout()\n    fig.show()\n\n\ndef ModelComplexity(X, y):\n    """""" Calculates the performance of the model as model complexity increases.\n        The learning and testing errors rates are then plotted. """"""\n    \n    # Create 10 cross-validation sets for training and testing\n    cv = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.2, random_state = 0)\n\n    # Vary the max_depth parameter from 1 to 10\n    max_depth = np.arange(1,11)\n\n    # Calculate the training and testing scores\n    train_scores, test_scores = curves.validation_curve(DecisionTreeRegressor(), X, y, \\\n        param_name = ""max_depth"", param_range = max_depth, cv = cv, scoring = \'r2\')\n\n    # Find the mean and standard deviation for smoothing\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot the validation curve\n    pl.figure(figsize=(7, 5))\n    pl.title(\'Decision Tree Regressor Complexity Performance\')\n    pl.plot(max_depth, train_mean, \'o-\', color = \'r\', label = \'Training Score\')\n    pl.plot(max_depth, test_mean, \'o-\', color = \'g\', label = \'Validation Score\')\n    pl.fill_between(max_depth, train_mean - train_std, \\\n        train_mean + train_std, alpha = 0.15, color = \'r\')\n    pl.fill_between(max_depth, test_mean - test_std, \\\n        test_mean + test_std, alpha = 0.15, color = \'g\')\n    \n    # Visual aesthetics\n    pl.legend(loc = \'lower right\')\n    pl.xlabel(\'Maximum Depth\')\n    pl.ylabel(\'Score\')\n    pl.ylim([-0.05,1.05])\n    pl.show()\n\n\ndef PredictTrials(X, y, fitter, data):\n    """""" Performs trials of fitting and predicting data. """"""\n\n    # Store the predicted prices\n    prices = []\n\n    for k in range(10):\n        # Split the data\n        X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n            test_size = 0.2, random_state = k)\n        \n        # Fit the data\n        reg = fitter(X_train, y_train)\n        \n        # Make a prediction\n        pred = reg.predict([data[0]])[0]\n        prices.append(pred)\n        \n        # Result\n        print ""Trial {}: ${:,.2f}"".format(k+1, pred)\n\n    # Display price range\n    print ""\\nRange in prices: ${:,.2f}"".format(max(prices) - min(prices))'"
