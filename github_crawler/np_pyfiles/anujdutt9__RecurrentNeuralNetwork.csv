file_path,api_count,code
RNN.py,23,"b'# Recurrent Neural Network from Scratch in Python 3\n\nimport copy\nimport numpy as np\n\n# np.random.seed(0)\n\n# Sigmoid Activation Function\n# To be applied at Hidden Layers and Output Layer\ndef sigmoid(z):\n    return (1 / (1 + np.exp(-z)))\n\n# Derivative of Sigmoid Function\n# Used in calculation of Back Propagation Loss\ndef sigmoidPrime(z):\n    return z * (1-z)\n\n\n# Generate Input Dataset\nint_to_binary = {}\nbinary_dim = 8\n\n# Calculate the largest value which can be attained\n# 2^8 = 256\nmax_val = (2**binary_dim)\n\n# Calculate Binary values for int from 0 to 256\nbinary_val = np.unpackbits(np.array([range(max_val)], dtype=np.uint8).T, axis=1)\n\n# Function to map Integer values to Binary values\nfor i in range(max_val):\n    int_to_binary[i] = binary_val[i]\n    # print(\'\\nInteger value: \',i)\n    # print(\'binary value: \', binary_val[i])\n\n\n# NN variables\nlearning_rate = 0.1\n\n# Inputs: Values to be added bit by bit\ninputLayerSize = 2\n\n# Hidden Layer with 16 neurons\nhiddenLayerSize = 16\n\n# Output at one time step is 1 bit\noutputLayerSize = 1\n\n# Initialize Weights\n# Weight of first Synapse (Synapse_0) from Input to Hidden Layer at Current Timestep\nW1 = 2 * np.random.random((inputLayerSize, hiddenLayerSize)) - 1\n\n# Weight of second Synapse (Synapse_1) from Hidden Layer to Output Layer\nW2 = 2 * np.random.random((hiddenLayerSize, outputLayerSize)) - 1\n\n# Weight of Synapse (Synapse_h) from Current Hidden Layer to Next Hidden Layer in Timestep\nW_h = 2 * np.random.random((hiddenLayerSize, hiddenLayerSize)) - 1\n\n\n# Initialize Updated Weights Values\nW1_update = np.zeros_like(W1)\nW2_update = np.zeros_like(W2)\nW_h_update = np.zeros_like(W_h)\n\n\n# Iterate over 10,000 samples for Training\nfor j in range(10000):\n    # ----------------------------- Compute True Values for the Sum (a+b) [binary encoded] --------------------------\n    # Generate a random sample value for 1st input\n    a_int = np.random.randint(max_val/2)\n    # Convert this Int value to Binary\n    a = int_to_binary[a_int]\n\n    # Generate a random sample value for 2nd input\n    b_int = np.random.randint(max_val/2)\n    # Map Int to Binary\n    b = int_to_binary[b_int]\n\n    # True Answer a + b = c\n    c_int = a_int + b_int\n    c = int_to_binary[c_int]\n\n    # Array to save predicted outputs (binary encoded)\n    d = np.zeros_like(c)\n\n    # Initialize overall error to ""0""\n    overallError = 0\n\n    # Save the values of dJdW1 and dJdW2 computed at Output layer into a list\n    output_layer_deltas = list()\n\n    # Save the values obtained at Hidden Layer of current state in a list to keep track\n    hidden_layer_values = list()\n\n    # Initially, there is no previous hidden state. So append ""0"" for that\n    hidden_layer_values.append(np.zeros(hiddenLayerSize))\n\n    # ----------------------------- Compute the Values for (a+b) using RNN [Forward Propagation] ----------------------\n    # position: location of the bit amongst 8 bits; starting point ""0""; ""0 - 7""\n    for position in range(binary_dim):\n        # Generate Input Data for RNN\n        # Take the binary values of ""a"" and ""b"" generated for each iteration of ""j""\n\n        # With increasing value of position, the bit location of ""a"" and ""b"" decreases from ""7 -> 0""\n        # and each iteration computes the sum of corresponding bit of ""a"" and ""b"".\n        # ex. for position = 0, X = [a[7],b[7]], 7th bit of a and b.\n        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n\n        # Actual value for (a+b) = c, c is an array of 8 bits, so take transpose to compare bit by bit with X value.\n        y = np.array([[c[binary_dim - position - 1]]]).T\n\n        # Values computed at current hidden layer\n        # [dot product of Input(X) and Weights(W1)] + [dot product of previous hidden layer values and Weights (W_h)]\n        # W_h: weight from previous step hidden layer to current step hidden layer\n        # W1: weights from current step input to current hidden layer\n        layer_1 = sigmoid(np.dot(X,W1) + np.dot(hidden_layer_values[-1],W_h))\n\n        # The new output using new Hidden layer values\n        layer_2 = sigmoid(np.dot(layer_1, W2))\n\n        # Calculate the error\n        output_error = y - layer_2\n\n        # Save the error deltas at each step as it will be propagated back\n        output_layer_deltas.append((output_error)*sigmoidPrime(layer_2))\n\n        # Save the sum of error at each binary position\n        overallError += np.abs(output_error[0])\n\n        # Round off the values to nearest ""0"" or ""1"" and save it to a list\n        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n\n        # Save the hidden layer to be used later\n        hidden_layer_values.append(copy.deepcopy(layer_1))\n\n    future_layer_1_delta = np.zeros(hiddenLayerSize)\n\n# ----------------------------------- Back Propagating the Error Values to All Previous Time-steps ---------------------\n    for position in range(binary_dim):\n        # a[0], b[0] -> a[1]b[1] ....\n        X = np.array([[a[position], b[position]]])\n        # The last step Hidden Layer where we are currently a[0],b[0]\n        layer_1 = hidden_layer_values[-position - 1]\n        # The hidden layer before the current layer, a[1],b[1]\n        prev_hidden_layer = hidden_layer_values[-position-2]\n        # Errors at Output Layer, a[1],b[1]\n        output_layer_delta = output_layer_deltas[-position-1]\n        layer_1_delta = (future_layer_1_delta.dot(W_h.T) + output_layer_delta.dot(W2.T)) * sigmoidPrime(layer_1)\n\n        # Update all the weights and try again\n        W2_update += np.atleast_2d(layer_1).T.dot(output_layer_delta)\n        W_h_update += np.atleast_2d(prev_hidden_layer).T.dot(layer_1_delta)\n        W1_update += X.T.dot(layer_1_delta)\n\n        future_layer_1_delta = layer_1_delta\n\n    # Update the weights with the values\n    W1 += W1_update * learning_rate\n    W2 += W2_update * learning_rate\n    W_h += W_h_update * learning_rate\n\n    # Clear the updated weights values\n    W1_update *= 0\n    W2_update *= 0\n    W_h_update *= 0\n\n\n    # Print out the Progress of the RNN\n    if (j % 1000 == 0):\n        print(""Error:"" + str(overallError))\n        print(""Pred:"" + str(d))\n        print(""True:"" + str(c))\n        out = 0\n        for index, x in enumerate(reversed(d)):\n            out += x * pow(2, index)\n        print(str(a_int) + "" + "" + str(b_int) + "" = "" + str(out))\n        print(""------------"")\n\n# ------------------------------------- EOC -----------------------------'"
