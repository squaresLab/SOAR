file_path,api_count,code
__init__.py,0,b''
bbox.py,11,"b'from __future__ import division\n\nimport torch \nimport random\n\nimport numpy as np\nimport cv2\n\ndef confidence_filter(result, confidence):\n    conf_mask = (result[:,:,4] > confidence).float().unsqueeze(2)\n    result = result*conf_mask    \n    \n    return result\n\ndef confidence_filter_cls(result, confidence):\n    max_scores = torch.max(result[:,:,5:25], 2)[0]\n    res = torch.cat((result, max_scores),2)\n    print(res.shape)\n    \n    \n    cond_1 = (res[:,:,4] > confidence).float()\n    cond_2 = (res[:,:,25] > 0.995).float()\n    \n    conf = cond_1 + cond_2\n    conf = torch.clamp(conf, 0.0, 1.0)\n    conf = conf.unsqueeze(2)\n    result = result*conf   \n    return result\n\n\n\ndef get_abs_coord(box):\n    box[2], box[3] = abs(box[2]), abs(box[3])\n    x1 = (box[0] - box[2]/2) - 1 \n    y1 = (box[1] - box[3]/2) - 1 \n    x2 = (box[0] + box[2]/2) - 1 \n    y2 = (box[1] + box[3]/2) - 1\n    return x1, y1, x2, y2\n    \n\n\ndef sanity_fix(box):\n    if (box[0] > box[2]):\n        box[0], box[2] = box[2], box[0]\n    \n    if (box[1] >  box[3]):\n        box[1], box[3] = box[3], box[1]\n        \n    return box\n\ndef bbox_iou(box1, box2):\n    """"""\n    Returns the IoU of two bounding boxes \n    \n    \n    """"""\n    #Get the coordinates of bounding boxes\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n    \n    #get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    \n    #Intersection area\n    if torch.cuda.is_available():\n            inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape).cuda())*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape).cuda())\n    else:\n            inter_area = torch.max(inter_rect_x2 - inter_rect_x1 + 1,torch.zeros(inter_rect_x2.shape))*torch.max(inter_rect_y2 - inter_rect_y1 + 1, torch.zeros(inter_rect_x2.shape))\n    \n    #Union Area\n    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n    \n    iou = inter_area / (b1_area + b2_area - inter_area)\n    \n    return iou\n\n\ndef pred_corner_coord(prediction):\n    #Get indices of non-zero confidence bboxes\n    ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    \n    box = prediction[ind_nz[0], ind_nz[1]]\n    \n    \n    box_a = box.new(box.shape)\n    box_a[:,0] = (box[:,0] - box[:,2]/2)\n    box_a[:,1] = (box[:,1] - box[:,3]/2)\n    box_a[:,2] = (box[:,0] + box[:,2]/2) \n    box_a[:,3] = (box[:,1] + box[:,3]/2)\n    box[:,:4] = box_a[:,:4]\n    \n    prediction[ind_nz[0], ind_nz[1]] = box\n    \n    return prediction\n\n\n\n\ndef write(x, batches, results, colors, classes):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    img = results[int(x[0])]\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n'"
cam_demo.py,7,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport argparse\nimport pickle as pkl\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""imgs/messi.jpg"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = cv2.resize(orig_im, (inp_dim, inp_dim))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v3 Cam Demo\')\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.25)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""160"", type = str)\n    return parser.parse_args()\n\n\n\nif __name__ == \'__main__\':\n    cfgfile = ""cfg/yolov3.cfg""\n    weightsfile = ""yolov3.weights""\n    num_classes = 80\n\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n    CUDA = torch.cuda.is_available()\n    \n\n    \n    \n    num_classes = 80\n    bbox_attrs = 5 + num_classes\n    \n    model = Darknet(cfgfile)\n    model.load_weights(weightsfile)\n    \n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    \n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n            \n    model.eval()\n    \n    videofile = \'video.avi\'\n    \n    cap = cv2.VideoCapture(0)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n#            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            \n            output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n            \n\n        \n            output[:,1:5] = torch.clamp(output[:,1:5], 0.0, float(inp_dim))/inp_dim\n            \n#            im_dim = im_dim.repeat(output.size(0), 1)\n            output[:,[1,3]] *= frame.shape[1]\n            output[:,[2,4]] *= frame.shape[0]\n\n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
darknet.py,14,"b'from __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom util import count_parameters as count\nfrom util import convert2cpu as cpu\nfrom util import predict_transform\n\nclass test_net(nn.Module):\n    def __init__(self, num_layers, input_size):\n        super(test_net, self).__init__()\n        self.num_layers= num_layers\n        self.linear_1 = nn.Linear(input_size, 5)\n        self.middle = nn.ModuleList([nn.Linear(5,5) for x in range(num_layers)])\n        self.output = nn.Linear(5,2)\n    \n    def forward(self, x):\n        x = x.view(-1)\n        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)\n        return fwd(x)\n        \ndef get_test_input():\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (416,416)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    return img_\n\n\ndef parse_cfg(cfgfile):\n    """"""\n    Takes a configuration file\n    \n    Returns a list of blocks. Each blocks describes a block in the neural\n    network to be built. Block is represented as a dictionary in the list\n    \n    """"""\n    file = open(cfgfile, \'r\')\n    lines = file.read().split(\'\\n\')     #store the lines in a list\n    lines = [x for x in lines if len(x) > 0] #get read of the empty lines \n    lines = [x for x in lines if x[0] != \'#\']  \n    lines = [x.rstrip().lstrip() for x in lines]\n\n    \n    block = {}\n    blocks = []\n    \n    for line in lines:\n        if line[0] == ""["":               #This marks the start of a new block\n            if len(block) != 0:\n                blocks.append(block)\n                block = {}\n            block[""type""] = line[1:-1].rstrip()\n        else:\n            key,value = line.split(""="")\n            block[key.rstrip()] = value.lstrip()\n    blocks.append(block)\n\n    return blocks\n#    print(\'\\n\\n\'.join([repr(x) for x in blocks]))\n\nimport pickle as pkl\n\nclass MaxPoolStride1(nn.Module):\n    def __init__(self, kernel_size):\n        super(MaxPoolStride1, self).__init__()\n        self.kernel_size = kernel_size\n        self.pad = kernel_size - 1\n    \n    def forward(self, x):\n        padded_x = F.pad(x, (0,self.pad,0,self.pad), mode=""replicate"")\n        pooled_x = nn.MaxPool2d(self.kernel_size, self.pad)(padded_x)\n        return pooled_x\n    \n\nclass EmptyLayer(nn.Module):\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n        \n\nclass DetectionLayer(nn.Module):\n    def __init__(self, anchors):\n        super(DetectionLayer, self).__init__()\n        self.anchors = anchors\n    \n    def forward(self, x, inp_dim, num_classes, confidence):\n        x = x.data\n        global CUDA\n        prediction = x\n        prediction = predict_transform(prediction, inp_dim, self.anchors, num_classes, confidence, CUDA)\n        return prediction\n        \n\n        \n\n\nclass Upsample(nn.Module):\n    def __init__(self, stride=2):\n        super(Upsample, self).__init__()\n        self.stride = stride\n        \n    def forward(self, x):\n        stride = self.stride\n        assert(x.data.dim() == 4)\n        B = x.data.size(0)\n        C = x.data.size(1)\n        H = x.data.size(2)\n        W = x.data.size(3)\n        ws = stride\n        hs = stride\n        x = x.view(B, C, H, 1, W, 1).expand(B, C, H, stride, W, stride).contiguous().view(B, C, H*stride, W*stride)\n        return x\n#       \n        \nclass ReOrgLayer(nn.Module):\n    def __init__(self, stride = 2):\n        super(ReOrgLayer, self).__init__()\n        self.stride= stride\n        \n    def forward(self,x):\n        assert(x.data.dim() == 4)\n        B,C,H,W = x.data.shape\n        hs = self.stride\n        ws = self.stride\n        assert(H % hs == 0),  ""The stride "" + str(self.stride) + "" is not a proper divisor of height "" + str(H)\n        assert(W % ws == 0),  ""The stride "" + str(self.stride) + "" is not a proper divisor of height "" + str(W)\n        x = x.view(B,C, H // hs, hs, W // ws, ws).transpose(-2,-3).contiguous()\n        x = x.view(B,C, H // hs * W // ws, hs, ws)\n        x = x.view(B,C, H // hs * W // ws, hs*ws).transpose(-1,-2).contiguous()\n        x = x.view(B, C, ws*hs, H // ws, W // ws).transpose(1,2).contiguous()\n        x = x.view(B, C*ws*hs, H // ws, W // ws)\n        return x\n\n\ndef create_modules(blocks):\n    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n    \n    module_list = nn.ModuleList()\n    \n    index = 0    #indexing blocks helps with implementing route  layers (skip connections)\n\n    \n    prev_filters = 3\n    \n    output_filters = []\n    \n    for x in blocks:\n        module = nn.Sequential()\n        \n        if (x[""type""] == ""net""):\n            continue\n        \n        #If it\'s a convolutional layer\n        if (x[""type""] == ""convolutional""):\n            #Get the info about the layer\n            activation = x[""activation""]\n            try:\n                batch_normalize = int(x[""batch_normalize""])\n                bias = False\n            except:\n                batch_normalize = 0\n                bias = True\n                \n            filters= int(x[""filters""])\n            padding = int(x[""pad""])\n            kernel_size = int(x[""size""])\n            stride = int(x[""stride""])\n            \n            if padding:\n                pad = (kernel_size - 1) // 2\n            else:\n                pad = 0\n                \n            #Add the convolutional layer\n            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n            module.add_module(""conv_{0}"".format(index), conv)\n            \n            #Add the Batch Norm Layer\n            if batch_normalize:\n                bn = nn.BatchNorm2d(filters)\n                module.add_module(""batch_norm_{0}"".format(index), bn)\n            \n            #Check the activation. \n            #It is either Linear or a Leaky ReLU for YOLO\n            if activation == ""leaky"":\n                activn = nn.LeakyReLU(0.1, inplace = True)\n                module.add_module(""leaky_{0}"".format(index), activn)\n            \n            \n            \n        #If it\'s an upsampling layer\n        #We use Bilinear2dUpsampling\n        \n        elif (x[""type""] == ""upsample""):\n            stride = int(x[""stride""])\n#            upsample = Upsample(stride)\n            upsample = nn.Upsample(scale_factor = 2, mode = ""nearest"")\n            module.add_module(""upsample_{}"".format(index), upsample)\n        \n        #If it is a route layer\n        elif (x[""type""] == ""route""):\n            x[""layers""] = x[""layers""].split(\',\')\n            \n            #Start  of a route\n            start = int(x[""layers""][0])\n            \n            #end, if there exists one.\n            try:\n                end = int(x[""layers""][1])\n            except:\n                end = 0\n                \n            \n            \n            #Positive anotation\n            if start > 0: \n                start = start - index\n            \n            if end > 0:\n                end = end - index\n\n            \n            route = EmptyLayer()\n            module.add_module(""route_{0}"".format(index), route)\n            \n            \n            \n            if end < 0:\n                filters = output_filters[index + start] + output_filters[index + end]\n            else:\n                filters= output_filters[index + start]\n                        \n            \n        \n        #shortcut corresponds to skip connection\n        elif x[""type""] == ""shortcut"":\n            from_ = int(x[""from""])\n            shortcut = EmptyLayer()\n            module.add_module(""shortcut_{}"".format(index), shortcut)\n            \n            \n        elif x[""type""] == ""maxpool"":\n            stride = int(x[""stride""])\n            size = int(x[""size""])\n            if stride != 1:\n                maxpool = nn.MaxPool2d(size, stride)\n            else:\n                maxpool = MaxPoolStride1(size)\n            \n            module.add_module(""maxpool_{}"".format(index), maxpool)\n        \n        #Yolo is the detection layer\n        elif x[""type""] == ""yolo"":\n            mask = x[""mask""].split("","")\n            mask = [int(x) for x in mask]\n            \n            \n            anchors = x[""anchors""].split("","")\n            anchors = [int(a) for a in anchors]\n            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n            anchors = [anchors[i] for i in mask]\n            \n            detection = DetectionLayer(anchors)\n            module.add_module(""Detection_{}"".format(index), detection)\n        \n            \n            \n        else:\n            print(""Something I dunno"")\n            assert False\n\n\n        module_list.append(module)\n        prev_filters = filters\n        output_filters.append(filters)\n        index += 1\n        \n    \n    return (net_info, module_list)\n\n\n\nclass Darknet(nn.Module):\n    def __init__(self, cfgfile):\n        super(Darknet, self).__init__()\n        self.blocks = parse_cfg(cfgfile)\n        self.net_info, self.module_list = create_modules(self.blocks)\n        self.header = torch.IntTensor([0,0,0,0])\n        self.seen = 0\n\n        \n        \n    def get_blocks(self):\n        return self.blocks\n    \n    def get_module_list(self):\n        return self.module_list\n\n                \n    def forward(self, x, CUDA):\n        detections = []\n        modules = self.blocks[1:]\n        outputs = {}   #We cache the outputs for the route layer\n        \n        \n        write = 0\n        for i in range(len(modules)):        \n            \n            module_type = (modules[i][""type""])\n            if module_type == ""convolutional"" or module_type == ""upsample"" or module_type == ""maxpool"":\n                \n                x = self.module_list[i](x)\n                outputs[i] = x\n\n                \n            elif module_type == ""route"":\n                layers = modules[i][""layers""]\n                layers = [int(a) for a in layers]\n                \n                if (layers[0]) > 0:\n                    layers[0] = layers[0] - i\n\n                if len(layers) == 1:\n                    x = outputs[i + (layers[0])]\n\n                else:\n                    if (layers[1]) > 0:\n                        layers[1] = layers[1] - i\n                        \n                    map1 = outputs[i + layers[0]]\n                    map2 = outputs[i + layers[1]]\n                    \n                    \n                    x = torch.cat((map1, map2), 1)\n                outputs[i] = x\n            \n            elif  module_type == ""shortcut"":\n                from_ = int(modules[i][""from""])\n                x = outputs[i-1] + outputs[i+from_]\n                outputs[i] = x\n                \n            \n            \n            elif module_type == \'yolo\':        \n                \n                anchors = self.module_list[i][0].anchors\n                #Get the input dimensions\n                inp_dim = int (self.net_info[""height""])\n                \n                #Get the number of classes\n                num_classes = int (modules[i][""classes""])\n                \n                #Output the result\n                x = x.data\n                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n                \n                if type(x) == int:\n                    continue\n\n                \n                if not write:\n                    detections = x\n                    write = 1\n                \n                else:\n                    detections = torch.cat((detections, x), 1)\n                \n                outputs[i] = outputs[i-1]\n                \n        \n        \n        try:\n            return detections\n        except:\n            return 0\n\n            \n    def load_weights(self, weightfile):\n        \n        #Open the weights file\n        fp = open(weightfile, ""rb"")\n\n        #The first 4 values are header information \n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number \n        # 4. IMages seen \n        header = np.fromfile(fp, dtype = np.int32, count = 5)\n        self.header = torch.from_numpy(header)\n        self.seen = self.header[3]\n        \n        #The rest of the values are the weights\n        # Let\'s load them up\n        weights = np.fromfile(fp, dtype = np.float32)\n        \n        ptr = 0\n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i + 1][""type""]\n            \n            if module_type == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                \n                conv = model[0]\n                \n                if (batch_normalize):\n                    bn = model[1]\n                    \n                    #Get the number of weights of Batch Norm Layer\n                    num_bn_biases = bn.bias.numel()\n                    \n                    #Load the weights\n                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n                    ptr += num_bn_biases\n                    \n                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n                    ptr  += num_bn_biases\n                    \n                    #Cast the loaded weights into dims of model weights. \n                    bn_biases = bn_biases.view_as(bn.bias.data)\n                    bn_weights = bn_weights.view_as(bn.weight.data)\n                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n                    bn_running_var = bn_running_var.view_as(bn.running_var)\n\n                    #Copy the data to model\n                    bn.bias.data.copy_(bn_biases)\n                    bn.weight.data.copy_(bn_weights)\n                    bn.running_mean.copy_(bn_running_mean)\n                    bn.running_var.copy_(bn_running_var)\n                \n                else:\n                    #Number of biases\n                    num_biases = conv.bias.numel()\n                \n                    #Load the weights\n                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n                    ptr = ptr + num_biases\n                    \n                    #reshape the loaded weights according to the dims of the model weights\n                    conv_biases = conv_biases.view_as(conv.bias.data)\n                    \n                    #Finally copy the data\n                    conv.bias.data.copy_(conv_biases)\n                    \n                    \n                #Let us load the weights for the Convolutional layers\n                num_weights = conv.weight.numel()\n                \n                #Do the same as above for weights\n                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n                ptr = ptr + num_weights\n\n                conv_weights = conv_weights.view_as(conv.weight.data)\n                conv.weight.data.copy_(conv_weights)\n                \n    def save_weights(self, savedfile, cutoff = 0):\n            \n        if cutoff <= 0:\n            cutoff = len(self.blocks) - 1\n        \n        fp = open(savedfile, \'wb\')\n        \n        # Attach the header at the top of the file\n        self.header[3] = self.seen\n        header = self.header\n\n        header = header.numpy()\n        header.tofile(fp)\n        \n        # Now, let us save the weights \n        for i in range(len(self.module_list)):\n            module_type = self.blocks[i+1][""type""]\n            \n            if (module_type) == ""convolutional"":\n                model = self.module_list[i]\n                try:\n                    batch_normalize = int(self.blocks[i+1][""batch_normalize""])\n                except:\n                    batch_normalize = 0\n                    \n                conv = model[0]\n\n                if (batch_normalize):\n                    bn = model[1]\n                \n                    #If the parameters are on GPU, convert them back to CPU\n                    #We don\'t convert the parameter to GPU\n                    #Instead. we copy the parameter and then convert it to CPU\n                    #This is done as weight are need to be saved during training\n                    cpu(bn.bias.data).numpy().tofile(fp)\n                    cpu(bn.weight.data).numpy().tofile(fp)\n                    cpu(bn.running_mean).numpy().tofile(fp)\n                    cpu(bn.running_var).numpy().tofile(fp)\n                \n            \n                else:\n                    cpu(conv.bias.data).numpy().tofile(fp)\n                \n                \n                #Let us save the weights for the Convolutional layers\n                cpu(conv.weight.data).numpy().tofile(fp)\n               \n\n\n\n\n#\n#dn = Darknet(\'cfg/yolov3.cfg\')\n#dn.load_weights(""yolov3.weights"")\n#inp = get_test_input()\n#a, interms = dn(inp)\n#dn.eval()\n#a_i, interms_i = dn(inp)\n'"
detect.py,14,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nimport argparse\nimport os \nimport os.path as osp\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport itertools\n\nclass test_net(nn.Module):\n    def __init__(self, num_layers, input_size):\n        super(test_net, self).__init__()\n        self.num_layers= num_layers\n        self.linear_1 = nn.Linear(input_size, 5)\n        self.middle = nn.ModuleList([nn.Linear(5,5) for x in range(num_layers)])\n        self.output = nn.Linear(5,2)\n    \n    def forward(self, x):\n        x = x.view(-1)\n        fwd = nn.Sequential(self.linear_1, *self.middle, self.output)\n        return fwd(x)\n        \ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    num_classes\n    return img_\n\n\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v3 Detection Module\')\n   \n    parser.add_argument(""--images"", dest = \'images\', help = \n                        ""Image / Directory containing images to perform detection upon"",\n                        default = ""imgs"", type = str)\n    parser.add_argument(""--det"", dest = \'det\', help = \n                        ""Image / Directory to store detections to"",\n                        default = ""det"", type = str)\n    parser.add_argument(""--bs"", dest = ""bs"", help = ""Batch size"", default = 1)\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.5)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--cfg"", dest = \'cfgfile\', help = \n                        ""Config file"",\n                        default = ""cfg/yolov3.cfg"", type = str)\n    parser.add_argument(""--weights"", dest = \'weightsfile\', help = \n                        ""weightsfile"",\n                        default = ""yolov3.weights"", type = str)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""416"", type = str)\n    parser.add_argument(""--scales"", dest = ""scales"", help = ""Scales to use for detection"",\n                        default = ""1,2,3"", type = str)\n    \n    return parser.parse_args()\n\nif __name__ ==  \'__main__\':\n    args = arg_parse()\n    \n    scales = args.scales\n    \n    \n#        scales = [int(x) for x in scales.split(\',\')]\n#        \n#        \n#        \n#        args.reso = int(args.reso)\n#        \n#        num_boxes = [args.reso//32, args.reso//16, args.reso//8]    \n#        scale_indices = [3*(x**2) for x in num_boxes]\n#        scale_indices = list(itertools.accumulate(scale_indices, lambda x,y : x+y))\n#    \n#        \n#        li = []\n#        i = 0\n#        for scale in scale_indices:        \n#            li.extend(list(range(i, scale))) \n#            i = scale\n#        \n#        scale_indices = li\n\n    images = args.images\n    batch_size = int(args.bs)\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n    classes = load_classes(\'data/coco.names\') \n\n    #Set up the neural network\n    print(""Loading network....."")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(""Network successfully loaded"")\n    \n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    #If there\'s a GPU availible, put the model on GPU\n    if CUDA:\n        model.cuda()\n    \n    \n    #Set the model in evaluation mode\n    model.eval()\n    \n    read_dir = time.time()\n    #Detection phase\n    try:\n        imlist = [osp.join(osp.realpath(\'.\'), images, img) for img in os.listdir(images) if os.path.splitext(img)[1] == \'.png\' or os.path.splitext(img)[1] ==\'.jpeg\' or os.path.splitext(img)[1] ==\'.jpg\']\n    except NotADirectoryError:\n        imlist = []\n        imlist.append(osp.join(osp.realpath(\'.\'), images))\n    except FileNotFoundError:\n        print (""No file or directory with the name {}"".format(images))\n        exit()\n        \n    if not os.path.exists(args.det):\n        os.makedirs(args.det)\n        \n    load_batch = time.time()\n    \n    batches = list(map(prep_image, imlist, [inp_dim for x in range(len(imlist))]))\n    im_batches = [x[0] for x in batches]\n    orig_ims = [x[1] for x in batches]\n    im_dim_list = [x[2] for x in batches]\n    im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n    \n    \n    \n    if CUDA:\n        im_dim_list = im_dim_list.cuda()\n    \n    leftover = 0\n    \n    if (len(im_dim_list) % batch_size):\n        leftover = 1\n        \n        \n    if batch_size != 1:\n        num_batches = len(imlist) // batch_size + leftover            \n        im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n                            len(im_batches))]))  for i in range(num_batches)]        \n\n\n    i = 0\n    \n\n    write = False\n    model(get_test_input(inp_dim, CUDA), CUDA)\n    \n    start_det_loop = time.time()\n    \n    objs = {}\n    \n    \n    \n    for batch in im_batches:\n        #load the image \n        start = time.time()\n        if CUDA:\n            batch = batch.cuda()\n        \n\n        #Apply offsets to the result predictions\n        #Tranform the predictions as described in the YOLO paper\n        #flatten the prediction vector \n        # B x (bbox cord x no. of anchors) x grid_w x grid_h --> B x bbox x (all the boxes) \n        # Put every proposed box as a row.\n        with torch.no_grad():\n            prediction = model(Variable(batch), CUDA)\n        \n#        prediction = prediction[:,scale_indices]\n\n        \n        #get the boxes with object confidence > threshold\n        #Convert the cordinates to absolute coordinates\n        #perform NMS on these boxes, and save the results \n        #I could have done NMS and saving seperately to have a better abstraction\n        #But both these operations require looping, hence \n        #clubbing these ops in one loop instead of two. \n        #loops are slower than vectorised operations. \n        \n        prediction = write_results(prediction, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n        \n        \n        if type(prediction) == int:\n            i += 1\n            continue\n\n        end = time.time()\n        \n                    \n#        print(end - start)\n\n            \n\n        prediction[:,0] += i*batch_size\n        \n    \n            \n          \n        if not write:\n            output = prediction\n            write = 1\n        else:\n            output = torch.cat((output,prediction))\n            \n        \n        \n\n        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n            im_id = i*batch_size + im_num\n            objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n            print(""{0:20s} predicted in {1:6.3f} seconds"".format(image.split(""/"")[-1], (end - start)/batch_size))\n            print(""{0:20s} {1:s}"".format(""Objects Detected:"", "" "".join(objs)))\n            print(""----------------------------------------------------------"")\n        i += 1\n\n        \n        if CUDA:\n            torch.cuda.synchronize()\n    \n    try:\n        output\n    except NameError:\n        print(""No detections were made"")\n        exit()\n        \n    im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n    \n    scaling_factor = torch.min(inp_dim/im_dim_list,1)[0].view(-1,1)\n    \n    \n    output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n    output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n    \n    \n    \n    output[:,1:5] /= scaling_factor\n    \n    for i in range(output.shape[0]):\n        output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n        output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n        \n        \n    output_recast = time.time()\n    \n    \n    class_load = time.time()\n\n    colors = pkl.load(open(""pallete"", ""rb""))\n    \n    \n    draw = time.time()\n\n\n    def write(x, batches, results):\n        c1 = tuple(x[1:3].int())\n        c2 = tuple(x[3:5].int())\n        img = results[int(x[0])]\n        cls = int(x[-1])\n        label = ""{0}"".format(classes[cls])\n        color = random.choice(colors)\n        cv2.rectangle(img, c1, c2,color, 1)\n        t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n        c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n        cv2.rectangle(img, c1, c2,color, -1)\n        cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1)\n        return img\n    \n            \n    list(map(lambda x: write(x, im_batches, orig_ims), output))\n      \n    det_names = pd.Series(imlist).apply(lambda x: ""{}/det_{}"".format(args.det,x.split(""/"")[-1]))\n    \n    list(map(cv2.imwrite, det_names, orig_ims))\n    \n    end = time.time()\n    \n    print()\n    print(""SUMMARY"")\n    print(""----------------------------------------------------------"")\n    print(""{:25s}: {}"".format(""Task"", ""Time Taken (in seconds)""))\n    print()\n    print(""{:25s}: {:2.3f}"".format(""Reading addresses"", load_batch - read_dir))\n    print(""{:25s}: {:2.3f}"".format(""Loading batch"", start_det_loop - load_batch))\n    print(""{:25s}: {:2.3f}"".format(""Detection ("" + str(len(imlist)) +  "" images)"", output_recast - start_det_loop))\n    print(""{:25s}: {:2.3f}"".format(""Output Processing"", class_load - output_recast))\n    print(""{:25s}: {:2.3f}"".format(""Drawing Boxes"", end - draw))\n    print(""{:25s}: {:2.3f}"".format(""Average time_per_img"", (end - load_batch)/len(imlist)))\n    print(""----------------------------------------------------------"")\n\n    \n    torch.cuda.empty_cache()\n    \n    \n        \n        \n    \n    \n'"
preprocess.py,5,"b'from __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom util import count_parameters as count\nfrom util import convert2cpu as cpu\nfrom PIL import Image, ImageDraw\n\n\ndef letterbox_image(img, inp_dim):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    img_w, img_h = img.shape[1], img.shape[0]\n    w, h = inp_dim\n    new_w = int(img_w * min(w/img_w, h/img_h))\n    new_h = int(img_h * min(w/img_w, h/img_h))\n    resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n    \n    canvas = np.full((inp_dim[1], inp_dim[0], 3), 128)\n\n    canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n    \n    return canvas\n\n\n        \ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = cv2.imread(img)\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef prep_image_pil(img, network_dim):\n    orig_im = Image.open(img)\n    img = orig_im.convert(\'RGB\')\n    dim = img.size\n    img = img.resize(network_dim)\n    img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n    img = img.view(*network_dim, 3).transpose(0,1).transpose(0,2).contiguous()\n    img = img.view(1, 3,*network_dim)\n    img = img.float().div(255.0)\n    return (img, orig_im, dim)\n\ndef inp_to_image(inp):\n    inp = inp.cpu().squeeze()\n    inp = inp*255\n    try:\n        inp = inp.data.numpy()\n    except RuntimeError:\n        inp = inp.numpy()\n    inp = inp.transpose(1,2,0)\n\n    inp = inp[:,:,::-1]\n    return inp\n\n\n'"
util.py,42,"b'\nfrom __future__ import division\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nimport matplotlib.pyplot as plt\nfrom bbox import bbox_iou\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef count_learnable_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef convert2cpu(matrix):\n    if matrix.is_cuda:\n        return torch.FloatTensor(matrix.size()).copy_(matrix)\n    else:\n        return matrix\n\ndef predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n    grid_size = inp_dim // stride\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    \n    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n\n\n\n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n\n\n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n    \n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if CUDA:\n        x_offset = x_offset.cuda()\n        y_offset = y_offset.cuda()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.FloatTensor(anchors)\n    \n    if CUDA:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n\n    prediction[:,:,:4] *= stride\n   \n    \n    return prediction\n\ndef load_classes(namesfile):\n    fp = open(namesfile, ""r"")\n    names = fp.read().split(""\\n"")[:-1]\n    return names\n\ndef get_im_dim(im):\n    im = cv2.imread(im)\n    w,h = im.shape[1], im.shape[0]\n    return w,h\n\ndef unique(tensor):\n    tensor_np = tensor.cpu().numpy()\n    unique_np = np.unique(tensor_np)\n    unique_tensor = torch.from_numpy(unique_np)\n    \n    tensor_res = tensor.new(unique_tensor.shape)\n    tensor_res.copy_(unique_tensor)\n    return tensor_res\n\ndef write_results(prediction, confidence, num_classes, nms = True, nms_conf = 0.4):\n    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n\n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n    \n    \n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n    \n\n    \n    batch_size = prediction.size(0)\n    \n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n\n\n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n        \n\n        \n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.float().unsqueeze(1)\n        max_conf_score = max_conf_score.float().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n        \n\n        \n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n\n        \n        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n        \n        #Get the various classes detected in the image\n        try:\n            img_classes = unique(image_pred_[:,-1])\n        except:\n             continue\n        #WE will do NMS classwise\n        for cls in img_classes:\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            \n\n            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n\n\t\t\n        \n             #sort the detections such that the entry with the maximum objectness\n             #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            #if nms has to be done\n            if nms:\n                #For each detection\n                for i in range(idx):\n                    #Get the IOUs of all boxes that come after the one we are looking at \n                    #in the loop\n                    try:\n                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                    except ValueError:\n                        break\n        \n                    except IndexError:\n                        break\n                    \n                    #Zero out all the detections that have IoU > treshhold\n                    iou_mask = (ious < nms_conf).float().unsqueeze(1)\n                    image_pred_class[i+1:] *= iou_mask       \n                    \n                    #Remove the non-zero entries\n                    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                    image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n                    \n                    \n\n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            \n            \n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Mar 24 00:12:16 2018\n\n@author: ayooshmac\n""""""\n\ndef predict_transform_half(prediction, inp_dim, anchors, num_classes, CUDA = True):\n    batch_size = prediction.size(0)\n    stride =  inp_dim // prediction.size(2)\n\n    bbox_attrs = 5 + num_classes\n    num_anchors = len(anchors)\n    grid_size = inp_dim // stride\n\n    \n    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n    prediction = prediction.transpose(1,2).contiguous()\n    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n    \n    \n    #Sigmoid the  centre_X, centre_Y. and object confidencce\n    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n\n    \n    #Add the center offsets\n    grid_len = np.arange(grid_size)\n    a,b = np.meshgrid(grid_len, grid_len)\n    \n    x_offset = torch.FloatTensor(a).view(-1,1)\n    y_offset = torch.FloatTensor(b).view(-1,1)\n    \n    if CUDA:\n        x_offset = x_offset.cuda().half()\n        y_offset = y_offset.cuda().half()\n    \n    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n    \n    prediction[:,:,:2] += x_y_offset\n      \n    #log space transform height and the width\n    anchors = torch.HalfTensor(anchors)\n    \n    if CUDA:\n        anchors = anchors.cuda()\n    \n    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n\n    #Softmax the class scores\n    prediction[:,:,5: 5 + num_classes] = nn.Softmax(-1)(Variable(prediction[:,:, 5 : 5 + num_classes])).data\n\n    prediction[:,:,:4] *= stride\n    \n    \n    return prediction\n\n\ndef write_results_half(prediction, confidence, num_classes, nms = True, nms_conf = 0.4):\n    conf_mask = (prediction[:,:,4] > confidence).half().unsqueeze(2)\n    prediction = prediction*conf_mask\n    \n    try:\n        ind_nz = torch.nonzero(prediction[:,:,4]).transpose(0,1).contiguous()\n    except:\n        return 0\n    \n    \n    \n    box_a = prediction.new(prediction.shape)\n    box_a[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n    box_a[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n    box_a[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n    box_a[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n    prediction[:,:,:4] = box_a[:,:,:4]\n    \n    \n    \n    batch_size = prediction.size(0)\n    \n    output = prediction.new(1, prediction.size(2) + 1)\n    write = False\n    \n    for ind in range(batch_size):\n        #select the image from the batch\n        image_pred = prediction[ind]\n\n        \n        #Get the class having maximum score, and the index of that class\n        #Get rid of num_classes softmax scores \n        #Add the class index and the class score of class having maximum score\n        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n        max_conf = max_conf.half().unsqueeze(1)\n        max_conf_score = max_conf_score.half().unsqueeze(1)\n        seq = (image_pred[:,:5], max_conf, max_conf_score)\n        image_pred = torch.cat(seq, 1)\n        \n        \n        #Get rid of the zero entries\n        non_zero_ind =  (torch.nonzero(image_pred[:,4]))\n        try:\n            image_pred_ = image_pred[non_zero_ind.squeeze(),:]\n        except:\n            continue\n        \n        #Get the various classes detected in the image\n        img_classes = unique(image_pred_[:,-1].long()).half()\n        \n        \n        \n                \n        #WE will do NMS classwise\n        for cls in img_classes:\n            #get the detections with one particular class\n            cls_mask = image_pred_*(image_pred_[:,-1] == cls).half().unsqueeze(1)\n            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n            \n\n            image_pred_class = image_pred_[class_mask_ind]\n\n        \n             #sort the detections such that the entry with the maximum objectness\n             #confidence is at the top\n            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n            image_pred_class = image_pred_class[conf_sort_index]\n            idx = image_pred_class.size(0)\n            \n            #if nms has to be done\n            if nms:\n                #For each detection\n                for i in range(idx):\n                    #Get the IOUs of all boxes that come after the one we are looking at \n                    #in the loop\n                    try:\n                        ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n                    except ValueError:\n                        break\n        \n                    except IndexError:\n                        break\n                    \n                    #Zero out all the detections that have IoU > treshhold\n                    iou_mask = (ious < nms_conf).half().unsqueeze(1)\n                    image_pred_class[i+1:] *= iou_mask       \n                    \n                    #Remove the non-zero entries\n                    non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n                    image_pred_class = image_pred_class[non_zero_ind]\n                    \n                    \n            \n            #Concatenate the batch_id of the image to the detection\n            #this helps us identify which image does the detection correspond to \n            #We use a linear straucture to hold ALL the detections from the batch\n            #the batch_dim is flattened\n            #batch is identified by extra batch column\n            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)\n            seq = batch_ind, image_pred_class\n            \n            if not write:\n                output = torch.cat(seq,1)\n                write = True\n            else:\n                out = torch.cat(seq,1)\n                output = torch.cat((output,out))\n    \n    return output\n'"
video_demo.py,11,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v3 Video Detection Module\')\n   \n    parser.add_argument(""--video"", dest = \'video\', help = \n                        ""Video to run detection upon"",\n                        default = ""video.avi"", type = str)\n    parser.add_argument(""--dataset"", dest = ""dataset"", help = ""Dataset on which the network has been trained"", default = ""pascal"")\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.5)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--cfg"", dest = \'cfgfile\', help = \n                        ""Config file"",\n                        default = ""cfg/yolov3.cfg"", type = str)\n    parser.add_argument(""--weights"", dest = \'weightsfile\', help = \n                        ""weightsfile"",\n                        default = ""yolov3.weights"", type = str)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""416"", type = str)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n    num_classes = 80\n\n    CUDA = torch.cuda.is_available()\n    \n    bbox_attrs = 5 + num_classes\n    \n    print(""Loading network....."")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(""Network successfully loaded"")\n\n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    if CUDA:\n        model.cuda()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = args.video\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                im_dim = im_dim.cuda()\n                img = img.cuda()\n            \n            with torch.no_grad():   \n                output = model(Variable(img), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n            \n            \n\n            \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
video_demo_half.py,10,"b'from __future__ import division\nimport time\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport cv2 \nfrom util import *\nfrom darknet import Darknet\nfrom preprocess import prep_image, inp_to_image, letterbox_image\nimport pandas as pd\nimport random \nimport pickle as pkl\nimport argparse\n\n\ndef get_test_input(input_dim, CUDA):\n    img = cv2.imread(""dog-cycle-car.png"")\n    img = cv2.resize(img, (input_dim, input_dim)) \n    img_ =  img[:,:,::-1].transpose((2,0,1))\n    img_ = img_[np.newaxis,:,:,:]/255.0\n    img_ = torch.from_numpy(img_).float()\n    img_ = Variable(img_)\n    \n    if CUDA:\n        img_ = img_.cuda()\n    \n    return img_\n\ndef prep_image(img, inp_dim):\n    """"""\n    Prepare image for inputting to the neural network. \n    \n    Returns a Variable \n    """"""\n\n    orig_im = img\n    dim = orig_im.shape[1], orig_im.shape[0]\n    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n    return img_, orig_im, dim\n\ndef write(x, img):\n    c1 = tuple(x[1:3].int())\n    c2 = tuple(x[3:5].int())\n    cls = int(x[-1])\n    label = ""{0}"".format(classes[cls])\n    color = random.choice(colors)\n    cv2.rectangle(img, c1, c2,color, 1)\n    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n    cv2.rectangle(img, c1, c2,color, -1)\n    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n    return img\n\ndef arg_parse():\n    """"""\n    Parse arguements to the detect module\n    \n    """"""\n    \n    \n    parser = argparse.ArgumentParser(description=\'YOLO v2 Video Detection Module\')\n   \n    parser.add_argument(""--video"", dest = \'video\', help = \n                        ""Video to run detection upon"",\n                        default = ""video.avi"", type = str)\n    parser.add_argument(""--dataset"", dest = ""dataset"", help = ""Dataset on which the network has been trained"", default = ""pascal"")\n    parser.add_argument(""--confidence"", dest = ""confidence"", help = ""Object Confidence to filter predictions"", default = 0.5)\n    parser.add_argument(""--nms_thresh"", dest = ""nms_thresh"", help = ""NMS Threshhold"", default = 0.4)\n    parser.add_argument(""--cfg"", dest = \'cfgfile\', help = \n                        ""Config file"",\n                        default = ""cfg/yolov3.cfg"", type = str)\n    parser.add_argument(""--weights"", dest = \'weightsfile\', help = \n                        ""weightsfile"",\n                        default = ""yolov3.weights"", type = str)\n    parser.add_argument(""--reso"", dest = \'reso\', help = \n                        ""Input resolution of the network. Increase to increase accuracy. Decrease to increase speed"",\n                        default = ""416"", type = str)\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    args = arg_parse()\n    confidence = float(args.confidence)\n    nms_thesh = float(args.nms_thresh)\n    start = 0\n\n    CUDA = torch.cuda.is_available()\n\n        \n\n    CUDA = torch.cuda.is_available()\n    num_classes = 80 \n    bbox_attrs = 5 + num_classes\n    \n    print(""Loading network....."")\n    model = Darknet(args.cfgfile)\n    model.load_weights(args.weightsfile)\n    print(""Network successfully loaded"")\n    \n    model.net_info[""height""] = args.reso\n    inp_dim = int(model.net_info[""height""])\n    assert inp_dim % 32 == 0 \n    assert inp_dim > 32\n\n    \n    if CUDA:\n        model.cuda().half()\n        \n    model(get_test_input(inp_dim, CUDA), CUDA)\n\n    model.eval()\n    \n    videofile = \'video.avi\'\n    \n    cap = cv2.VideoCapture(videofile)\n    \n    assert cap.isOpened(), \'Cannot capture source\'\n    \n    frames = 0\n    start = time.time()    \n    while cap.isOpened():\n        \n        ret, frame = cap.read()\n        if ret:\n            \n\n            img, orig_im, dim = prep_image(frame, inp_dim)\n            \n            im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n            \n            \n            if CUDA:\n                img = img.cuda().half()\n                im_dim = im_dim.half().cuda()\n                write_results = write_results_half\n                predict_transform = predict_transform_half\n            \n            \n            output = model(Variable(img, volatile = True), CUDA)\n            output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n\n           \n            if type(output) == int:\n                frames += 1\n                print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n                cv2.imshow(""frame"", orig_im)\n                key = cv2.waitKey(1)\n                if key & 0xFF == ord(\'q\'):\n                    break\n                continue\n\n        \n            im_dim = im_dim.repeat(output.size(0), 1)\n            scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n            \n            output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n            output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n            \n            output[:,1:5] /= scaling_factor\n    \n            for i in range(output.shape[0]):\n                output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n                output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n            \n            \n            classes = load_classes(\'data/coco.names\')\n            colors = pkl.load(open(""pallete"", ""rb""))\n            \n            list(map(lambda x: write(x, orig_im), output))\n            \n            \n            cv2.imshow(""frame"", orig_im)\n            key = cv2.waitKey(1)\n            if key & 0xFF == ord(\'q\'):\n                break\n            frames += 1\n            print(""FPS of the video is {:5.2f}"".format( frames / (time.time() - start)))\n\n            \n        else:\n            break\n    \n\n    \n    \n\n'"
