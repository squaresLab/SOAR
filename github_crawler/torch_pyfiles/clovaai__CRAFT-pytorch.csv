file_path,api_count,code
craft.py,7,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom basenet.vgg16_bn import vgg16_bn, init_weights\n\nclass double_conv(nn.Module):\n    def __init__(self, in_ch, mid_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),\n            nn.BatchNorm2d(mid_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass CRAFT(nn.Module):\n    def __init__(self, pretrained=False, freeze=False):\n        super(CRAFT, self).__init__()\n\n        """""" Base network """"""\n        self.basenet = vgg16_bn(pretrained, freeze)\n\n        """""" U network """"""\n        self.upconv1 = double_conv(1024, 512, 256)\n        self.upconv2 = double_conv(512, 256, 128)\n        self.upconv3 = double_conv(256, 128, 64)\n        self.upconv4 = double_conv(128, 64, 32)\n\n        num_class = 2\n        self.conv_cls = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n            nn.Conv2d(16, num_class, kernel_size=1),\n        )\n\n        init_weights(self.upconv1.modules())\n        init_weights(self.upconv2.modules())\n        init_weights(self.upconv3.modules())\n        init_weights(self.upconv4.modules())\n        init_weights(self.conv_cls.modules())\n        \n    def forward(self, x):\n        """""" Base network """"""\n        sources = self.basenet(x)\n\n        """""" U network """"""\n        y = torch.cat([sources[0], sources[1]], dim=1)\n        y = self.upconv1(y)\n\n        y = F.interpolate(y, size=sources[2].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[2]], dim=1)\n        y = self.upconv2(y)\n\n        y = F.interpolate(y, size=sources[3].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[3]], dim=1)\n        y = self.upconv3(y)\n\n        y = F.interpolate(y, size=sources[4].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[4]], dim=1)\n        feature = self.upconv4(y)\n\n        y = self.conv_cls(feature)\n\n        return y.permute(0,2,3,1), feature\n\nif __name__ == \'__main__\':\n    model = CRAFT(pretrained=True).cuda()\n    output, _ = model(torch.randn(1, 3, 768, 768).cuda())\n    print(output.shape)'"
craft_utils.py,0,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport cv2\nimport math\n\n"""""" auxilary functions """"""\n# unwarp corodinates\ndef warpCoord(Minv, pt):\n    out = np.matmul(Minv, (pt[0], pt[1], 1))\n    return np.array([out[0]/out[2], out[1]/out[2]])\n"""""" end of auxilary functions """"""\n\n\ndef getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text):\n    # prepare data\n    linkmap = linkmap.copy()\n    textmap = textmap.copy()\n    img_h, img_w = textmap.shape\n\n    """""" labeling method """"""\n    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n\n    text_score_comb = np.clip(text_score + link_score, 0, 1)\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n\n    det = []\n    mapper = []\n    for k in range(1,nLabels):\n        # size filtering\n        size = stats[k, cv2.CC_STAT_AREA]\n        if size < 10: continue\n\n        # thresholding\n        if np.max(textmap[labels==k]) < text_threshold: continue\n\n        # make segmentation map\n        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n        segmap[labels==k] = 255\n        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        # boundary check\n        if sx < 0 : sx = 0\n        if sy < 0 : sy = 0\n        if ex >= img_w: ex = img_w\n        if ey >= img_h: ey = img_h\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n\n        # make box\n        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n\n        # align diamond-shape\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n\n        # make clock-wise order\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4-startidx, 0)\n        box = np.array(box)\n\n        det.append(box)\n        mapper.append(k)\n\n    return det, labels, mapper\n\ndef getPoly_core(boxes, labels, mapper, linkmap):\n    # configs\n    num_cp = 5\n    max_len_ratio = 0.7\n    expand_ratio = 1.45\n    max_r = 2.0\n    step_r = 0.2\n\n    polys = []  \n    for k, box in enumerate(boxes):\n        # size filter for small instance\n        w, h = int(np.linalg.norm(box[0] - box[1]) + 1), int(np.linalg.norm(box[1] - box[2]) + 1)\n        if w < 10 or h < 10:\n            polys.append(None); continue\n\n        # warp image\n        tar = np.float32([[0,0],[w,0],[w,h],[0,h]])\n        M = cv2.getPerspectiveTransform(box, tar)\n        word_label = cv2.warpPerspective(labels, M, (w, h), flags=cv2.INTER_NEAREST)\n        try:\n            Minv = np.linalg.inv(M)\n        except:\n            polys.append(None); continue\n\n        # binarization for selected label\n        cur_label = mapper[k]\n        word_label[word_label != cur_label] = 0\n        word_label[word_label > 0] = 1\n\n        """""" Polygon generation """"""\n        # find top/bottom contours\n        cp = []\n        max_len = -1\n        for i in range(w):\n            region = np.where(word_label[:,i] != 0)[0]\n            if len(region) < 2 : continue\n            cp.append((i, region[0], region[-1]))\n            length = region[-1] - region[0] + 1\n            if length > max_len: max_len = length\n\n        # pass if max_len is similar to h\n        if h * max_len_ratio < max_len:\n            polys.append(None); continue\n\n        # get pivot points with fixed length\n        tot_seg = num_cp * 2 + 1\n        seg_w = w / tot_seg     # segment width\n        pp = [None] * num_cp    # init pivot points\n        cp_section = [[0, 0]] * tot_seg\n        seg_height = [0] * num_cp\n        seg_num = 0\n        num_sec = 0\n        prev_h = -1\n        for i in range(0,len(cp)):\n            (x, sy, ey) = cp[i]\n            if (seg_num + 1) * seg_w <= x and seg_num <= tot_seg:\n                # average previous segment\n                if num_sec == 0: break\n                cp_section[seg_num] = [cp_section[seg_num][0] / num_sec, cp_section[seg_num][1] / num_sec]\n                num_sec = 0\n\n                # reset variables\n                seg_num += 1\n                prev_h = -1\n\n            # accumulate center points\n            cy = (sy + ey) * 0.5\n            cur_h = ey - sy + 1\n            cp_section[seg_num] = [cp_section[seg_num][0] + x, cp_section[seg_num][1] + cy]\n            num_sec += 1\n\n            if seg_num % 2 == 0: continue # No polygon area\n\n            if prev_h < cur_h:\n                pp[int((seg_num - 1)/2)] = (x, cy)\n                seg_height[int((seg_num - 1)/2)] = cur_h\n                prev_h = cur_h\n\n        # processing last segment\n        if num_sec != 0:\n            cp_section[-1] = [cp_section[-1][0] / num_sec, cp_section[-1][1] / num_sec]\n\n        # pass if num of pivots is not sufficient or segment widh is smaller than character height \n        if None in pp or seg_w < np.max(seg_height) * 0.25:\n            polys.append(None); continue\n\n        # calc median maximum of pivot points\n        half_char_h = np.median(seg_height) * expand_ratio / 2\n\n        # calc gradiant and apply to make horizontal pivots\n        new_pp = []\n        for i, (x, cy) in enumerate(pp):\n            dx = cp_section[i * 2 + 2][0] - cp_section[i * 2][0]\n            dy = cp_section[i * 2 + 2][1] - cp_section[i * 2][1]\n            if dx == 0:     # gradient if zero\n                new_pp.append([x, cy - half_char_h, x, cy + half_char_h])\n                continue\n            rad = - math.atan2(dy, dx)\n            c, s = half_char_h * math.cos(rad), half_char_h * math.sin(rad)\n            new_pp.append([x - s, cy - c, x + s, cy + c])\n\n        # get edge points to cover character heatmaps\n        isSppFound, isEppFound = False, False\n        grad_s = (pp[1][1] - pp[0][1]) / (pp[1][0] - pp[0][0]) + (pp[2][1] - pp[1][1]) / (pp[2][0] - pp[1][0])\n        grad_e = (pp[-2][1] - pp[-1][1]) / (pp[-2][0] - pp[-1][0]) + (pp[-3][1] - pp[-2][1]) / (pp[-3][0] - pp[-2][0])\n        for r in np.arange(0.5, max_r, step_r):\n            dx = 2 * half_char_h * r\n            if not isSppFound:\n                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n                dy = grad_s * dx\n                p = np.array(new_pp[0]) - np.array([dx, dy, dx, dy])\n                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n                    spp = p\n                    isSppFound = True\n            if not isEppFound:\n                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n                dy = grad_e * dx\n                p = np.array(new_pp[-1]) + np.array([dx, dy, dx, dy])\n                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n                    epp = p\n                    isEppFound = True\n            if isSppFound and isEppFound:\n                break\n\n        # pass if boundary of polygon is not found\n        if not (isSppFound and isEppFound):\n            polys.append(None); continue\n\n        # make final polygon\n        poly = []\n        poly.append(warpCoord(Minv, (spp[0], spp[1])))\n        for p in new_pp:\n            poly.append(warpCoord(Minv, (p[0], p[1])))\n        poly.append(warpCoord(Minv, (epp[0], epp[1])))\n        poly.append(warpCoord(Minv, (epp[2], epp[3])))\n        for p in reversed(new_pp):\n            poly.append(warpCoord(Minv, (p[2], p[3])))\n        poly.append(warpCoord(Minv, (spp[2], spp[3])))\n\n        # add to final result\n        polys.append(np.array(poly))\n\n    return polys\n\ndef getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text, poly=False):\n    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text)\n\n    if poly:\n        polys = getPoly_core(boxes, labels, mapper, linkmap)\n    else:\n        polys = [None] * len(boxes)\n\n    return boxes, polys\n\ndef adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n    if len(polys) > 0:\n        polys = np.array(polys)\n        for k in range(len(polys)):\n            if polys[k] is not None:\n                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n    return polys\n'"
file_utils.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport numpy as np\nimport cv2\nimport imgproc\n\n# borrowed from https://github.com/lengstrom/fast-style-transfer/blob/master/src/utils.py\ndef get_files(img_dir):\n    imgs, masks, xmls = list_files(img_dir)\n    return imgs, masks, xmls\n\ndef list_files(in_path):\n    img_files = []\n    mask_files = []\n    gt_files = []\n    for (dirpath, dirnames, filenames) in os.walk(in_path):\n        for file in filenames:\n            filename, ext = os.path.splitext(file)\n            ext = str.lower(ext)\n            if ext == \'.jpg\' or ext == \'.jpeg\' or ext == \'.gif\' or ext == \'.png\' or ext == \'.pgm\':\n                img_files.append(os.path.join(dirpath, file))\n            elif ext == \'.bmp\':\n                mask_files.append(os.path.join(dirpath, file))\n            elif ext == \'.xml\' or ext == \'.gt\' or ext == \'.txt\':\n                gt_files.append(os.path.join(dirpath, file))\n            elif ext == \'.zip\':\n                continue\n    # img_files.sort()\n    # mask_files.sort()\n    # gt_files.sort()\n    return img_files, mask_files, gt_files\n\ndef saveResult(img_file, img, boxes, dirname=\'./result/\', verticals=None, texts=None):\n        """""" save text detection result one by one\n        Args:\n            img_file (str): image file name\n            img (array): raw image context\n            boxes (array): array of result file\n                Shape: [num_detections, 4] for BB output / [num_detections, 4] for QUAD output\n        Return:\n            None\n        """"""\n        img = np.array(img)\n\n        # make result file list\n        filename, file_ext = os.path.splitext(os.path.basename(img_file))\n\n        # result directory\n        res_file = dirname + ""res_"" + filename + \'.txt\'\n        res_img_file = dirname + ""res_"" + filename + \'.jpg\'\n\n        if not os.path.isdir(dirname):\n            os.mkdir(dirname)\n\n        with open(res_file, \'w\') as f:\n            for i, box in enumerate(boxes):\n                poly = np.array(box).astype(np.int32).reshape((-1))\n                strResult = \',\'.join([str(p) for p in poly]) + \'\\r\\n\'\n                f.write(strResult)\n\n                poly = poly.reshape(-1, 2)\n                cv2.polylines(img, [poly.reshape((-1, 1, 2))], True, color=(0, 0, 255), thickness=2)\n                ptColor = (0, 255, 255)\n                if verticals is not None:\n                    if verticals[i]:\n                        ptColor = (255, 0, 0)\n\n                if texts is not None:\n                    font = cv2.FONT_HERSHEY_SIMPLEX\n                    font_scale = 0.5\n                    cv2.putText(img, ""{}"".format(texts[i]), (poly[0][0]+1, poly[0][1]+1), font, font_scale, (0, 0, 0), thickness=1)\n                    cv2.putText(img, ""{}"".format(texts[i]), tuple(poly[0]), font, font_scale, (0, 255, 255), thickness=1)\n\n        # Save result image\n        cv2.imwrite(res_img_file, img)\n\n'"
imgproc.py,0,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nfrom skimage import io\nimport cv2\n\ndef loadImage(img_file):\n    img = io.imread(img_file)           # RGB order\n    if img.shape[0] == 2: img = img[0]\n    if len(img.shape) == 2 : img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    if img.shape[2] == 4:   img = img[:,:,:3]\n    img = np.array(img)\n\n    return img\n\ndef normalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n    # should be RGB order\n    img = in_img.copy().astype(np.float32)\n\n    img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)\n    img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)\n    return img\n\ndef denormalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n    # should be RGB order\n    img = in_img.copy()\n    img *= variance\n    img += mean\n    img *= 255.0\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    return img\n\ndef resize_aspect_ratio(img, square_size, interpolation, mag_ratio=1):\n    height, width, channel = img.shape\n\n    # magnify image size\n    target_size = mag_ratio * max(height, width)\n\n    # set original image size\n    if target_size > square_size:\n        target_size = square_size\n    \n    ratio = target_size / max(height, width)    \n\n    target_h, target_w = int(height * ratio), int(width * ratio)\n    proc = cv2.resize(img, (target_w, target_h), interpolation = interpolation)\n\n\n    # make canvas and paste image\n    target_h32, target_w32 = target_h, target_w\n    if target_h % 32 != 0:\n        target_h32 = target_h + (32 - target_h % 32)\n    if target_w % 32 != 0:\n        target_w32 = target_w + (32 - target_w % 32)\n    resized = np.zeros((target_h32, target_w32, channel), dtype=np.float32)\n    resized[0:target_h, 0:target_w, :] = proc\n    target_h, target_w = target_h32, target_w32\n\n    size_heatmap = (int(target_w/2), int(target_h/2))\n\n    return resized, ratio, size_heatmap\n\ndef cvt2HeatmapImg(img):\n    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n    img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n    return img\n'"
refinenet.py,5,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom basenet.vgg16_bn import init_weights\n\n\nclass RefineNet(nn.Module):\n    def __init__(self):\n        super(RefineNet, self).__init__()\n\n        self.last_conv = nn.Sequential(\n            nn.Conv2d(34, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True)\n        )\n\n        self.aspp1 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, dilation=6, padding=6), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1)\n        )\n\n        self.aspp2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, dilation=12, padding=12), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1)\n        )\n\n        self.aspp3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, dilation=18, padding=18), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1)\n        )\n\n        self.aspp4 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, dilation=24, padding=24), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1)\n        )\n\n        init_weights(self.last_conv.modules())\n        init_weights(self.aspp1.modules())\n        init_weights(self.aspp2.modules())\n        init_weights(self.aspp3.modules())\n        init_weights(self.aspp4.modules())\n\n    def forward(self, y, upconv4):\n        refine = torch.cat([y.permute(0,3,1,2), upconv4], dim=1)\n        refine = self.last_conv(refine)\n\n        aspp1 = self.aspp1(refine)\n        aspp2 = self.aspp2(refine)\n        aspp3 = self.aspp3(refine)\n        aspp4 = self.aspp4(refine)\n\n        #out = torch.add([aspp1, aspp2, aspp3, aspp4], dim=1)\n        out = aspp1 + aspp2 + aspp3 + aspp4\n        return out.permute(0, 2, 3, 1)  # , refine.permute(0,2,3,1)\n'"
test.py,12,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport time\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nfrom PIL import Image\n\nimport cv2\nfrom skimage import io\nimport numpy as np\nimport craft_utils\nimport imgproc\nimport file_utils\nimport json\nimport zipfile\n\nfrom craft import CRAFT\n\nfrom collections import OrderedDict\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""y"", ""true"", ""t"", ""1"")\n\nparser = argparse.ArgumentParser(description=\'CRAFT Text Detection\')\nparser.add_argument(\'--trained_model\', default=\'weights/craft_mlt_25k.pth\', type=str, help=\'pretrained model\')\nparser.add_argument(\'--text_threshold\', default=0.7, type=float, help=\'text confidence threshold\')\nparser.add_argument(\'--low_text\', default=0.4, type=float, help=\'text low-bound score\')\nparser.add_argument(\'--link_threshold\', default=0.4, type=float, help=\'link confidence threshold\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool, help=\'Use cuda for inference\')\nparser.add_argument(\'--canvas_size\', default=1280, type=int, help=\'image size for inference\')\nparser.add_argument(\'--mag_ratio\', default=1.5, type=float, help=\'image magnification ratio\')\nparser.add_argument(\'--poly\', default=False, action=\'store_true\', help=\'enable polygon type\')\nparser.add_argument(\'--show_time\', default=False, action=\'store_true\', help=\'show processing time\')\nparser.add_argument(\'--test_folder\', default=\'/data/\', type=str, help=\'folder path to input images\')\nparser.add_argument(\'--refine\', default=False, action=\'store_true\', help=\'enable link refiner\')\nparser.add_argument(\'--refiner_model\', default=\'weights/craft_refiner_CTW1500.pth\', type=str, help=\'pretrained refiner model\')\n\nargs = parser.parse_args()\n\n\n"""""" For test images in a folder """"""\nimage_list, _, _ = file_utils.get_files(args.test_folder)\n\nresult_folder = \'./result/\'\nif not os.path.isdir(result_folder):\n    os.mkdir(result_folder)\n\ndef test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly, refine_net=None):\n    t0 = time.time()\n\n    # resize\n    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n    ratio_h = ratio_w = 1 / target_ratio\n\n    # preprocessing\n    x = imgproc.normalizeMeanVariance(img_resized)\n    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n    if cuda:\n        x = x.cuda()\n\n    # forward pass\n    with torch.no_grad():\n        y, feature = net(x)\n\n    # make score and link map\n    score_text = y[0,:,:,0].cpu().data.numpy()\n    score_link = y[0,:,:,1].cpu().data.numpy()\n\n    # refine link\n    if refine_net is not None:\n        with torch.no_grad():\n            y_refiner = refine_net(y, feature)\n        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n\n    t0 = time.time() - t0\n    t1 = time.time()\n\n    # Post-processing\n    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n\n    # coordinate adjustment\n    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n    for k in range(len(polys)):\n        if polys[k] is None: polys[k] = boxes[k]\n\n    t1 = time.time() - t1\n\n    # render results (optional)\n    render_img = score_text.copy()\n    render_img = np.hstack((render_img, score_link))\n    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n\n    if args.show_time : print(""\\ninfer/postproc time : {:.3f}/{:.3f}"".format(t0, t1))\n\n    return boxes, polys, ret_score_text\n\n\n\nif __name__ == \'__main__\':\n    # load net\n    net = CRAFT()     # initialize\n\n    print(\'Loading weights from checkpoint (\' + args.trained_model + \')\')\n    if args.cuda:\n        net.load_state_dict(copyStateDict(torch.load(args.trained_model)))\n    else:\n        net.load_state_dict(copyStateDict(torch.load(args.trained_model, map_location=\'cpu\')))\n\n    if args.cuda:\n        net = net.cuda()\n        net = torch.nn.DataParallel(net)\n        cudnn.benchmark = False\n\n    net.eval()\n\n    # LinkRefiner\n    refine_net = None\n    if args.refine:\n        from refinenet import RefineNet\n        refine_net = RefineNet()\n        print(\'Loading weights of refiner from checkpoint (\' + args.refiner_model + \')\')\n        if args.cuda:\n            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model)))\n            refine_net = refine_net.cuda()\n            refine_net = torch.nn.DataParallel(refine_net)\n        else:\n            refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location=\'cpu\')))\n\n        refine_net.eval()\n        args.poly = True\n\n    t = time.time()\n\n    # load data\n    for k, image_path in enumerate(image_list):\n        print(""Test image {:d}/{:d}: {:s}"".format(k+1, len(image_list), image_path), end=\'\\r\')\n        image = imgproc.loadImage(image_path)\n\n        bboxes, polys, score_text = test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, refine_net)\n\n        # save score text\n        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n        mask_file = result_folder + ""/res_"" + filename + \'_mask.jpg\'\n        cv2.imwrite(mask_file, score_text)\n\n        file_utils.saveResult(image_path, image[:,:,::-1], polys, dirname=result_folder)\n\n    print(""elapsed time : {}s"".format(time.time() - t))\n'"
basenet/__init__.py,0,b''
basenet/vgg16_bn.py,9,"b'from collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torchvision import models\nfrom torchvision.models.vgg import model_urls\n\ndef init_weights(modules):\n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.zero_()\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n\nclass vgg16_bn(torch.nn.Module):\n    def __init__(self, pretrained=True, freeze=True):\n        super(vgg16_bn, self).__init__()\n        model_urls[\'vgg16_bn\'] = model_urls[\'vgg16_bn\'].replace(\'https://\', \'http://\')\n        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(12):         # conv2_2\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 19):         # conv3_3\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(19, 29):         # conv4_3\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(29, 39):         # conv5_3\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n\n        # fc6, fc7 without atrous conv\n        self.slice5 = torch.nn.Sequential(\n                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n                nn.Conv2d(1024, 1024, kernel_size=1)\n        )\n\n        if not pretrained:\n            init_weights(self.slice1.modules())\n            init_weights(self.slice2.modules())\n            init_weights(self.slice3.modules())\n            init_weights(self.slice4.modules())\n\n        init_weights(self.slice5.modules())        # no pretrained model for fc6 and fc7\n\n        if freeze:\n            for param in self.slice1.parameters():      # only first conv\n                param.requires_grad= False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu2_2 = h\n        h = self.slice2(h)\n        h_relu3_2 = h\n        h = self.slice3(h)\n        h_relu4_3 = h\n        h = self.slice4(h)\n        h_relu5_3 = h\n        h = self.slice5(h)\n        h_fc7 = h\n        vgg_outputs = namedtuple(""VggOutputs"", [\'fc7\', \'relu5_3\', \'relu4_3\', \'relu3_2\', \'relu2_2\'])\n        out = vgg_outputs(h_fc7, h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n        return out\n'"
