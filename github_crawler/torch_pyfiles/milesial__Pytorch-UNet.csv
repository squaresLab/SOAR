file_path,api_count,code
dice_loss.py,5,"b'import torch\nfrom torch.autograd import Function\n\n\nclass DiceCoeff(Function):\n    """"""Dice coeff for individual examples""""""\n\n    def forward(self, input, target):\n        self.save_for_backward(input, target)\n        eps = 0.0001\n        self.inter = torch.dot(input.view(-1), target.view(-1))\n        self.union = torch.sum(input) + torch.sum(target) + eps\n\n        t = (2 * self.inter.float() + eps) / self.union.float()\n        return t\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         / (self.union * self.union)\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target\n\n\ndef dice_coeff(input, target):\n    """"""Dice coeff for batches""""""\n    if input.is_cuda:\n        s = torch.FloatTensor(1).cuda().zero_()\n    else:\n        s = torch.FloatTensor(1).zero_()\n\n    for i, c in enumerate(zip(input, target)):\n        s = s + DiceCoeff().forward(c[0], c[1])\n\n    return s / (i + 1)\n'"
eval.py,5,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nfrom dice_loss import dice_coeff\n\n\ndef eval_net(net, loader, device):\n    """"""Evaluation without the densecrf with the dice coefficient""""""\n    net.eval()\n    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n    n_val = len(loader)  # the number of batch\n    tot = 0\n\n    with tqdm(total=n_val, desc=\'Validation round\', unit=\'batch\', leave=False) as pbar:\n        for batch in loader:\n            imgs, true_masks = batch[\'image\'], batch[\'mask\']\n            imgs = imgs.to(device=device, dtype=torch.float32)\n            true_masks = true_masks.to(device=device, dtype=mask_type)\n\n            with torch.no_grad():\n                mask_pred = net(imgs)\n\n            if net.n_classes > 1:\n                tot += F.cross_entropy(mask_pred, true_masks).item()\n            else:\n                pred = torch.sigmoid(mask_pred)\n                pred = (pred > 0.5).float()\n                tot += dice_coeff(pred, true_masks).item()\n            pbar.update()\n\n    net.train()\n    return tot / n_val\n'"
predict.py,7,"b'import argparse\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom unet import UNet\nfrom utils.data_vis import plot_img_and_mask\nfrom utils.dataset import BasicDataset\n\n\ndef predict_img(net,\n                full_img,\n                device,\n                scale_factor=1,\n                out_threshold=0.5):\n    net.eval()\n\n    img = torch.from_numpy(BasicDataset.preprocess(full_img, scale_factor))\n\n    img = img.unsqueeze(0)\n    img = img.to(device=device, dtype=torch.float32)\n\n    with torch.no_grad():\n        output = net(img)\n\n        if net.n_classes > 1:\n            probs = F.softmax(output, dim=1)\n        else:\n            probs = torch.sigmoid(output)\n\n        probs = probs.squeeze(0)\n\n        tf = transforms.Compose(\n            [\n                transforms.ToPILImage(),\n                transforms.Resize(full_img.size[1]),\n                transforms.ToTensor()\n            ]\n        )\n\n        probs = tf(probs.cpu())\n        full_mask = probs.squeeze().cpu().numpy()\n\n    return full_mask > out_threshold\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\'Predict masks from input images\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--model\', \'-m\', default=\'MODEL.pth\',\n                        metavar=\'FILE\',\n                        help=""Specify the file in which the model is stored"")\n    parser.add_argument(\'--input\', \'-i\', metavar=\'INPUT\', nargs=\'+\',\n                        help=\'filenames of input images\', required=True)\n\n    parser.add_argument(\'--output\', \'-o\', metavar=\'INPUT\', nargs=\'+\',\n                        help=\'Filenames of ouput images\')\n    parser.add_argument(\'--viz\', \'-v\', action=\'store_true\',\n                        help=""Visualize the images as they are processed"",\n                        default=False)\n    parser.add_argument(\'--no-save\', \'-n\', action=\'store_true\',\n                        help=""Do not save the output masks"",\n                        default=False)\n    parser.add_argument(\'--mask-threshold\', \'-t\', type=float,\n                        help=""Minimum probability value to consider a mask pixel white"",\n                        default=0.5)\n    parser.add_argument(\'--scale\', \'-s\', type=float,\n                        help=""Scale factor for the input images"",\n                        default=0.5)\n\n    return parser.parse_args()\n\n\ndef get_output_filenames(args):\n    in_files = args.input\n    out_files = []\n\n    if not args.output:\n        for f in in_files:\n            pathsplit = os.path.splitext(f)\n            out_files.append(""{}_OUT{}"".format(pathsplit[0], pathsplit[1]))\n    elif len(in_files) != len(args.output):\n        logging.error(""Input files and output files are not of the same length"")\n        raise SystemExit()\n    else:\n        out_files = args.output\n\n    return out_files\n\n\ndef mask_to_image(mask):\n    return Image.fromarray((mask * 255).astype(np.uint8))\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    in_files = args.input\n    out_files = get_output_filenames(args)\n\n    net = UNet(n_channels=3, n_classes=1)\n\n    logging.info(""Loading model {}"".format(args.model))\n\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    logging.info(f\'Using device {device}\')\n    net.to(device=device)\n    net.load_state_dict(torch.load(args.model, map_location=device))\n\n    logging.info(""Model loaded !"")\n\n    for i, fn in enumerate(in_files):\n        logging.info(""\\nPredicting image {} ..."".format(fn))\n\n        img = Image.open(fn)\n\n        mask = predict_img(net=net,\n                           full_img=img,\n                           scale_factor=args.scale,\n                           out_threshold=args.mask_threshold,\n                           device=device)\n\n        if not args.no_save:\n            out_fn = out_files[i]\n            result = mask_to_image(mask)\n            result.save(out_files[i])\n\n            logging.info(""Mask saved to {}"".format(out_files[i]))\n\n        if args.viz:\n            logging.info(""Visualizing results for image {}, close to continue ..."".format(fn))\n            plot_img_and_mask(img, mask)\n'"
submit.py,1,"b'"""""" Submit code specific to the kaggle challenge""""""\n\nimport os\n\nimport torch\nfrom PIL import Image\nimport numpy as np\n\nfrom predict import predict_img\nfrom unet import UNet\n\n# credits to https://stackoverflow.com/users/6076729/manuel-lagunas\ndef rle_encode(mask_image):\n    pixels = mask_image.flatten()\n    # We avoid issues with \'1\' at the start or end (at the corners of\n    # the original image) by setting those pixels to \'0\' explicitly.\n    # We do not expect these to be non-zero for an accurate mask,\n    # so this should not harm the score.\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] = runs[1::2] - runs[:-1:2]\n    return runs\n\n\ndef submit(net, gpu=False):\n    """"""Used for Kaggle submission: predicts and encode all test images""""""\n    dir = \'data/test/\'\n\n    N = len(list(os.listdir(dir)))\n    with open(\'SUBMISSION.csv\', \'a\') as f:\n        f.write(\'img,rle_mask\\n\')\n        for index, i in enumerate(os.listdir(dir)):\n            print(\'{}/{}\'.format(index, N))\n\n            img = Image.open(dir + i)\n\n            mask = predict_img(net, img, gpu)\n            enc = rle_encode(mask)\n            f.write(\'{},{}\\n\'.format(i, \' \'.join(map(str, enc))))\n\n\nif __name__ == \'__main__\':\n    net = UNet(3, 1).cuda()\n    net.load_state_dict(torch.load(\'MODEL.pth\'))\n    submit(net, True)\n'"
train.py,10,"b'import argparse\nimport logging\nimport os\nimport sys\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom tqdm import tqdm\n\nfrom eval import eval_net\nfrom unet import UNet\n\nfrom torch.utils.tensorboard import SummaryWriter\nfrom utils.dataset import BasicDataset\nfrom torch.utils.data import DataLoader, random_split\n\ndir_img = \'data/imgs/\'\ndir_mask = \'data/masks/\'\ndir_checkpoint = \'checkpoints/\'\n\n\ndef train_net(net,\n              device,\n              epochs=5,\n              batch_size=1,\n              lr=0.001,\n              val_percent=0.1,\n              save_cp=True,\n              img_scale=0.5):\n\n    dataset = BasicDataset(dir_img, dir_mask, img_scale)\n    n_val = int(len(dataset) * val_percent)\n    n_train = len(dataset) - n_val\n    train, val = random_split(dataset, [n_train, n_val])\n    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n\n    writer = SummaryWriter(comment=f\'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}\')\n    global_step = 0\n\n    logging.info(f\'\'\'Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {lr}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Checkpoints:     {save_cp}\n        Device:          {device.type}\n        Images scaling:  {img_scale}\n    \'\'\')\n\n    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\' if net.n_classes > 1 else \'max\', patience=2)\n    if net.n_classes > 1:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n\n    for epoch in range(epochs):\n        net.train()\n\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f\'Epoch {epoch + 1}/{epochs}\', unit=\'img\') as pbar:\n            for batch in train_loader:\n                imgs = batch[\'image\']\n                true_masks = batch[\'mask\']\n                assert imgs.shape[1] == net.n_channels, \\\n                    f\'Network has been defined with {net.n_channels} input channels, \' \\\n                    f\'but loaded images have {imgs.shape[1]} channels. Please check that \' \\\n                    \'the images are loaded correctly.\'\n\n                imgs = imgs.to(device=device, dtype=torch.float32)\n                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n                true_masks = true_masks.to(device=device, dtype=mask_type)\n\n                masks_pred = net(imgs)\n                loss = criterion(masks_pred, true_masks)\n                epoch_loss += loss.item()\n                writer.add_scalar(\'Loss/train\', loss.item(), global_step)\n\n                pbar.set_postfix(**{\'loss (batch)\': loss.item()})\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n                optimizer.step()\n\n                pbar.update(imgs.shape[0])\n                global_step += 1\n                if global_step % (len(dataset) // (10 * batch_size)) == 0:\n                    for tag, value in net.named_parameters():\n                        tag = tag.replace(\'.\', \'/\')\n                        writer.add_histogram(\'weights/\' + tag, value.data.cpu().numpy(), global_step)\n                        writer.add_histogram(\'grads/\' + tag, value.grad.data.cpu().numpy(), global_step)\n                    val_score = eval_net(net, val_loader, device)\n                    scheduler.step(val_score)\n                    writer.add_scalar(\'learning_rate\', optimizer.param_groups[0][\'lr\'], global_step)\n\n                    if net.n_classes > 1:\n                        logging.info(\'Validation cross entropy: {}\'.format(val_score))\n                        writer.add_scalar(\'Loss/test\', val_score, global_step)\n                    else:\n                        logging.info(\'Validation Dice Coeff: {}\'.format(val_score))\n                        writer.add_scalar(\'Dice/test\', val_score, global_step)\n\n                    writer.add_images(\'images\', imgs, global_step)\n                    if net.n_classes == 1:\n                        writer.add_images(\'masks/true\', true_masks, global_step)\n                        writer.add_images(\'masks/pred\', torch.sigmoid(masks_pred) > 0.5, global_step)\n\n        if save_cp:\n            try:\n                os.mkdir(dir_checkpoint)\n                logging.info(\'Created checkpoint directory\')\n            except OSError:\n                pass\n            torch.save(net.state_dict(),\n                       dir_checkpoint + f\'CP_epoch{epoch + 1}.pth\')\n            logging.info(f\'Checkpoint {epoch + 1} saved !\')\n\n    writer.close()\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=\'Train the UNet on images and target masks\',\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'-e\', \'--epochs\', metavar=\'E\', type=int, default=5,\n                        help=\'Number of epochs\', dest=\'epochs\')\n    parser.add_argument(\'-b\', \'--batch-size\', metavar=\'B\', type=int, nargs=\'?\', default=1,\n                        help=\'Batch size\', dest=\'batchsize\')\n    parser.add_argument(\'-l\', \'--learning-rate\', metavar=\'LR\', type=float, nargs=\'?\', default=0.1,\n                        help=\'Learning rate\', dest=\'lr\')\n    parser.add_argument(\'-f\', \'--load\', dest=\'load\', type=str, default=False,\n                        help=\'Load model from a .pth file\')\n    parser.add_argument(\'-s\', \'--scale\', dest=\'scale\', type=float, default=0.5,\n                        help=\'Downscaling factor of the images\')\n    parser.add_argument(\'-v\', \'--validation\', dest=\'val\', type=float, default=10.0,\n                        help=\'Percent of the data that is used as validation (0-100)\')\n\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.INFO, format=\'%(levelname)s: %(message)s\')\n    args = get_args()\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    logging.info(f\'Using device {device}\')\n\n    # Change here to adapt to your data\n    # n_channels=3 for RGB images\n    # n_classes is the number of probabilities you want to get per pixel\n    #   - For 1 class and background, use n_classes=1\n    #   - For 2 classes, use n_classes=1\n    #   - For N > 2 classes, use n_classes=N\n    net = UNet(n_channels=3, n_classes=1, bilinear=True)\n    logging.info(f\'Network:\\n\'\n                 f\'\\t{net.n_channels} input channels\\n\'\n                 f\'\\t{net.n_classes} output channels (classes)\\n\'\n                 f\'\\t{""Bilinear"" if net.bilinear else ""Transposed conv""} upscaling\')\n\n    if args.load:\n        net.load_state_dict(\n            torch.load(args.load, map_location=device)\n        )\n        logging.info(f\'Model loaded from {args.load}\')\n\n    net.to(device=device)\n    # faster convolutions, but more memory\n    # cudnn.benchmark = True\n\n    try:\n        train_net(net=net,\n                  epochs=args.epochs,\n                  batch_size=args.batchsize,\n                  lr=args.lr,\n                  device=device,\n                  img_scale=args.scale,\n                  val_percent=args.val / 100)\n    except KeyboardInterrupt:\n        torch.save(net.state_dict(), \'INTERRUPTED.pth\')\n        logging.info(\'Saved interrupt\')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\n'"
unet/__init__.py,0,b'from .unet_model import UNet\n'
unet/unet_model.py,1,"b'"""""" Full assembly of the parts to form the complete network """"""\n\nimport torch.nn.functional as F\n\nfrom .unet_parts import *\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits\n'"
unet/unet_parts.py,3,"b'"""""" Parts of the U-Net model """"""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DoubleConv(nn.Module):\n    """"""(convolution => [BN] => ReLU) * 2""""""\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    """"""Downscaling with maxpool then double conv""""""\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    """"""Upscaling then double conv""""""\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        # if you have padding issues, see\n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n'"
utils/data_vis.py,0,"b""import matplotlib.pyplot as plt\n\n\ndef plot_img_and_mask(img, mask):\n    classes = mask.shape[2] if len(mask.shape) > 2 else 1\n    fig, ax = plt.subplots(1, classes + 1)\n    ax[0].set_title('Input image')\n    ax[0].imshow(img)\n    if classes > 1:\n        for i in range(classes):\n            ax[i+1].set_title(f'Output mask (class {i+1})')\n            ax[i+1].imshow(mask[:, :, i])\n    else:\n        ax[1].set_title(f'Output mask')\n        ax[1].imshow(mask)\n    plt.xticks([]), plt.yticks([])\n    plt.show()\n"""
utils/dataset.py,3,"b""from os.path import splitext\nfrom os import listdir\nimport numpy as np\nfrom glob import glob\nimport torch\nfrom torch.utils.data import Dataset\nimport logging\nfrom PIL import Image\n\n\nclass BasicDataset(Dataset):\n    def __init__(self, imgs_dir, masks_dir, scale=1):\n        self.imgs_dir = imgs_dir\n        self.masks_dir = masks_dir\n        self.scale = scale\n        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n\n        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n                    if not file.startswith('.')]\n        logging.info(f'Creating dataset with {len(self.ids)} examples')\n\n    def __len__(self):\n        return len(self.ids)\n\n    @classmethod\n    def preprocess(cls, pil_img, scale):\n        w, h = pil_img.size\n        newW, newH = int(scale * w), int(scale * h)\n        assert newW > 0 and newH > 0, 'Scale is too small'\n        pil_img = pil_img.resize((newW, newH))\n\n        img_nd = np.array(pil_img)\n\n        if len(img_nd.shape) == 2:\n            img_nd = np.expand_dims(img_nd, axis=2)\n\n        # HWC to CHW\n        img_trans = img_nd.transpose((2, 0, 1))\n        if img_trans.max() > 1:\n            img_trans = img_trans / 255\n\n        return img_trans\n\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        mask_file = glob(self.masks_dir + idx + '.*')\n        img_file = glob(self.imgs_dir + idx + '.*')\n\n        assert len(mask_file) == 1, \\\n            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n        assert len(img_file) == 1, \\\n            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n        mask = Image.open(mask_file[0])\n        img = Image.open(img_file[0])\n\n        assert img.size == mask.size, \\\n            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n\n        img = self.preprocess(img, self.scale)\n        mask = self.preprocess(mask, self.scale)\n\n        return {\n            'image': torch.from_numpy(img).type(torch.FloatTensor),\n            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n        }\n"""
