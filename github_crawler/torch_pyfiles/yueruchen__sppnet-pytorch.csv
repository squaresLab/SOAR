file_path,api_count,code
cnn_with_spp.py,4,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\nfrom spp_layer import spatial_pyramid_pool\nclass SPP_NET(nn.Module):\n    '''\n    A CNN model which adds spp layer so that we can input multi-size tensor\n    '''\n    def __init__(self, opt, input_nc, ndf=64,  gpu_ids=[]):\n        super(SPP_NET, self).__init__()\n        self.gpu_ids = gpu_ids\n        self.output_num = [4,2,1]\n        \n        self.conv1 = nn.Conv2d(input_nc, ndf, 4, 2, 1, bias=False)\n        \n        self.conv2 = nn.Conv2d(ndf, ndf * 2, 4, 1, 1, bias=False)\n        self.BN1 = nn.BatchNorm2d(ndf * 2)\n\n        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 4, 1, 1, bias=False)\n        self.BN2 = nn.BatchNorm2d(ndf * 4)\n\n        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 4, 1, 1, bias=False)\n        self.BN3 = nn.BatchNorm2d(ndf * 8)\n\n        self.conv5 = nn.Conv2d(ndf * 8, 64, 4, 1, 0, bias=False)\n        self.fc1 = nn.Linear(10752,4096)\n        self.fc2 = nn.Linear(4096,1000)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.LReLU1(x)\n\n        x = self.conv2(x)\n        x = F.leaky_relu(self.BN1(x))\n\n        x = self.conv3(x)\n        x = F.leaky_relu(self.BN2(x))\n        \n        x = self.conv4(x)\n        # x = F.leaky_relu(self.BN3(x))\n        # x = self.conv5(x)\n        spp = spatial_pyramid_pool(x,1,[int(x.size(2)),int(x.size(3))],self.output_num)\n        # print(spp.size())\n        fc1 = self.fc1(spp)\n        fc2 = self.fc2(fc1)\n        s = nn.Sigmoid()\n        output = s(fc2)\n        return output\n\n    \n"""
spp_layer.py,1,"b'import math\ndef spatial_pyramid_pool(self,previous_conv, num_sample, previous_conv_size, out_pool_size):\n    \'\'\'\n    previous_conv: a tensor vector of previous convolution layer\n    num_sample: an int number of image in the batch\n    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n    out_pool_size: a int vector of expected output size of max pooling layer\n    \n    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n    \'\'\'    \n    # print(previous_conv.size())\n    for i in range(len(out_pool_size)):\n        # print(previous_conv_size)\n        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size[i]))\n        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size[i]))\n        h_pad = (h_wid*out_pool_size[i] - previous_conv_size[0] + 1)/2\n        w_pad = (w_wid*out_pool_size[i] - previous_conv_size[1] + 1)/2\n        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(h_pad, w_pad))\n        x = maxpool(previous_conv)\n        if(i == 0):\n            spp = x.view(num_sample,-1)\n            # print(""spp size:"",spp.size())\n        else:\n            # print(""size:"",spp.size())\n            spp = torch.cat((spp,x.view(num_sample,-1)), 1)\n    return spp'"
