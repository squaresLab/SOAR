file_path,api_count,code
main.py,11,"b""'''Train CIFAR10 with PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\n\nfrom models import *\nfrom utils import progress_bar\n\n\nparser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\nparser.add_argument('--lr', default=0.1, type=float, help='learning rate')\nparser.add_argument('--resume', '-r', action='store_true',\n                    help='resume from checkpoint')\nargs = parser.parse_args()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint('==> Preparing data..')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntrainset = torchvision.datasets.CIFAR10(\n    root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Model\nprint('==> Building model..')\n# net = VGG('VGG19')\n# net = ResNet18()\n# net = PreActResNet18()\n# net = GoogLeNet()\n# net = DenseNet121()\n# net = ResNeXt29_2x64d()\n# net = MobileNet()\n# net = MobileNetV2()\n# net = DPN92()\n# net = ShuffleNetG2()\n# net = SENet18()\n# net = ShuffleNetV2(1)\n# net = EfficientNetB0()\nnet = RegNetX_200MF()\nnet = net.to(device)\nif device == 'cuda':\n    net = torch.nn.DataParallel(net)\n    cudnn.benchmark = True\n\nif args.resume:\n    # Load checkpoint.\n    print('==> Resuming from checkpoint..')\n    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n    checkpoint = torch.load('./checkpoint/ckpt.pth')\n    net.load_state_dict(checkpoint['net'])\n    best_acc = checkpoint['acc']\n    start_epoch = checkpoint['epoch']\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=args.lr,\n                      momentum=0.9, weight_decay=5e-4)\n\n\n# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print('Saving..')\n        state = {\n            'net': net.state_dict(),\n            'acc': acc,\n            'epoch': epoch,\n        }\n        if not os.path.isdir('checkpoint'):\n            os.mkdir('checkpoint')\n        torch.save(state, './checkpoint/ckpt.pth')\n        best_acc = acc\n\n\nfor epoch in range(start_epoch, start_epoch+200):\n    train(epoch)\n    test(epoch)\n"""
utils.py,5,"b""'''Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n'''\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\n\n_, term_width = os.popen('stty size', 'r').read().split()\nterm_width = int(term_width)\n\nTOTAL_BAR_LENGTH = 65.\nlast_time = time.time()\nbegin_time = last_time\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f\n"""
models/__init__.py,0,b'from .vgg import *\nfrom .dpn import *\nfrom .lenet import *\nfrom .senet import *\nfrom .pnasnet import *\nfrom .densenet import *\nfrom .googlenet import *\nfrom .shufflenet import *\nfrom .shufflenetv2 import *\nfrom .resnet import *\nfrom .resnext import *\nfrom .preact_resnet import *\nfrom .mobilenet import *\nfrom .mobilenetv2 import *\nfrom .efficientnet import *\nfrom .regnet import *\n'
models/densenet.py,4,"b""'''DenseNet in PyTorch.'''\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out,x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2*growth_rate\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3]*growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n\ndef test():\n    net = densenet_cifar()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
models/dpn.py,4,"b""'''Dual Path Networks in PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n        super(Bottleneck, self).__init__()\n        self.out_planes = out_planes\n        self.dense_depth = dense_depth\n\n        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_planes)\n        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n\n        self.shortcut = nn.Sequential()\n        if first_layer:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_planes+dense_depth)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        x = self.shortcut(x)\n        d = self.out_planes\n        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n        out = F.relu(out)\n        return out\n\n\nclass DPN(nn.Module):\n    def __init__(self, cfg):\n        super(DPN, self).__init__()\n        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.last_planes = 64\n        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)\n\n    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for i,stride in enumerate(strides):\n            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n            self.last_planes = out_planes + (i+2) * dense_depth\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef DPN26():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (2,2,2,2),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\ndef DPN92():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (3,4,20,3),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\n\ndef test():\n    net = DPN92()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
models/efficientnet.py,4,"b'\'\'\'EfficientNet in PyTorch.\n\nPaper: ""EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"".\n\nReference: https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef swish(x):\n    return x * x.sigmoid()\n\n\ndef drop_connect(x, drop_ratio):\n    keep_ratio = 1.0 - drop_ratio\n    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n    mask.bernoulli_(keep_ratio)\n    x.div_(keep_ratio)\n    x.mul_(mask)\n    return x\n\n\nclass SE(nn.Module):\n    \'\'\'Squeeze-and-Excitation block with Swish.\'\'\'\n\n    def __init__(self, in_channels, se_channels):\n        super(SE, self).__init__()\n        self.se1 = nn.Conv2d(in_channels, se_channels,\n                             kernel_size=1, bias=True)\n        self.se2 = nn.Conv2d(se_channels, in_channels,\n                             kernel_size=1, bias=True)\n\n    def forward(self, x):\n        out = F.adaptive_avg_pool2d(x, (1, 1))\n        out = swish(self.se1(out))\n        out = self.se2(out).sigmoid()\n        out = x * out\n        return out\n\n\nclass Block(nn.Module):\n    \'\'\'expansion + depthwise + pointwise + squeeze-excitation\'\'\'\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 expand_ratio=1,\n                 se_ratio=0.,\n                 drop_rate=0.):\n        super(Block, self).__init__()\n        self.stride = stride\n        self.drop_rate = drop_rate\n        self.expand_ratio = expand_ratio\n\n        # Expansion\n        channels = expand_ratio * in_channels\n        self.conv1 = nn.Conv2d(in_channels,\n                               channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(channels)\n\n        # Depthwise conv\n        self.conv2 = nn.Conv2d(channels,\n                               channels,\n                               kernel_size=kernel_size,\n                               stride=stride,\n                               padding=(1 if kernel_size == 3 else 2),\n                               groups=channels,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n        # SE layers\n        se_channels = int(in_channels * se_ratio)\n        self.se = SE(channels, se_channels)\n\n        # Output\n        self.conv3 = nn.Conv2d(channels,\n                               out_channels,\n                               kernel_size=1,\n                               stride=1,\n                               padding=0,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n\n        # Skip connection if in and out shapes are the same (MV-V2 style)\n        self.has_skip = (stride == 1) and (in_channels == out_channels)\n\n    def forward(self, x):\n        out = x if self.expand_ratio == 1 else swish(self.bn1(self.conv1(x)))\n        out = swish(self.bn2(self.conv2(out)))\n        out = self.se(out)\n        out = self.bn3(self.conv3(out))\n        if self.has_skip:\n            if self.training and self.drop_rate > 0:\n                out = drop_connect(out, self.drop_rate)\n            out = out + x\n        return out\n\n\nclass EfficientNet(nn.Module):\n    def __init__(self, cfg, num_classes=10):\n        super(EfficientNet, self).__init__()\n        self.cfg = cfg\n        self.conv1 = nn.Conv2d(3,\n                               32,\n                               kernel_size=3,\n                               stride=1,\n                               padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_channels=32)\n        self.linear = nn.Linear(cfg[\'out_channels\'][-1], num_classes)\n\n    def _make_layers(self, in_channels):\n        layers = []\n        cfg = [self.cfg[k] for k in [\'expansion\', \'out_channels\', \'num_blocks\', \'kernel_size\',\n                                     \'stride\']]\n        b = 0\n        blocks = sum(self.cfg[\'num_blocks\'])\n        for expansion, out_channels, num_blocks, kernel_size, stride in zip(*cfg):\n            strides = [stride] + [1] * (num_blocks - 1)\n            for stride in strides:\n                drop_rate = self.cfg[\'drop_connect_rate\'] * b / blocks\n                layers.append(\n                    Block(in_channels,\n                          out_channels,\n                          kernel_size,\n                          stride,\n                          expansion,\n                          se_ratio=0.25,\n                          drop_rate=drop_rate))\n                in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = swish(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.adaptive_avg_pool2d(out, 1)\n        out = out.view(out.size(0), -1)\n        dropout_rate = self.cfg[\'dropout_rate\']\n        if self.training and dropout_rate > 0:\n            out = F.dropout(out, p=dropout_rate)\n        out = self.linear(out)\n        return out\n\n\ndef EfficientNetB0():\n    cfg = {\n        \'num_blocks\': [1, 2, 2, 3, 3, 4, 1],\n        \'expansion\': [1, 6, 6, 6, 6, 6, 6],\n        \'out_channels\': [16, 24, 40, 80, 112, 192, 320],\n        \'kernel_size\': [3, 3, 5, 3, 5, 5, 3],\n        \'stride\': [1, 2, 2, 2, 1, 2, 1],\n        \'dropout_rate\': 0.2,\n        \'drop_connect_rate\': 0.2,\n    }\n    return EfficientNet(cfg)\n\n\ndef test():\n    net = EfficientNetB0()\n    x = torch.randn(2, 3, 32, 32)\n    y = net(x)\n    print(y.shape)\n\n\nif __name__ == \'__main__\':\n    test()\n'"
models/googlenet.py,4,"b""'''GoogLeNet with PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1,y2,y3,y4], 1)\n\n\nclass GoogLeNet(nn.Module):\n    def __init__(self):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n\n        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = GoogLeNet()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n"""
models/lenet.py,2,"b""'''LeNet in PyTorch.'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out\n"""
models/mobilenet.py,3,"b'\'\'\'MobileNet in PyTorch.\n\nSee the paper ""MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications""\nfor more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'Depthwise conv + Pointwise conv\'\'\'\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out\n\n\nclass MobileNet(nn.Module):\n    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n\n    def __init__(self, num_classes=10):\n        super(MobileNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.linear = nn.Linear(1024, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for x in self.cfg:\n            out_planes = x if isinstance(x, int) else x[0]\n            stride = 1 if isinstance(x, int) else x[1]\n            layers.append(Block(in_planes, out_planes, stride))\n            in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.avg_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNet()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
models/mobilenetv2.py,3,"b'\'\'\'MobileNetV2 in PyTorch.\n\nSee the paper ""Inverted Residuals and Linear Bottlenecks:\nMobile Networks for Classification, Detection and Segmentation"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'expand + depthwise + pointwise\'\'\'\n    def __init__(self, in_planes, out_planes, expansion, stride):\n        super(Block, self).__init__()\n        self.stride = stride\n\n        planes = expansion * in_planes\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 1 and in_planes != out_planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = out + self.shortcut(x) if self.stride==1 else out\n        return out\n\n\nclass MobileNetV2(nn.Module):\n    # (expansion, out_planes, num_blocks, stride)\n    cfg = [(1,  16, 1, 1),\n           (6,  24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n           (6,  32, 3, 2),\n           (6,  64, 4, 2),\n           (6,  96, 3, 1),\n           (6, 160, 3, 2),\n           (6, 320, 1, 1)]\n\n    def __init__(self, num_classes=10):\n        super(MobileNetV2, self).__init__()\n        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        self.linear = nn.Linear(1280, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for expansion, out_planes, num_blocks, stride in self.cfg:\n            strides = [stride] + [1]*(num_blocks-1)\n            for stride in strides:\n                layers.append(Block(in_planes, out_planes, expansion, stride))\n                in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNetV2()\n    x = torch.randn(2,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
models/pnasnet.py,4,"b""'''PNASNet in PyTorch.\n\nPaper: Progressive Neural Architecture Search\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SepConv(nn.Module):\n    '''Separable Convolution.'''\n    def __init__(self, in_planes, out_planes, kernel_size, stride):\n        super(SepConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, out_planes,\n                               kernel_size, stride,\n                               padding=(kernel_size-1)//2,\n                               bias=False, groups=in_planes)\n        self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        return self.bn1(self.conv1(x))\n\n\nclass CellA(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellA, self).__init__()\n        self.stride = stride\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        if stride==2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        y1 = self.sep_conv1(x)\n        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride==2:\n            y2 = self.bn1(self.conv1(y2))\n        return F.relu(y1+y2)\n\nclass CellB(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellB, self).__init__()\n        self.stride = stride\n        # Left branch\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)\n        # Right branch\n        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)\n        if stride==2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n        # Reduce channels\n        self.conv2 = nn.Conv2d(2*out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        # Left branch\n        y1 = self.sep_conv1(x)\n        y2 = self.sep_conv2(x)\n        # Right branch\n        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride==2:\n            y3 = self.bn1(self.conv1(y3))\n        y4 = self.sep_conv3(x)\n        # Concat & reduce channels\n        b1 = F.relu(y1+y2)\n        b2 = F.relu(y3+y4)\n        y = torch.cat([b1,b2], 1)\n        return F.relu(self.bn2(self.conv2(y)))\n\nclass PNASNet(nn.Module):\n    def __init__(self, cell_type, num_cells, num_planes):\n        super(PNASNet, self).__init__()\n        self.in_planes = num_planes\n        self.cell_type = cell_type\n\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_planes)\n\n        self.layer1 = self._make_layer(num_planes, num_cells=6)\n        self.layer2 = self._downsample(num_planes*2)\n        self.layer3 = self._make_layer(num_planes*2, num_cells=6)\n        self.layer4 = self._downsample(num_planes*4)\n        self.layer5 = self._make_layer(num_planes*4, num_cells=6)\n\n        self.linear = nn.Linear(num_planes*4, 10)\n\n    def _make_layer(self, planes, num_cells):\n        layers = []\n        for _ in range(num_cells):\n            layers.append(self.cell_type(self.in_planes, planes, stride=1))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def _downsample(self, planes):\n        layer = self.cell_type(self.in_planes, planes, stride=2)\n        self.in_planes = planes\n        return layer\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = F.avg_pool2d(out, 8)\n        out = self.linear(out.view(out.size(0), -1))\n        return out\n\n\ndef PNASNetA():\n    return PNASNet(CellA, num_cells=6, num_planes=44)\n\ndef PNASNetB():\n    return PNASNet(CellB, num_cells=6, num_planes=32)\n\n\ndef test():\n    net = PNASNetB()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
models/preact_resnet.py,3,"b""'''Pre-activation ResNet in PyTorch.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBlock(nn.Module):\n    '''Pre-activation version of the BasicBlock.'''\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    '''Pre-activation version of the original Bottleneck module.'''\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef PreActResNet18():\n    return PreActResNet(PreActBlock, [2,2,2,2])\n\ndef PreActResNet34():\n    return PreActResNet(PreActBlock, [3,4,6,3])\n\ndef PreActResNet50():\n    return PreActResNet(PreActBottleneck, [3,4,6,3])\n\ndef PreActResNet101():\n    return PreActResNet(PreActBottleneck, [3,4,23,3])\n\ndef PreActResNet152():\n    return PreActResNet(PreActBottleneck, [3,8,36,3])\n\n\ndef test():\n    net = PreActResNet18()\n    y = net((torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()\n"""
models/regnet.py,3,"b'\'\'\'RegNet in PyTorch.\n\nPaper: ""Designing Network Design Spaces"".\n\nReference: https://github.com/keras-team/keras-applications/blob/master/keras_applications/efficientnet.py\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SE(nn.Module):\n    \'\'\'Squeeze-and-Excitation block.\'\'\'\n\n    def __init__(self, in_planes, se_planes):\n        super(SE, self).__init__()\n        self.se1 = nn.Conv2d(in_planes, se_planes, kernel_size=1, bias=True)\n        self.se2 = nn.Conv2d(se_planes, in_planes, kernel_size=1, bias=True)\n\n    def forward(self, x):\n        out = F.adaptive_avg_pool2d(x, (1, 1))\n        out = F.relu(self.se1(out))\n        out = self.se2(out).sigmoid()\n        out = x * out\n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, w_in, w_out, stride, group_width, bottleneck_ratio, se_ratio):\n        super(Block, self).__init__()\n        # 1x1\n        w_b = int(round(w_out * bottleneck_ratio))\n        self.conv1 = nn.Conv2d(w_in, w_b, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(w_b)\n        # 3x3\n        num_groups = w_b // group_width\n        self.conv2 = nn.Conv2d(w_b, w_b, kernel_size=3,\n                               stride=stride, padding=1, groups=num_groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(w_b)\n        # se\n        self.with_se = se_ratio > 0\n        if self.with_se:\n            w_se = int(round(w_in * se_ratio))\n            self.se = SE(w_b, w_se)\n        # 1x1\n        self.conv3 = nn.Conv2d(w_b, w_out, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(w_out)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or w_in != w_out:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(w_in, w_out,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(w_out)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        if self.with_se:\n            out = self.se(out)\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass RegNet(nn.Module):\n    def __init__(self, cfg, num_classes=10):\n        super(RegNet, self).__init__()\n        self.cfg = cfg\n        self.in_planes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(0)\n        self.layer2 = self._make_layer(1)\n        self.layer3 = self._make_layer(2)\n        self.layer4 = self._make_layer(3)\n        self.linear = nn.Linear(self.cfg[\'widths\'][-1], num_classes)\n\n    def _make_layer(self, idx):\n        depth = self.cfg[\'depths\'][idx]\n        width = self.cfg[\'widths\'][idx]\n        stride = self.cfg[\'strides\'][idx]\n        group_width = self.cfg[\'group_width\']\n        bottleneck_ratio = self.cfg[\'bottleneck_ratio\']\n        se_ratio = self.cfg[\'se_ratio\']\n\n        layers = []\n        for i in range(depth):\n            s = stride if i == 0 else 1\n            layers.append(Block(self.in_planes, width,\n                                s, group_width, bottleneck_ratio, se_ratio))\n            self.in_planes = width\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef RegNetX_200MF():\n    cfg = {\n        \'depths\': [1, 1, 4, 7],\n        \'widths\': [24, 56, 152, 368],\n        \'strides\': [1, 1, 2, 2],\n        \'group_width\': 8,\n        \'bottleneck_ratio\': 1,\n        \'se_ratio\': 0,\n    }\n    return RegNet(cfg)\n\n\ndef RegNetX_400MF():\n    cfg = {\n        \'depths\': [1, 2, 7, 12],\n        \'widths\': [32, 64, 160, 384],\n        \'strides\': [1, 1, 2, 2],\n        \'group_width\': 16,\n        \'bottleneck_ratio\': 1,\n        \'se_ratio\': 0,\n    }\n    return RegNet(cfg)\n\n\ndef RegNetY_400MF():\n    cfg = {\n        \'depths\': [1, 2, 7, 12],\n        \'widths\': [32, 64, 160, 384],\n        \'strides\': [1, 1, 2, 2],\n        \'group_width\': 16,\n        \'bottleneck_ratio\': 1,\n        \'se_ratio\': 0.25,\n    }\n    return RegNet(cfg)\n\n\ndef test():\n    net = RegNetX_200MF()\n    print(net)\n    x = torch.randn(2, 3, 32, 32)\n    y = net(x)\n    print(y.shape)\n\n\nif __name__ == \'__main__\':\n    test()\n'"
models/resnet.py,3,"b""'''ResNet in PyTorch.\n\nFor Pre-activation ResNet, see 'preact_resnet.py'.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3, 4, 6, 3])\n\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3, 4, 6, 3])\n\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3, 4, 23, 3])\n\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3, 8, 36, 3])\n\n\ndef test():\n    net = ResNet18()\n    y = net(torch.randn(1, 3, 32, 32))\n    print(y.size())\n\n# test()\n"""
models/resnext.py,3,"b'\'\'\'ResNeXt in PyTorch.\n\nSee the paper ""Aggregated Residual Transformations for Deep Neural Networks"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'Grouped convolution block.\'\'\'\n    expansion = 2\n\n    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n        super(Block, self).__init__()\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*group_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*group_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n        super(ResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.bottleneck_width = bottleneck_width\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(num_blocks[0], 1)\n        self.layer2 = self._make_layer(num_blocks[1], 2)\n        self.layer3 = self._make_layer(num_blocks[2], 2)\n        # self.layer4 = self._make_layer(num_blocks[3], 2)\n        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n\n    def _make_layer(self, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n        # Increase bottleneck_width by 2 after each stage.\n        self.bottleneck_width *= 2\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        # out = self.layer4(out)\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNeXt29_2x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n\ndef ResNeXt29_4x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n\ndef ResNeXt29_8x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n\ndef ResNeXt29_32x4d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n\ndef test_resnext():\n    net = ResNeXt29_2x64d()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test_resnext()\n'"
models/senet.py,3,"b""'''SENet in PyTorch.\n\nSENet is the winner of ImageNet-2017. The paper is not released yet.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w  # New broadcasting feature from v0.2!\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass PreActBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w\n\n        out += shortcut\n        return out\n\n\nclass SENet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(SENet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef SENet18():\n    return SENet(PreActBlock, [2,2,2,2])\n\n\ndef test():\n    net = SENet18()\n    y = net(torch.randn(1,3,32,32))\n    print(y.size())\n\n# test()\n"""
models/shufflenet.py,4,"b'\'\'\'ShuffleNet in PyTorch.\n\nSee the paper ""ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N,C,H,W = x.size()\n        g = self.groups\n        return x.view(N,g,C//g,H,W).permute(0,2,1,3,4).reshape(N,C,H,W)\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, groups):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n\n        mid_planes = out_planes/4\n        g = 1 if in_planes==24 else groups\n        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.shuffle1 = ShuffleBlock(groups=g)\n        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 2:\n            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.shuffle1(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        res = self.shortcut(x)\n        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n        return out\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self, cfg):\n        super(ShuffleNet, self).__init__()\n        out_planes = cfg[\'out_planes\']\n        num_blocks = cfg[\'num_blocks\']\n        groups = cfg[\'groups\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_planes = 24\n        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n        self.linear = nn.Linear(out_planes[2], 10)\n\n    def _make_layer(self, out_planes, num_blocks, groups):\n        layers = []\n        for i in range(num_blocks):\n            stride = 2 if i == 0 else 1\n            cat_planes = self.in_planes if i == 0 else 0\n            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n            self.in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ShuffleNetG2():\n    cfg = {\n        \'out_planes\': [200,400,800],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 2\n    }\n    return ShuffleNet(cfg)\n\ndef ShuffleNetG3():\n    cfg = {\n        \'out_planes\': [240,480,960],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 3\n    }\n    return ShuffleNet(cfg)\n\n\ndef test():\n    net = ShuffleNetG2()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
models/shufflenetv2.py,5,"b'\'\'\'ShuffleNetV2 in PyTorch.\n\nSee the paper ""ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups=2):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N, C, H, W = x.size()\n        g = self.groups\n        return x.view(N, g, C//g, H, W).permute(0, 2, 1, 3, 4).reshape(N, C, H, W)\n\n\nclass SplitBlock(nn.Module):\n    def __init__(self, ratio):\n        super(SplitBlock, self).__init__()\n        self.ratio = ratio\n\n    def forward(self, x):\n        c = int(x.size(1) * self.ratio)\n        return x[:, :c, :, :], x[:, c:, :, :]\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, split_ratio=0.5):\n        super(BasicBlock, self).__init__()\n        self.split = SplitBlock(split_ratio)\n        in_channels = int(in_channels * split_ratio)\n        self.conv1 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=3, stride=1, padding=1, groups=in_channels, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n        self.conv3 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(in_channels)\n        self.shuffle = ShuffleBlock()\n\n    def forward(self, x):\n        x1, x2 = self.split(x)\n        out = F.relu(self.bn1(self.conv1(x2)))\n        out = self.bn2(self.conv2(out))\n        out = F.relu(self.bn3(self.conv3(out)))\n        out = torch.cat([x1, out], 1)\n        out = self.shuffle(out)\n        return out\n\n\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DownBlock, self).__init__()\n        mid_channels = out_channels // 2\n        # left\n        self.conv1 = nn.Conv2d(in_channels, in_channels,\n                               kernel_size=3, stride=2, padding=1, groups=in_channels, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_channels)\n        # right\n        self.conv3 = nn.Conv2d(in_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(mid_channels)\n        self.conv4 = nn.Conv2d(mid_channels, mid_channels,\n                               kernel_size=3, stride=2, padding=1, groups=mid_channels, bias=False)\n        self.bn4 = nn.BatchNorm2d(mid_channels)\n        self.conv5 = nn.Conv2d(mid_channels, mid_channels,\n                               kernel_size=1, bias=False)\n        self.bn5 = nn.BatchNorm2d(mid_channels)\n\n        self.shuffle = ShuffleBlock()\n\n    def forward(self, x):\n        # left\n        out1 = self.bn1(self.conv1(x))\n        out1 = F.relu(self.bn2(self.conv2(out1)))\n        # right\n        out2 = F.relu(self.bn3(self.conv3(x)))\n        out2 = self.bn4(self.conv4(out2))\n        out2 = F.relu(self.bn5(self.conv5(out2)))\n        # concat\n        out = torch.cat([out1, out2], 1)\n        out = self.shuffle(out)\n        return out\n\n\nclass ShuffleNetV2(nn.Module):\n    def __init__(self, net_size):\n        super(ShuffleNetV2, self).__init__()\n        out_channels = configs[net_size][\'out_channels\']\n        num_blocks = configs[net_size][\'num_blocks\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_channels = 24\n        self.layer1 = self._make_layer(out_channels[0], num_blocks[0])\n        self.layer2 = self._make_layer(out_channels[1], num_blocks[1])\n        self.layer3 = self._make_layer(out_channels[2], num_blocks[2])\n        self.conv2 = nn.Conv2d(out_channels[2], out_channels[3],\n                               kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels[3])\n        self.linear = nn.Linear(out_channels[3], 10)\n\n    def _make_layer(self, out_channels, num_blocks):\n        layers = [DownBlock(self.in_channels, out_channels)]\n        for i in range(num_blocks):\n            layers.append(BasicBlock(out_channels))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        # out = F.max_pool2d(out, 3, stride=2, padding=1)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\nconfigs = {\n    0.5: {\n        \'out_channels\': (48, 96, 192, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n\n    1: {\n        \'out_channels\': (116, 232, 464, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n    1.5: {\n        \'out_channels\': (176, 352, 704, 1024),\n        \'num_blocks\': (3, 7, 3)\n    },\n    2: {\n        \'out_channels\': (224, 488, 976, 2048),\n        \'num_blocks\': (3, 7, 3)\n    }\n}\n\n\ndef test():\n    net = ShuffleNetV2(net_size=0.5)\n    x = torch.randn(3, 3, 32, 32)\n    y = net(x)\n    print(y.shape)\n\n\n# test()\n'"
models/vgg.py,3,"b""'''VGG11/13/16/19 in Pytorch.'''\nimport torch\nimport torch.nn as nn\n\n\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n\ndef test():\n    net = VGG('VGG11')\n    x = torch.randn(2,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n"""
